[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GSB 544: Data Science and Machine Learning with Python",
    "section": "",
    "text": "This text was created for the Cal Poly course “GSB 544: Data Science and Machine Learning with Python” by Dr. Kelly Bodwin and Dr. Hunter Glanz. Some parts of the material and text are borrowed from Dr. Emily Robinson’s R course and Dr. Dennis Sun’s python course\nThis text is not meant to be a complete course or textbook by itself; rather, think of it as “long-form” class slides. We will summarize the main concepts in each chapter, show you examples, point you to more in-depth readings from outside sources, and ask you to try out short tasks in python as you go.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n\n\n\n\n\n\n\nExample\n\n\n\nExample sections contain code and other information. Don’t skip them!\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote sections contain clarification points (anywhere I would normally say “note that ….). Make sure to read them to avoid any common pitfalls or misconceptions.\n\n\n\n\n\n\n\n\nRequired-reading\n\n\n\nConsider these sections to be required readings. This is where we will direct you to existing materials to explain or introduce a concept.\n\n\n\n\n\n\n\n\nRequired-video\n\n\n\nSimilarly, consider these sections to be required viewing for the course material.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nCheck-in sections contain small tasks that you need to do throughout the reading, to practice or prepare. Although they are not graded, please treat them as required!\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nEach chapter will have a longer practice exercise to complete and turn in. These are intended to be done with help from instructors and peers.\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\nConsider these to be optional readings/viewings. The world of python programming has so many interesting tidbits, we can’t possibly teach them all - but we want to share them with you nonetheless!\nWe will usually make these “click to expand” so that they don’t distract from your reading.\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nThese are personal opinion comments from the authors. Take them with a grain of salt; we aren’t the only python programmers worth listening to, we are just sharing what has worked for us.\nWe will usually make these “click to expand” so that they don’t distract from your reading.\n\n\n\n\n\n\nReferences or additional readings may come from the following texts:\n\nPython Data Science Handbook by Jake VanderPlas\nPython for Data Analysis by Wes McKinney\nPrinciples of Data Science by Dennis Sun\n\nFor extra practice with python programming, we recommend the DataQuest interactive tutorials or working through some lessons on Python for Everybody."
  },
  {
    "objectID": "00-setup.html",
    "href": "00-setup.html",
    "title": "1  Intro and Workflow Setup",
    "section": "",
    "text": "While most students will arrive having taken an introductory programming course and/or Summer python intensive workshop, it is important to start this class with some computer fundamentals and some setup and workflow preparation.\nThis chapter is meant to provide a resource for some basic computer skills and a guide to the workflow we expect you to follow when managing your code and data.\nIn this chapter you will:\n\nLearn the basics of a computer system.\nCreate a GitHub account and practice using it for managing code and data artifacts.\nPractice using Google Colab notebooks for quick tasks and activities.\nInstall python locally on your machine via Anaconda.\nPractice opening and using jupyter notebooks in Anaconda.\nInstall the quarto document rendering system.\nPractice using quarto to render nicely formatted html documents from jupyter notebooks.\n\nIf you already feel comfortable with your own file organization system; you prefer GitLab over GitHub; or you prefer to use another python distribution and IDE (like VSCode), that is acceptable. Just know that we may be less able to help you and troubleshoot if you deviate from the recommended workflow.\nFor grading consistency, we will require that you submit quarto-rendered documents for all labs and projects."
  },
  {
    "objectID": "00-setup.html#computer-basics",
    "href": "00-setup.html#computer-basics",
    "title": "1  Intro and Workflow Setup",
    "section": "1.1 Computer Basics",
    "text": "1.1 Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\n\n1.1.1 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\n\n\nLearn-more\n\n\n\n{{ <video “https://www.youtube.com/embed/Rdm8E59L8Og >}}\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\n\n\n\n1.1.2 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n\nLearn-more\n\n\n\n{{ <video https://www.youtube.com/embed/RhHMgkUdhdk” > }}\n\n\n\n\n1.1.3 File Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\n\n\n\n\n\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BV0-EPUYuQc >}}\nStop watching at 4:16.\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BMT3JUWmqYY >}}\n\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nRecommend watching - helpful for understanding file paths!\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home."
  },
  {
    "objectID": "00-setup.html#git-and-github",
    "href": "00-setup.html#git-and-github",
    "title": "1  Intro and Workflow Setup",
    "section": "1.2 Git and GitHub",
    "text": "1.2 Git and GitHub\nOne of the most important parts of a data scientist’s workflow is version tracking: the process of making sure that you have a record of the changes and updates you have made to your code.\n\n1.2.1 Git\nGit is a computer program that lives on your local computer. Once you designate a folder as a Git Repository, the program will automatically tracks changes to the files in side that folder.\n\n\n\n\n\n\nCheck-in\n\n\n\nClick here to install Git on your computer.\n\n\n\n\n1.2.2 GitHub\nGitHub, and the less used alternate GitLab, are websites where Git Repositories can be stored online. This is useful for sharing your repository (“repo”) with others, for multiple people collaborating on the same repository, and for yourself to be able to access your files from anywhere.\n\n\n\n\n\n\nCheck-in\n\n\n\nClick here to make a GitHub account, if you do not already have one.\nYou do not have to use your school email for this account.\n\n\n\n\n1.2.3 Practice with Repos\nIf you are already familiar with how to use Git and GitHub, you can skip the rest of this section, which will walk us through some practice making and editing repositories.\nFirst, watch this 15-minute video, which nicely illustrates the basics of version tracking:\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BCQHnlnPusY?si=L9C5waHxDzib-VwY >}}\n\n\nThen, watch this 10-minute video, which introduces the idea of branches, and important habit for collaborating with others (or your future self!)\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/oPpnCh7InLY?si=Yzezgt3R4n1OYBdV >}}\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nAlthough Git can sometimes be a headache, it is worth the struggle. Never again will you have to deal with a folder full of documents that looks like:\nProject-Final\nProject-Final-1\nProject-Final-again\nProject-Final-1-1\nProject-Final-for-real\n\n\n\nWorking with Git and GitHub can be made a lot easier by helper tools and apps. We recommend GitHub Desktop for your committing and pushing.\n\n\n1.2.4 Summary\nFor our purposes, it will be sufficient for you to learn to:\n\ncommit your work frequently as you make progress; about as often as you might save a document\npush your work every time you step away from your project\nbranch your repo when you want to try something and you aren’t sure it will work.\n\nIt will probably take you some time to get used to a workflow that feels natural to you - that’s okay! As long as you are trying out version control, you’re doing great."
  },
  {
    "objectID": "00-setup.html#anaconda-and-jupyter",
    "href": "00-setup.html#anaconda-and-jupyter",
    "title": "1  Intro and Workflow Setup",
    "section": "1.3 Anaconda and Jupyter",
    "text": "1.3 Anaconda and Jupyter\nNow, let’s talk about getting python actually set up and running.\n\n\n\n\n\n\n1.3.1 Anaconda\nOne downside of python is that it can sometimes be complicated to keep track of installs and updates.\n\n\n\n\n\nUnless you already have a python environment setup that works for you, we will suggest that you use Anaconda, which bundles together an installation of the most recent python version as well as multiple tools for interacting with the code.\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload Anaconda here\n\n\n\n\n1.3.2 Jupyter\nWhen you are writing ordinary text, you choose what type of document to use - Microsoft Word, Google Docs, LaTeX, etc.\nSimilarly, there are many types of files where you can write python code. By far the most common and popular is the jupyter notebook.\nThe advantage of a jupyter notebook is that ordinary text and “chunks” of code can be interspersed.\n\n\n\n\n\nJupyter notebooks have the file extension .ipynb for “i python notebook”.\n\n1.3.2.1 Google Colab\nOne way you may have seen the Jupyter notebooks before is on Google’s free cloud service, Google Colab.\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, to practice using Jupyter notebooks.\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nColab is an amazing data science tool that allows for easy collaboration.\nHowever, there is a limited amount of free compute time offered by Google, and not as much flexibility or control over the documents.\nThis is why we need Anaconda or similar local installations."
  },
  {
    "objectID": "00-setup.html#quarto",
    "href": "00-setup.html#quarto",
    "title": "1  Intro and Workflow Setup",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nAlthough jupyter and Colab are fantastic tools for data analysis, one major limitation is that the raw notebooks themselves are not the same as a final clear report.\nTo convert our interactive notebooks into professionally presented static documents, we will use a program called Quarto.\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload Quarto here\n\n\nOnce quarto is installed, converting a .ipynb file requires running only a single line in the Terminal:\nquarto render my_file.ipynb\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload the .ipynb file from your practice Colab notebook, open it using Anaconda, and render it using Quarto.\n\n\nHowever, there are also many, many options to make the final rendered document look even more visually pleasing and professional. Have a look at the Quarto documentation if you want to play around with themes, fonts, layouts, and so on.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nhttps://quarto.org/docs/get-started/hello/jupyter.html"
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "2  Programming Basics",
    "section": "",
    "text": "In this chapter, we will review some basics of general computer programming, and how they appear in python.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nSome of you may find this material to be unneeded review - if so, great!\nBut if you are new to programming, or it has been a while, this tutorial may help you refresh your knowledge."
  },
  {
    "objectID": "01-basics.html#basic-data-types",
    "href": "01-basics.html#basic-data-types",
    "title": "2  Programming Basics",
    "section": "2.1 Basic Data Types",
    "text": "2.1 Basic Data Types\nIt is important to have a base grasp on the types of data you might see in data analyses.\n\n2.1.1 Values and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn python, there are some very basic data types:\n\nlogical or boolean - False/True or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\ndouble or float - decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).1\n\nnumeric - python uses the name numeric to indicate a decimal value, regardless of precision.\ncharacter or string or object - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, python has a function to help you with that.\n\ntype(False)\ntype(2) # by default, python treats whole numbers as integers\ntype(2.0)  # to force it not to be an integer, add a .0\ntype(\"Hello, programmer!\")\n\nstr\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn python, boolean values are True and False. Capitalization matters a LOT.\nOther details: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\n\n\n\n2.1.2 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn python, we assign variables values using the syntax object_name = value You can read this as “object name gets value” in your head.\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\nmessage + \", sang the dolphins.\"\n\nyear + the_answer\n\nnot earth_demolished\n\nTrue\n\n\n\n2.1.2.1 Valid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, and _.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right.\n\n1st_thing = \"No starting with numbers\"\n\nfirst~thing = \"No other symbols\"\n\nfirst.thing = \"Periods have a particular meaning!\"\n\nSyntaxError: invalid syntax (3761243318.py, line 1)\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\n\n\nThere are a few different conventions for naming things that may be useful:\n\nsome_people_use_snake_case, where words are separated by underscores\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nA few people mix conventions with variables_thatLookLike_this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\n\n\n\n2.1.3 Type Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\ntype(2 + 3.14159) # add integer 2 and pi\ntype(2 + True) # add integer 2 and TRUE\ntype(True + False) # add TRUE and FALSE\n\nint\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how python handles type conversions.\n\n\n\n\n\n\nCheck-in\n\n\n\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “NA” or ” ” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell python that we want our string to be treated as a number, or vice-versa.\nIn python, we can explicitly convert a variable’s type using functions (int, float, str, etc.).\n\nx = 3\ny = \"3.14159\"\n\nx + y\n\nx + float(y)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'"
  },
  {
    "objectID": "01-basics.html#operators-and-functions",
    "href": "01-basics.html#operators-and-functions",
    "title": "2  Programming Basics",
    "section": "2.2 Operators and Functions",
    "text": "2.2 Operators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\n\nOperation\npython symbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n//\n\n\nModular Division\n%\n\n\nExponentiation\n**\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 // 3 would be 4, and 14 % 3 would be 2.\n\n14 // 3\n14 % 3\n\n2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\n\n2.2.1 Order of Operations\npython operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)**(5-2) # 2 ^ 3 = 8\n1 + 2**3 * 4 # 1 + (8 * 4)\n3*1**3 # 3 * 1\n\n3\n\n\n\n\n2.2.2 String Operations\nThe + operator also works on strings. Just remember that python doesn’t speak English - it neither knows nor cares if your strings are words, sentences, etc. So if you want to create good punctuation or spacing, that needs to be done in the code.\n\ngreeting = \"howdy\"\nperson = \"pardner\"\n\ngreeting + person\ngreeting + \", \" + person\n\n'howdy, pardner'\n\n\n\n\n2.2.3 Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. We only need to know how to use functions. Let’s look at the official documentation for the function round()`.\nround(number, ndigits=None)\n\nReturn number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input.\nThis tells us that the function requires one argument, number, a number to round. You also have the option to include a second argument, ndigits, if you want to round to something other than a whole number.\nWhen you call a function, you can either use the names of the arguments, or simply provide the information in the expected order.\nBy convention, we usually use names for optional arguments but not required ones.\n\nround(number = 2.718)\nround(2.718, 2)\n\n\nround(2.718)\nround(2.718, ndigits = 2)\n\n2.72\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe names of functions and their arguments are chosen by the developer who created them. You should never simply assume what a function or argument will do based on the name; always check documentation or try small test examples if you aren’t sure."
  },
  {
    "objectID": "01-basics.html#data-structures",
    "href": "01-basics.html#data-structures",
    "title": "2  Programming Basics",
    "section": "2.3 Data Structures",
    "text": "2.3 Data Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\nMethods or attributes are a special type of function that operate only on a specific data structure When using a method in python, you can use a period . to apply the function to an object.\nmy_nums = [1,2,3,4,5]\nmy_nums.sort()\nCareful, though! If a function is not specifically designed to be an attribute of the structure, this . trick won’t work.\n\nmy_nums.round()\n\nAttributeError: 'list' object has no attribute 'round'\n\n\n\n2.3.1 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nx = [\"a\", 3, True]\nx\n\n['a', 3, True]\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n2.3.1.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)3.\nIn python, we count from 0.\n\nx = [\"a\", 3, True]\n\nx[0] # This returns a list\nx[0:2] # This returns multiple elements in the list\n\nx.pop(0)\n\n'a'\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use the .pop attribute. The only downside to .pop is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n\n\n2.3.2 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \nBase python does not actually have a vector-type object! However, in data analysis we often have reasons to want a single-type data structure, so we will load an extra function called array from the numpy library to help us out. (More on libraries later!)\n\nfrom numpy import array\n\ndigits_pi = array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n# Access individual entries\ndigits_pi[1]\n\n# Print out the vector\ndigits_pi\n\narray([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific elements as well:\n\nfavorite_cats = array([\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"])\n\nfavorite_cats\n\nfavorite_cats[2] = \"Nyan Cat\"\n\nfavorite_cats\n\narray(['Grumpy', 'Garfield', 'Nyan Cat', 'Jean'], dtype='<U8')\n\n\nIf you’re curious about any of these cats, see the footnotes4.\n\n2.3.2.1 Boolean masking\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that tells us which elements of a corresponding vector we want to keep.\n\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\n\n\n\nNote that for boolean masking to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\n\n\n\n\nExample\n\n\n\n\n# Define a character vector\nweekdays = array([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"])\nweekend = array([\"Sunday\", \"Saturday\"])\n\n# Create logical vectors manually\nrelax_days = array([True, False, False, False, False, False, True])\n\n# Create logical vectors automatically\nfrom numpy import isin     # get a special function for arrays\nrelax_days = isin(weekdays, weekend) \n\nrelax_days\n\n# Using logical vectors to index the character vector\nweekdays[relax_days] \n\n# Using ~ to reverse the True and False\nweekdays[~relax_days] \n\narray(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n      dtype='<U9')\n\n\n\n\n\n\n2.3.2.2 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\n\n\n\n\n\nExample\n\n\n\n\narray([2, False, 3.1415, \"animal\"]) # all converted to strings\n\narray([2, False, 3.1415]) # converted to numerics\n\narray([2, False]) # converted to integers\n\narray([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - python decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\n\n\n\n\nWarning\n\n\n\nImplicit type conversions may seem convenient, but they are dangerous! Imagine that you created one of the arrays above, expecting it to be numeric, and only found out later that python had made it into strings.\n\n\n\n\n\n2.3.3 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\nOnce again, we need to use the numpy package to allow the matrix type to exist in python.\n\nfrom numpy import matrix\n\nmatrix([[1,2,3], [4,5,6]])\n\nmatrix([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how we give the matrix() function an argument that is a “list of lists”. That is, the first item in the list is [1,2,3] which is itself a list.\nYou can always think of lists as the most “neutral” data structure - if you don’t know what you want to use, it’s reasonably to start with the list, and then adjust from there, as we have with the array() and matrix() functions from numpy.\n\n\n\n2.3.3.1 Indexing in Matrices\npython uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [2,0] to get to the third row and first column entry (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\n\n\n\n\nExample\n\n\n\n\nmy_mat = matrix([[1,2,3,4], [4,5,6,7], [7,8,9,10]])\n\nmy_mat\n\nmy_mat[2,0] = 500\n\nmy_mat\n\nmatrix([[  1,   2,   3,   4],\n        [  4,   5,   6,   7],\n        [500,   8,   9,  10]])\n\n\n\n\nWe will not use matrices often in this class, but there are many math operations that are very specific to matrices. If you continue on in your data science journey, you will probably eventually need to do matrix algebra in python.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nTutorial: Linear Algebra in python"
  },
  {
    "objectID": "01-basics.html#libraries-and-open-source",
    "href": "01-basics.html#libraries-and-open-source",
    "title": "2  Programming Basics",
    "section": "2.4 Libraries and Open-Source",
    "text": "2.4 Libraries and Open-Source\n\n2.4.1 Open-source languages\nOne of the great things about python is that it is an open-source language. This means that it was and is developed by individuals in a community rather than a private company, and the core code of it is visible to everyone.\nThe major consequences are:\n\nIt is free for anyone to use, rather than behind a paywall. (SAS or Java are examples of languages produced by private companies that require paid licenses to use.)\nThe language grows quickly, and in many diverse ways, because anyone at all can write their own programs. (You will write functions in a couple weeks!)\nYou are not allowed to sell your code for profit. (You can still write private code to help your company with a task - but you may not charge money to others for the programs themselves.)\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nWe believe very strongly in the philosophy of open-source. However, it does have its downsides: mainly, that nearly all progress in the language is on a volunteer, community basis.\nAs a user of open source tools, we hope you will give back in whatever ways you can - sharing your work publicly, helping others learn, and encouraging private companies to fund open-source work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nThis very recent article, about the role of open source in today’s world of AI and social media, is quite interesting!\n\n\n\n\n\n2.4.2 Libraries\nWhen an open-source developer creates a new collection of functions and capabilities for python, and they want it to be easily usable and accessible to others, they bundle their code into a library. (You will sometimes here this called a package.)\nPackages that meet certain standards of quality and formatting are added to the Python Package Index, after which they can be esailly installed with pip (“package installer for python”).\nMost of the packages we will use in this class actually come pre-installed with Anaconda, so we won’t have to worry about this too much.\n\n\n\n\n\n\nCheck-in\n\n\n\nOne package we need that is not pre-installed is plotnine.\nOpen up either a terminal or a Jupyter notebook in Anaconda. Then type\npip install plotnine\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nPython is notoriously frustrating for managine package installs. We will keep things simple in this class, but if you reach a point where you are struggling with libraries, know that you are not alone.\n\n\n\n\n\n\n\n\n\n\n2.4.3 Using library functions\nWhen you want to use functions from a library in your current code project, you have two options:\n\n2.4.3.1 1. Import the whole library\nIt is possible to load the full functionality of a library into your notebook project by adding an import statement in your very first code chunk.\nThe downside of this is that you then need to reference all those functions using the package name:\n\nimport numpy\n\nmy_nums = numpy.array([1,2,3,4,5])\nnumpy.sum(my_nums)\n\n15\n\n\nBecause this can get tedious, it’s common practice to give the package a “nickname” that is shorter:\n\nimport numpy as np\nmy_nums = np.array([1,2,3,4,5])\nnp.sum(my_nums)\n\n15\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nThe reason for needing to use the library names is that nothing stops two developers from choosing the same name for their function. Python needs a way to know which library’s function you intended to use.\n\n\n\n\n\n2.4.3.2 2. Import only the functions you need.\nIf you only need a handful of functions from the library, and you want to avoid the extra typing of including the package name/nickname, you can pull those functions in directly:\n\nfrom numpy import array, sum\n\nmy_nums = array([1,2,3,4,5])\nsum(my_nums)\n\n15"
  },
  {
    "objectID": "01-basics.html#data-frames",
    "href": "01-basics.html#data-frames",
    "title": "2  Programming Basics",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\nSince we are interested in using python specifically for data analysis, we will mention one more important Data Structure: a data frame.\nUnlike lists (which can contain anything at all) or matrices (which must store all the same type of data), data frames are restricted by column. That is, every data entry within a single column must be the same type; but two columns in the same data frame can have two different types.\nOne way to think of a data frame is as a list of vectors that all have the same length.\n\n2.5.0.1 Pandas\nAs with vectors and matrices, we need help from an external package to construct and work efficiently with data frames. This library is called pandas, and you will learn many of its functions next week.\nFor now, let’s just look at a pandas data frame:\n\nimport pandas as pd\n\ndat = pd.read_csv(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\")\n\ndat\n\ndat.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 9 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   rowid              344 non-null    int64  \n 1   species            344 non-null    object \n 2   island             344 non-null    object \n 3   bill_length_mm     342 non-null    float64\n 4   bill_depth_mm      342 non-null    float64\n 5   flipper_length_mm  342 non-null    float64\n 6   body_mass_g        342 non-null    float64\n 7   sex                333 non-null    object \n 8   year               344 non-null    int64  \ndtypes: float64(4), int64(2), object(3)\nmemory usage: 24.3+ KB\n\n\nNotice how the columns all have specific types: integers, floats, or strings (“object”). They also each have names. We can access the vector of information in one column like so…\n\ndat.body_mass_g\n\n0      3750.0\n1      3800.0\n2      3250.0\n3         NaN\n4      3450.0\n        ...  \n339    4000.0\n340    3400.0\n341    3775.0\n342    4100.0\n343    3775.0\nName: body_mass_g, Length: 344, dtype: float64\n\n\n… which then lets us do things to that column vector just as we might for standalone vectors:\n\n## using methods\ndat.body_mass_g.mean()\n\n## editing elements\ndat.body_mass_g[0] = 10000000\ndat.body_mass_g\n\n## boolean masking\nbig_penguins = dat.body_mass_g > 6000\ndat.loc[big_penguins]\n\n\n\n\n\n  \n    \n      \n      rowid\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      1\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      10000000.0\n      male\n      2007\n    \n    \n      169\n      170\n      Gentoo\n      Biscoe\n      49.2\n      15.2\n      221.0\n      6300.0\n      male\n      2007\n    \n    \n      185\n      186\n      Gentoo\n      Biscoe\n      59.6\n      17.0\n      230.0\n      6050.0\n      male\n      2007"
  },
  {
    "objectID": "01-basics.html#summary",
    "href": "01-basics.html#summary",
    "title": "2  Programming Basics",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nWhew! How’s that for an overview?\n\n\n\n\n\nThe most important takeaways from this chapter are:\n\nObjects in python have types, and sometimes functions and operators behave differently based on the type.\nFunctions have both optional and required arguments. They take input and produce output.\nData can be stored in multiple different structures. The choice of structure depends on the dimensionality (1D or 2D) and the homogeneity (do all elements need to be the same type?)\nWe use indexing to access (and edit) individual elements or sections of data structures.\nWe use boolean masking to find only the elements of a vector, matrix, or data frame that meet a particular qualification.\npython is an open-source language. We will import many different libraries to add to our basic functionality.\n\n\n2.6.1 Practice Exercise\n\n\n\n\n\n\nPractice-exercise\n\n\n\nUse your new knowledge of objects, types, and common coding errors to solve this puzzle"
  },
  {
    "objectID": "02-plotnine.html",
    "href": "02-plotnine.html",
    "title": "3  Data Visualization in Python",
    "section": "",
    "text": "This document demonstrates the use of the plotnine library in Python to visualize data via the grammar of graphics framework.\nThe functions in plotnine originate from the ggplot2 R package, which is the R implementation of the grammar of graphics."
  },
  {
    "objectID": "02-plotnine.html#grammar-of-graphics",
    "href": "02-plotnine.html#grammar-of-graphics",
    "title": "3  Data Visualization in Python",
    "section": "3.2 Grammar of Graphics",
    "text": "3.2 Grammar of Graphics\nThe grammar of graphics is a framework for creating data visualizations.\nA visualization consists of:\n\nThe aesthetic: Which variables are dictating which plot elements.\nThe geometry: What shape of plot your are making.\n\nFor example, the plot below displays some of the data from the Palmer Penguins data set.\nFirst, though, we need to load the Palmer Penguins dataset.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Same for any other libraries you haven’t installed.\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom plotnine import ggplot, geom_point, aes, geom_boxplot\n\npenguins = load_penguins()\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nThe aesthetic is species on the x-axis, bill_length_mm on the y-axis, colored by species.\nThe geometry is a boxplot.\n\n\n\n\n\n\nCheck-in\n\n\n\nTake a look at the first page of the optional reading for plotnine. In groups of 3-4, discuss the differences between how they use plotnine and the way we used it in the code chunk above."
  },
  {
    "objectID": "02-plotnine.html#plotnine-i.e.-ggplot",
    "href": "02-plotnine.html#plotnine-i.e.-ggplot",
    "title": "3  Data Visualization in Python",
    "section": "3.3 plotnine (i.e. ggplot)",
    "text": "3.3 plotnine (i.e. ggplot)\nThe plotnine library implements the grammar of graphics in Python.\nCode for the previous example:\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n3.3.1 The aesthetic\n\n(ggplot(penguins, \naes(                           # <1>\n  x = \"species\",               # <2>\n  y = \"bill_length_mm\",        # <2>\n  fill = \"species\"))           # <2>\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\nThe aes() function is the place to specify aesthetics.\nx, y, and fill are three possible aesthetics that can be specified, that map variables in our data set to plot elements.\n\n\n\n3.3.2 The geometry\n\n(ggplot(penguins, \naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"species\"))\n+ geom_boxplot() # <1>\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\nA variety of geom_* functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)\n\n\n\n3.3.3 Other optional elements:\n\nThe scales of the x- and y-axes.\nThe color of elements that are not mapped to aesthetics.\nThe theme of the plot\n\n…and many more!\n\n\n3.3.4 Scales\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\nfrom plotnine import scale_y_reverse\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ scale_y_reverse()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.3.5 Non-aesthetic colors\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot(fill = \"cornflowerblue\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat will this show?\n\n\n(ggplot(penguins, \naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"cornflowerblue\"))\n+ geom_boxplot()\n)\n\n\n3.3.6 Themes\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\nfrom plotnine import theme_classic\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ theme_classic()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nExample\n\n\n\nWhat are the differences between the two plots above? What did the theme change?\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat are the aesthetics, geometry, scales, and other options in the cartoon plot below?\n\n\n\nAn xkcd comic of time spent going up the down escalator\n\n\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\nScales: https://ggplot2-book.org/scale-position.html\nThemes: https://ggplot2-book.org/polishing.html"
  },
  {
    "objectID": "02-plotnine.html#geometries-the-big-five",
    "href": "02-plotnine.html#geometries-the-big-five",
    "title": "3  Data Visualization in Python",
    "section": "3.4 Geometries: The “Big Five”",
    "text": "3.4 Geometries: The “Big Five”\n\n3.4.1 1. Bar Plots\nMost often used for showing counts of a categorical variable:\n\nfrom plotnine import geom_bar\n(ggplot(penguins,\naes(\n  x = \"species\"\n))\n+ geom_bar()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n… or relationships between two categorical variables:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"sex\"\n))\n+ geom_bar()\n)\n\nTypeError: '<' not supported between instances of 'str' and 'float'\n\n\nWould we rather see percents?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"fill\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nOr side-by-side?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"dodge\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nExample\n\n\n\nCompare and contrast the plots above? What information is lost or gained between each of them?\n\n\n\n\n3.4.2 2. Boxplots\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nSide-by-side using a categorical variable:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"sex\"\n))\n+ geom_boxplot()\n)\n\nTypeError: '<' not supported between instances of 'str' and 'float'\n\n\n\n\n3.4.3 3. Histograms\n\nfrom plotnine import geom_histogram\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 100)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 10)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.4 3.5 Densities\nSuppose you want to compare histograms by category:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_histogram()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nCleaner: smoothed histogram, or density:\n\nfrom plotnine import geom_density\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nEven cleaner: The alpha option:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density(alpha = 0.5)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.5 4. Scatterplots\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\"\n))\n+ geom_point()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nColors for extra information:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\",\n  color = \"species\"\n))\n+ geom_point()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.6 5. Line Plots\n\nfrom plotnine import geom_line\npenguins2 = penguins.groupby(by = [\"species\", \"sex\"]).mean()\n\n(ggplot(penguins2,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  color = \"sex\"\n))\n+ geom_point()\n+ geom_line()\n)\n\nTypeError: agg function failed [how->mean,dtype->object]\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\nggplot2 cheatsheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nplotnine: https://plotnine.readthedocs.io/en/stable/"
  },
  {
    "objectID": "02-plotnine.html#multiple-plots",
    "href": "02-plotnine.html#multiple-plots",
    "title": "3  Data Visualization in Python",
    "section": "3.5 Multiple Plots",
    "text": "3.5 Multiple Plots\n\n3.5.1 Facet Wrapping\n\nfrom plotnine import facet_wrap\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n+ facet_wrap(\"sex\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question."
  },
  {
    "objectID": "03-basic_data_operations.html",
    "href": "03-basic_data_operations.html",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "",
    "text": "This document demonstrates the use of the pandas library in Python to do basic data wrangling and summarization.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session, for packages that don’t come pre-installed)."
  },
  {
    "objectID": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "href": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.2 Reading Tabular Data into Python",
    "text": "4.2 Reading Tabular Data into Python\nWe’re going to be exploring pandas in the context of the famous Titanic dataset. We’ll work with a subset of this dataset, but more information about it all can be found here.\nWe start by loading the numpy and pandas libraries. Most of our data wrangling work will happen with functions from the pandas library, but the numpy library will be useful for performing certain mathematical operations should we choose to transform any of our data.\nimport numpy as np\nimport pandas as pd\ndata_dir = \"https://dlsun.github.io/pods/data/\"\ndf_titanic = pd.read_csv(data_dir + \"titanic.csv\")\n\n\n\n\n\n\nExample\n\n\n\nWe’ve already seen read_csv() used many times for importing CSV files into Python, but it bears repeating.\n\n\nData files of many different types and shapes can be read into Python with similar functions, but we will focus on tabular data.\n\n4.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you, navigate to the GapMinder Data site and download a single CSV file of your choice. Open it up in Excel or your application of choice. Is this dataset tidy? If not, then what would have to change to make it tidy?\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\nThe term “tidy data” was first popularized in this paper by R developer Hadley Wickham.\n\n\nYou may have noticed that plotnine (ggplot) is basically built to take tidy data. Variables are specified in the aesthetics function to map them (i.e. columns) in our dataset to plot elements. This type of behavior is EXTREMELY common among functions that work with data in all languages, and so the importance of getting our data into a tidy format cannot be overstated.\nIn Python, there are at least two quick ways to view a dataset we’ve read in:\n\ndf_titanic\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.11\n      0\n    \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      2\n      Abbott, Mr. Rossmore Edward\n      male\n      16.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.13\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2202\n      Wynn, Mr. Walter\n      male\n      41.0\n      deck crew\n      B\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2203\n      Yearsley, Mr. Harry\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2204\n      Young, Mr. Francis James\n      male\n      32.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2205\n      Zanetti, Sig. Minio\n      male\n      20.0\n      restaurant staff\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2206\n      Zarracchi, Sig. L.\n      male\n      26.0\n      restaurant staff\n      S\n      England\n      NaN\n      NaN\n      0\n    \n  \n\n2207 rows × 9 columns\n\n\n\n\ndf_titanic.head()\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.11\n      0\n    \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      2\n      Abbott, Mr. Rossmore Edward\n      male\n      16.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.13\n      1\n    \n  \n\n\n\n\nThe latter (.head()) is usually preferred in case the dataset is large.\n\n\n\n\n\n\nCheck-in\n\n\n\nDoes the titanic dataset appear to be in tidy format?"
  },
  {
    "objectID": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "href": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.3 The “Big Five” Verbs of Data Wrangling",
    "text": "4.3 The “Big Five” Verbs of Data Wrangling\nData wrangling can involve a lot of different steps and operations to get data into a tidy format and ready for analysis and visualization. The vast majority of these fall under the umbrella one the following five operations:\n\nSelect columns/variables of interest\nFilter rows/observations of interest\nArrange the rows of a dataset by column(s) of interest (i.e. order or sort)\nMutate the columns of a dataset (i.e. create or transform variables)\nSummarize the rows of a dataset for column(s) of interest\n\n\n4.3.1 Select Columns/Variables\nSuppose we want to select the age variable from the titanic DataFrame. There are three ways to do this.\n\nUse .loc, specifying both the rows and columns. (The colon : is Python shorthand for “all”.)\n\ndf_titanic.loc[:, \"age\"]\n\nAccess the column as you would a key in a dict.\n\ndf_titanic[\"age\"]\n\nAccess the column as an attribute of the DataFrame.\n\ndf_titanic.age\nMethod 3 (attribute access) is the most concise. However, it does not work if the variable name contains spaces or special characters, begins with a number, or matches an existing attribute of the DataFrame. So, methods 1 and 2 are usually safer and preferred.\nTo select multiple columns, you would pass in a list of variable names, instead of a single variable name. For example, to select both age and fare, either of the two methods below would work (and produce the same result):\n# Method 1\ndf_titanic.loc[:, [\"age\", \"fare\"]].head()\n\n# Method 2\ndf_titanic[[\"age\", \"fare\"]].head()\n\n\n4.3.2 Filter Rows/Observations\n\n4.3.2.1 Selecting Rows/Observations by Location\nBefore we see how to filter (i.e. subset) the rows of dataset based on some condition, let’s see how to select rows by explicitly identifying them.\nWe can select a row by its position using the .iloc attribute. Keeping in mind that the first row is actually row 0, the fourth row could be extracted as:\n\ndf_titanic.iloc[3]\n\nname        Abbott, Mrs. Rhoda Mary 'Rosa'\ngender                              female\nage                                   39.0\nclass                                  3rd\nembarked                                 S\ncountry                            England\nticketno                            2673.0\nfare                                 20.05\nsurvived                                 1\nName: 3, dtype: object\n\n\nNotice that a single row from a DataFrame is no longer a DataFrame but a different data structure, called a Series.\nWe can also select multiple rows by passing a list of positions to .iloc.\n\ndf_titanic.iloc[[1, 3]]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n  \n\n\n\n\nNotice that when we select multiple rows, we get a DataFrame back.\nSo a Series is used to store a single observation (across multiple variables), while a DataFrame is used to store multiple observations (across multiple variables).\nIf selecting consecutive rows, we can use Python’s slice notation. For example, the code below selects all rows from the fourth row, up to (but not including) the tenth row.\n\ndf_titanic.iloc[3:9]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.1300\n      1\n    \n    \n      5\n      Abelseth, Mr. Olaus Jørgensen\n      male\n      25.0\n      3rd\n      S\n      United States\n      348122.0\n      7.1300\n      1\n    \n    \n      6\n      Abelson, Mr. Samuel\n      male\n      30.0\n      2nd\n      C\n      France\n      3381.0\n      24.0000\n      0\n    \n    \n      7\n      Abelson, Mrs. Hannah\n      female\n      28.0\n      2nd\n      C\n      France\n      3381.0\n      24.0000\n      1\n    \n    \n      8\n      Abī-Al-Munà, Mr. Nāsīf Qāsim\n      male\n      27.0\n      3rd\n      C\n      Lebanon\n      2699.0\n      18.1509\n      1\n    \n  \n\n\n\n\n\n\n4.3.2.2 Selecting Rows/Observations by Condition\nWe’ll often want to filter or subset the rows of a dataset based on some condition. To do this we’ll take advantage of vectorization and boolean masking.\nRecall that we can compare the values of a variable/column to a particular value in the following way, and observe the result.\n\ndf_titanic[\"age\"] > 30\n\n0        True\n1       False\n2       False\n3        True\n4       False\n        ...  \n2202     True\n2203     True\n2204     True\n2205    False\n2206    False\nName: age, Length: 2207, dtype: bool\n\n\nWe can use these True and False values to filter/subset the dataset! The following subsets the titanic dataset down to only those individuals (rows) with ages over 30.\n\ndf_titanic[df_titanic[\"age\"] > 30]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.1100\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      12\n      Ahlin, Mrs. Johanna Persdotter\n      female\n      40.0\n      3rd\n      S\n      Sweden\n      7546.0\n      9.0906\n      0\n    \n    \n      15\n      Aldworth, Mr. Augustus Henry\n      male\n      35.0\n      2nd\n      S\n      England\n      248744.0\n      13.0000\n      0\n    \n    \n      21\n      Allen, Mr. William Henry\n      male\n      39.0\n      3rd\n      S\n      England\n      373450.0\n      8.0100\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2197\n      Worthman, Mr. William Henry\n      male\n      37.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2200\n      Wright, Mr. William\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2202\n      Wynn, Mr. Walter\n      male\n      41.0\n      deck crew\n      B\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2203\n      Yearsley, Mr. Harry\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2204\n      Young, Mr. Francis James\n      male\n      32.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n  \n\n984 rows × 9 columns\n\n\n\nWe can combine multiple conditions using & (and) and | (or). The following subsets the titanic dataset down to females over 30 years of age.\n\ndf_titanic[(df_titanic[\"age\"] > 30) & (df_titanic[\"gender\"] == \"female\")]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      12\n      Ahlin, Mrs. Johanna Persdotter\n      female\n      40.0\n      3rd\n      S\n      Sweden\n      7546.0\n      9.0906\n      0\n    \n    \n      35\n      Andersson, Miss. Ida Augusta Margareta\n      female\n      38.0\n      3rd\n      S\n      Sweden\n      347091.0\n      7.1506\n      0\n    \n    \n      40\n      Andersson, Mrs. Alfrida Konstantia Brogren\n      female\n      39.0\n      3rd\n      S\n      Sweden\n      347082.0\n      31.0506\n      0\n    \n    \n      44\n      Andrews, Miss. Kornelia Theodosia\n      female\n      62.0\n      1st\n      C\n      United States\n      13502.0\n      77.1902\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1997\n      Robinson, Mrs. Annie\n      female\n      41.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2059\n      Smith, Miss. Katherine Elizabeth\n      female\n      45.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2076\n      Stap, Miss. Sarah Agnes\n      female\n      47.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2143\n      Wallis, Mrs. Catherine Jane\n      female\n      36.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2145\n      Walsh, Miss. Catherine\n      female\n      32.0\n      victualling crew\n      S\n      Ireland\n      NaN\n      NaN\n      0\n    \n  \n\n206 rows × 9 columns\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith the 2-3 people around you, how would you find the just the names of the males under 20 years of age who survived (in the titanic dataset) with a single line of code?\n\n\n\n\n\n4.3.3 Arrange Rows\nAs part of exploratory data analysis and some reporting efforts, we will want to sort a dataset or set of results by one or more variables of interest.\nWe can do this with .sort_values in either ascending or descending order.\nThe following sorts the titanic dataset by age in decreasing order.\n\ndf_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      1176\n      Svensson, Mr. Johan\n      male\n      74.000000\n      3rd\n      S\n      Sweden\n      347060.0\n      7.1506\n      0\n    \n    \n      820\n      Mitchell, Mr. Henry Michael\n      male\n      72.000000\n      2nd\n      S\n      England\n      24580.0\n      10.1000\n      0\n    \n    \n      53\n      Artagaveytia, Mr. Ramon\n      male\n      71.000000\n      1st\n      C\n      Argentina\n      17609.0\n      49.1001\n      0\n    \n    \n      456\n      Goldschmidt, Mr. George B.\n      male\n      71.000000\n      1st\n      C\n      United States\n      17754.0\n      34.1301\n      0\n    \n    \n      282\n      Crosby, Captain. Edward Gifford\n      male\n      70.000000\n      1st\n      S\n      United States\n      5735.0\n      71.0000\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      Tannūs, Master. As'ad\n      male\n      0.416667\n      3rd\n      C\n      Lebanon\n      2625.0\n      8.1004\n      1\n    \n    \n      296\n      Danbom, Master. Gilbert Sigvard Emanuel\n      male\n      0.333333\n      3rd\n      S\n      Sweden\n      347080.0\n      14.0800\n      0\n    \n    \n      316\n      Dean, Miss. Elizabeth Gladys 'Millvina'\n      female\n      0.166667\n      3rd\n      S\n      England\n      2315.0\n      20.1106\n      1\n    \n    \n      439\n      Gheorgheff, Mr. Stanio\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349254.0\n      7.1711\n      0\n    \n    \n      677\n      Kraeff, Mr. Theodor\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349253.0\n      7.1711\n      0\n    \n  \n\n2207 rows × 9 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that in these last few sections, we have not made any permanent changes to the df_titanic object. We have only asked python do some selecting/filtering/sorting and then to print out the results, not save them.\nIf we wanted df_titanic to become permanently sorted by age, we would re-assign the object:\ndf_titanic = df_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou will sometimes see object reassignment happen in a different way, using an inplace = True argument, like this:\ndf_titanic.sort_values(by = [\"age\"], ascending=False, inplace=True)\nWe strongly recommend against this approach, for two reason:\n\nWhen an object is “overwritten” via reassignment, that’s a major decision; you lose the old version of the object. It should be made deliberately and obviously. The inplace argument is easy to miss when copying/editing code, so it can lead to accidental overwriting that is hard to keep track of.\nNot all functions of DataFrames have an inplace option. It can be frustrating to get into the habit of using it, only to find out the hard way that it’s not available half the time!\n\n\n\n\n\n4.3.4 Mutate Column(s)\nThe variables available to us in our original dataset contain all of the information we have access to, but the best insights may instead come from transformations of those variables.\n\n4.3.4.1 Transforming Quantitative Variables\nOne of the simplest reasons to want to transform a quantitative variable is to change the measurement units.\nHere we change the age of passengers from a value in years to a value in decades.\ndf_titanic[\"age\"] = df_titanic[\"age\"] / 10\nIf we have a quantitative variable that is particularly skewed, then it might be a good idea to transform the values of that variable…like taking the log of the values.\n\n\n\n\n\n\nNote\n\n\n\nThis was a strategy you saw employed with the GapMinder data!\n\n\nBelow is an example of taking the log of the fare variable. Notice that we’re making use of the numpy here to take the log.\ndf_titanic[\"fare\"] = np.log(df_titanic[\"fare\"])\nRemember that we can take advantage of vectorization here too. The following operation wouldn’t really make physical sense, but it’s an example of creating a new variable out of existing variables.\ndf_titanic[\"nonsense\"] = df_titanic[\"fare\"] / df_titanic[\"age\"]\nNote that we created the new variable, nonsense, by specifying on the left side of the = here and populating that column/variable via the expression on the right side of the =.\nWe could want to create a new variable by categorizing (or discretizing) the values of a quantitative variable (i.e. convert a quantitative variable to a categorical variable). We can do so with cut.\nIn the following, we create a new age_cat variable which represents whether a person is a child or an adult.\ndf_titanic[\"age_cat\"] = pd.cut(df_titanic[\"age\"],\n                              bins = [0, 18, 100],\n                              labels = [\"child\", \"adult\"])\n\n\n\n\n\n\nCheck-in\n\n\n\nConsider the four mutations we just performed. In which ones did we reassign a column of the dataset, thus replacing the old values with new ones? In which ones did we create a brand-new column, thus retaining the old column(s) that were involved in the calculation?\n\n\n\n\n4.3.4.2 Transforming Categorical Variables\nIn some situations, especially later with modeling, we’ll need to convert categorical variables (stored as text) into quantitative (often coded) variables. Binary categorical variables can be converted into quantitative variables by coding one category as 1 and the other category as 0. (In fact, the survived column in the titanic dataset has already been coded this way.) The easiest way to do this is to create a boolean mask. For example, to convert gender to a quantitative variable female, which is 1 if the passenger was female and 0 otherwise, we can do the following:\ndf_titanic[\"female\"] = 1 * (df_titanic[\"gender\"] == \"female\")\nWhat do we do about a categorical variable with more than twwo categories, like embarked, which has four categories? In general, a categorical variable with K categories can be converted into K separate 0/1 variables, or dummy variables. Each of the K dummy variables is an indicator for one of the K categories. That is, a dummy variable is 1 if the observation fell into its particular category and 0 otherwise.\nAlthough it is not difficult to create dummy variables manually, the easiest way to create them is the get_dummies() function in pandas.\n\npd.get_dummies(df_titanic[\"embarked\"])\n\n\n\n\n\n  \n    \n      \n      B\n      C\n      Q\n      S\n    \n  \n  \n    \n      1176\n      False\n      False\n      False\n      True\n    \n    \n      820\n      False\n      False\n      False\n      True\n    \n    \n      53\n      False\n      True\n      False\n      False\n    \n    \n      456\n      False\n      True\n      False\n      False\n    \n    \n      282\n      False\n      False\n      False\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      False\n      True\n      False\n      False\n    \n    \n      296\n      False\n      False\n      False\n      True\n    \n    \n      316\n      False\n      False\n      False\n      True\n    \n    \n      439\n      False\n      True\n      False\n      False\n    \n    \n      677\n      False\n      True\n      False\n      False\n    \n  \n\n2207 rows × 4 columns\n\n\n\nWe may also want to change the levels of a categorical variable. A categorical variable can be transformed by mapping its levels to new levels. For example, we may only be interested in whether a person on the titanic was a passenger or a crew member. The variable class is too detailed. We can create a new variable, type, that is derived from the existing variable class. Observations with a class of “1st”, “2nd”, or “3rd” get a value of “passenger”, while observations with a class of “victualling crew”, “engineering crew”, or “deck crew” get a value of “crew”.\n\ndf_titanic[\"type\"] = df_titanic[\"class\"].map({\n    \"1st\": \"passenger\",\n    \"2nd\": \"passenger\",\n    \"3rd\": \"passenger\",\n    \"victualling crew\": \"crew\",\n    \"engineering crew\": \"crew\",\n    \"deck crew\": \"crew\"\n})\n\ndf_titanic\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n      nonsense\n      age_cat\n      female\n      type\n    \n  \n  \n    \n      1176\n      Svensson, Mr. Johan\n      male\n      7.400000\n      3rd\n      S\n      Sweden\n      347060.0\n      1.967196\n      0\n      0.265837\n      child\n      0\n      passenger\n    \n    \n      820\n      Mitchell, Mr. Henry Michael\n      male\n      7.200000\n      2nd\n      S\n      England\n      24580.0\n      2.312535\n      0\n      0.321185\n      child\n      0\n      passenger\n    \n    \n      53\n      Artagaveytia, Mr. Ramon\n      male\n      7.100000\n      1st\n      C\n      Argentina\n      17609.0\n      3.893861\n      0\n      0.548431\n      child\n      0\n      passenger\n    \n    \n      456\n      Goldschmidt, Mr. George B.\n      male\n      7.100000\n      1st\n      C\n      United States\n      17754.0\n      3.530180\n      0\n      0.497208\n      child\n      0\n      passenger\n    \n    \n      282\n      Crosby, Captain. Edward Gifford\n      male\n      7.000000\n      1st\n      S\n      United States\n      5735.0\n      4.262680\n      0\n      0.608954\n      child\n      0\n      passenger\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      Tannūs, Master. As'ad\n      male\n      0.041667\n      3rd\n      C\n      Lebanon\n      2625.0\n      2.091913\n      1\n      50.205923\n      child\n      0\n      passenger\n    \n    \n      296\n      Danbom, Master. Gilbert Sigvard Emanuel\n      male\n      0.033333\n      3rd\n      S\n      Sweden\n      347080.0\n      2.644755\n      0\n      79.342661\n      child\n      0\n      passenger\n    \n    \n      316\n      Dean, Miss. Elizabeth Gladys 'Millvina'\n      female\n      0.016667\n      3rd\n      S\n      England\n      2315.0\n      3.001247\n      1\n      180.074822\n      child\n      1\n      passenger\n    \n    \n      439\n      Gheorgheff, Mr. Stanio\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349254.0\n      1.970059\n      0\n      NaN\n      NaN\n      0\n      passenger\n    \n    \n      677\n      Kraeff, Mr. Theodor\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349253.0\n      1.970059\n      0\n      NaN\n      NaN\n      0\n      passenger\n    \n  \n\n2207 rows × 13 columns\n\n\n\n\n\n\n4.3.5 Summarizing Rows\nSummarization of the rows of a dataset for column(s) of interest can take many different forms. This introduction will not be exhaustive, but certainly cover the basics.\n\n4.3.5.1 Summarizing a Quantitative Variable\nThere are a few descriptive statistics that can be computed directly including, but not limited to, the mean and median.\n\ndf_titanic[\"age\"].mean()\n\ndf_titanic[\"age\"].median()\n\ndf_titanic[[\"age\", \"fare\"]].mean()\n\nage     3.043673\nfare    2.918311\ndtype: float64\n\n\nWe can ask for a slightly more comprehensive description using .describe()\n\ndf_titanic[\"age\"].describe()\n\ndf_titanic.describe()\n\n\n\n\n\n  \n    \n      \n      age\n      ticketno\n      fare\n      survived\n      nonsense\n      female\n    \n  \n  \n    \n      count\n      2205.000000\n      1.316000e+03\n      1291.000000\n      2207.000000\n      1289.000000\n      2207.000000\n    \n    \n      mean\n      3.043673\n      2.842157e+05\n      2.918311\n      0.322157\n      2.147877\n      0.221568\n    \n    \n      std\n      1.215968\n      6.334726e+05\n      0.974452\n      0.467409\n      7.237694\n      0.415396\n    \n    \n      min\n      0.016667\n      2.000000e+00\n      1.108728\n      0.000000\n      0.265837\n      0.000000\n    \n    \n      25%\n      2.200000\n      1.426225e+04\n      1.971383\n      0.000000\n      0.742371\n      0.000000\n    \n    \n      50%\n      2.900000\n      1.114265e+05\n      2.645480\n      0.000000\n      0.936833\n      0.000000\n    \n    \n      75%\n      3.800000\n      3.470770e+05\n      3.435945\n      1.000000\n      1.260935\n      0.000000\n    \n    \n      max\n      7.400000\n      3.101317e+06\n      6.238443\n      1.000000\n      180.074822\n      1.000000\n    \n  \n\n\n\n\nNote that, by default, .describe() provides descriptive statistics for only the quantitative variables in the dataset.\nWe can enhance numerical summaries with .groupby(), which allows us to specify one or more variables that we’d like to group our work by.\n\ndf_titanic[[\"age\", \"survived\"]].groupby(\"survived\").mean()\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      survived\n      \n    \n  \n  \n    \n      0\n      3.083194\n    \n    \n      1\n      2.960631\n    \n  \n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you, look up how you would compute the correlation between two quantitative variables in Python. Compute the correlation between the age and fare variables in the titanic dataset.\n\n\n\n\n4.3.5.2 Summarizing a Categorical Variable\nWhen it comes to categorical variables we’re most often interested in frequency distributions (counts), relative frequency distributions, and cross-tabulations.\n\ndf_titanic[\"class\"].unique()\n\ndf_titanic[\"class\"].describe()\n\ncount     2207\nunique       7\ntop        3rd\nfreq       709\nName: class, dtype: object\n\n\nThe .unique() here allows us to see the unique values of the class variable. Notice that the results of .describe() on a categorical variable are much different.\nTo completely summarize a single categorical variable, we report the number of times each level appeared, or its frequency.\n\ndf_titanic[\"class\"].value_counts()\n\nclass\n3rd                 709\nvictualling crew    431\n1st                 324\nengineering crew    324\n2nd                 284\nrestaurant staff     69\ndeck crew            66\nName: count, dtype: int64\n\n\nInstead of reporting counts, we can also report proportions or probabilities, or the relative frequencies. We can calculate the relative frequencies by specifying normalize=True in .value_counts().\n\ndf_titanic[\"class\"].value_counts(normalize=True)\n\nclass\n3rd                 0.321251\nvictualling crew    0.195288\n1st                 0.146806\nengineering crew    0.146806\n2nd                 0.128681\nrestaurant staff    0.031264\ndeck crew           0.029905\nName: proportion, dtype: float64\n\n\nCross-tabulations are one way we can investigate possible relationships between categorical variables. For example, what can we say about the relationship between gender and survival on the Titanic?\n\n\n\n\n\n\nCheck-in\n\n\n\nSummarize gender and survival individually by computing the frequency distributions of each.\n\n\nThis does not tell us how gender interacts with survival. To do that, we need to produce a cross-tabulation, or a “cross-tab” for short. (Statisticians tend to call this a contingency table or a two-way table.)\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"])\n\n\n\n\n\n  \n    \n      gender\n      female\n      male\n    \n    \n      survived\n      \n      \n    \n  \n  \n    \n      0\n      130\n      1366\n    \n    \n      1\n      359\n      352\n    \n  \n\n\n\n\nA cross-tabulation of two categorical variables is a two-dimensional array, with the levels of one variable along the rows and the levels of the other variable along the columns. Each cell in this array contains the number of observations that had a particular combination of levels. So in the Titanic data set, there were 359 females who survived and 1366 males who died. From the cross-tabulation, we can see that there were more females who survived than not, while there were more males who died than not. Clearly, gender had a strong influence on survival because of the Titanic’s policy of “women and children first”.\nTo get probabilities instead of counts, we specify normalize=True.\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"], normalize=True)\n\n\n\n\n\n  \n    \n      gender\n      female\n      male\n    \n    \n      survived\n      \n      \n    \n  \n  \n    \n      0\n      0.058903\n      0.618940\n    \n    \n      1\n      0.162664\n      0.159493\n    \n  \n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat about conditional proportions? With 2-3 people around you, discuss how you would compute the proportion of females that survived and the proportion of males that survived and then do it.\nNote, there are multiple ways to do this.\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question."
  },
  {
    "objectID": "04-pivoting_joining.html",
    "href": "04-pivoting_joining.html",
    "title": "5  Pivoting and Joining",
    "section": "",
    "text": "This document demonstrates the use of the pandas library in Python to do pivoting and joining of datasets.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session).\n\n\nimport pandas as pd\n# Population data from GapMinder\npopulation = pd.read_csv(\"/content/pop.csv\")"
  },
  {
    "objectID": "04-pivoting_joining.html#pivoting-data-in-python",
    "href": "04-pivoting_joining.html#pivoting-data-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.2 Pivoting Data in Python",
    "text": "5.2 Pivoting Data in Python\nData come in all shapes and forms! Rare is the day when we can open a dataset for the first time and it’s ready for every type of visualization or analysis that we could want to do with it.\nIn addition to the wrangling we discussed in the previous chapter, there may be a need to reshape the dataset entirely. For example, the column names might be values themselves that we want to make use of.\nRecall our introduction of tidy data in the previous chapter…\n\n5.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\nIn the previous chapter you were asked to open up a GapMinder dataset here and to comment on whether this dataset was tidy or not. The answer was no, this dataset is not tidy. These datasets come with a row representing a country, each column representing a year, and each cell representing the value of the global indicator selected. To be tidy these three variables (country, year, global indicator) should each have their own column, instead of the year variable taking values as the column headers.\n\n\n5.2.2 Wide to Long Format\nThe GapMinder dataset is an example of what’s commonly referred to as data in a wide format. To make this dataset tidy we aim for a dataset with columns for country, year, and global indicator (e.g. population). Three columns is many fewer than the current number of columns, and so we will convert this dataset from wide to long format.\n\n\n\n\n\n\nWarning\n\n\n\nIt often helps to physically draw/map out what our current dataset looks like and what the look of our target dataset is, before actually trying to write any code to do this. Writing the code can be extremely easier after this exercise, and only makes future pivot operations easier.\n\n\nIn order to convert our dataset from wide to long format we will use .melt() (or .wide_to_long()) in pandas.\nlong_population = population.melt(id_vars=[\"country\"], var_name=\"year\", value_name=\"population\")\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you navigate to GapMinder, download the population dataset, and convert it from wide to long format. Does the result look how you expect? Is any other wrangling necessary?\n\n\n\n\n5.2.3 Long to Wide Format\nEven though certain data shapes are not considered tidy, they may be more conducive to performing certain operations than other shapes. For example, what if we were interested in the change in country population between 1950 and 2010? In the original wide shape of the GapMinder data this operation would have been a simple difference of columns like below.\npopulation[\"pop_diff\"] = population[\"2010\"] - population[\"1950\"]\n\n\n\n\n\n\nCheck-in\n\n\n\nWhy doesn’t the above code work without further wrangling? What in the dataset needs to change for this operation to work?\n\n\nIn the long format of our Gapminder dataset (long_population), this operation is less straightforward. Sometimes datasets come to us in long format and to do things like the operation above we need to convert that dataset from long to wide format. We can go the reverse direction (i.e. long to wide format) with .pivot() in pandas.\nwide_population = long_population.pivot(index = \"country\", columns = \"year\", values = \"population\")\nwide_population = wide_population.reset_index()\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nWe haven’t spent much time discussing the index of a pandas DataFrame, but you can think of it like an address for data, or slices of data in a DataFrame. You can also think of an index (or indices) as row names, or axis labels, for your dataset. This can be useful for a number of functions in Python, and can enhance the look of results or visualizations.\nHowever, understanding them is not critical for what we will do in Python. Furthermore, variables that are indices for a DataFrame cannot be accessed or referenced in the same way as other variables in the DataFrame. So, we will avoid their use if possible."
  },
  {
    "objectID": "04-pivoting_joining.html#joining-datasets-in-python",
    "href": "04-pivoting_joining.html#joining-datasets-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.3 Joining Datasets in Python",
    "text": "5.3 Joining Datasets in Python\nThe information you need is often spread across multiple data sets, so you will need to combine multiple data sets into one. In this chapter, we discuss strategies for combining information from multiple (tabular) data sets.\nAs a working example, we will use a data set of baby names collected by the Social Security Administration. Each data set in this collection contains the names of all babies born in the United States in a particular year. This data is publicly available, and a copy has been made available at https://dlsun.github.io/pods/data/names/.\n\n5.3.1 Concatenating and Merging Data\n\n5.3.1.1 Concatenation\nSometimes, the rows of data are spread across multiple files, and we want to combine the rows into a single data set. The process of combining rows from different data sets is known as concatenation.\nVisually, to concatenate two DataFrames, we simply stack them on top of one another.\nFor example, suppose we want to understand how the popularity of different names evolved between 1995 and 2015. The 1995 names and the 2015 names are stored in two different files: yob1995.txt and yob2015.txt, respectively. To carry out this analysis, we will need to combine these two data sets into one.\n\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames1995\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n    \n    \n      1\n      Ashley\n      F\n      26603\n    \n    \n      2\n      Emily\n      F\n      24378\n    \n    \n      3\n      Samantha\n      F\n      21646\n    \n    \n      4\n      Sarah\n      F\n      21369\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      26075\n      Zerek\n      M\n      5\n    \n    \n      26076\n      Zhen\n      M\n      5\n    \n    \n      26077\n      Ziggy\n      M\n      5\n    \n    \n      26078\n      Zuberi\n      M\n      5\n    \n    \n      26079\n      Zyon\n      M\n      5\n    \n  \n\n26080 rows × 3 columns\n\n\n\n\nnames2015 = pd.read_csv(data_dir + \"yob2015.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames2015\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Emma\n      F\n      20455\n    \n    \n      1\n      Olivia\n      F\n      19691\n    \n    \n      2\n      Sophia\n      F\n      17417\n    \n    \n      3\n      Ava\n      F\n      16378\n    \n    \n      4\n      Isabella\n      F\n      15617\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      33116\n      Zykell\n      M\n      5\n    \n    \n      33117\n      Zyking\n      M\n      5\n    \n    \n      33118\n      Zykir\n      M\n      5\n    \n    \n      33119\n      Zyrus\n      M\n      5\n    \n    \n      33120\n      Zyus\n      M\n      5\n    \n  \n\n33121 rows × 3 columns\n\n\n\nTo concatenate the two, we use the pd.concat() function, which accepts a list of pandas objects (DataFrames or Series) and concatenates them.\n\npd.concat([names1995, names2015])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n    \n    \n      1\n      Ashley\n      F\n      26603\n    \n    \n      2\n      Emily\n      F\n      24378\n    \n    \n      3\n      Samantha\n      F\n      21646\n    \n    \n      4\n      Sarah\n      F\n      21369\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      33116\n      Zykell\n      M\n      5\n    \n    \n      33117\n      Zyking\n      M\n      5\n    \n    \n      33118\n      Zykir\n      M\n      5\n    \n    \n      33119\n      Zyrus\n      M\n      5\n    \n    \n      33120\n      Zyus\n      M\n      5\n    \n  \n\n59201 rows × 3 columns\n\n\n\n\nThere is no longer any way to distinguish the 1995 data from the 2015 data. To fix this, we can add a Year column to each DataFrame before we concatenate.\nThe indexes from the original DataFrames are preserved in the concatenated DataFrame. (To see this, observe that the last index in the DataFrame is about 33000, which corresponds to the number of rows in names2015, even though there are 59000 rows in the DataFrame.) That means that there are two rows with an index of 0, two rows with an index of 1, and so on. To force pandas to generate a completely new index for this DataFrame, ignoring the indices from the original DataFrames, we specify ignore_index=True.\n\n\nnames1995[\"Year\"] = 1995\nnames2015[\"Year\"] = 2015\nnames = pd.concat([names1995, names2015], ignore_index=True)\nnames\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      59196\n      Zykell\n      M\n      5\n      2015\n    \n    \n      59197\n      Zyking\n      M\n      5\n      2015\n    \n    \n      59198\n      Zykir\n      M\n      5\n      2015\n    \n    \n      59199\n      Zyrus\n      M\n      5\n      2015\n    \n    \n      59200\n      Zyus\n      M\n      5\n      2015\n    \n  \n\n59201 rows × 4 columns\n\n\n\nNow this is a DataFrame we can use!\n\n\n5.3.1.2 Merging (a.k.a Joining)\nMore commonly, the data sets that we want to combine actually contain different information about the same observations. In other words, instead of stacking the DataFrames on top of each other, as in concatenation, we want to stack them next to each other. The process of combining columns or variables from different data sets is known as merging or joining.\nThe observations may be in a different order in the two data sets, so merging is not as simple as placing the two DataFrames side-by-side.\nMerging is an operation on two DataFrames that returns a third DataFrame. By convention, the first DataFrame is referred to as the one on the “left”, while the second DataFrame is the one on the “right”.\nThis naming convention is reflected in the syntax of the .merge() function in pandas. In the code below, the “left” DataFrame, names1995, is quite literally on the left in the code, while the “right” DataFrame, names2015, is to the right. We also specify the variables to match across the two DataFrames.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Year_x\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      148\n      2015\n    \n  \n\n15680 rows × 6 columns\n\n\n\nThe most important component of merging two datasets is the presence of at least one key variable that both datasets share. This variable is sometimes referred to as an ID variable. It’s this variable that we will want to merge on, i.e. use to combine the two datasets intelligently.\nThe variables that we joined on (Name and Sex) appear once in the final DataFrame. The variable Count, which we did not join on, appears twice—since there was a column called Count in both of the original DataFrames. Notice that pandas automatically appended the suffix _x to the name of the variable from the left DataFrame and _y to the one from the right DataFrame. We can customize the suffixes by specifying the suffixes= parameter.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"], suffixes=(\"1995\", \"2015\"))\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count1995\n      Year1995\n      Count2015\n      Year2015\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      148\n      2015\n    \n  \n\n15680 rows × 6 columns\n\n\n\nIn the code above, we assumed that the columns that we joined on had the same names in the two data sets. What if they had different names? For example, suppose the variable had been called Sex in one data set and Gender in the other. We can specify which variables to use from the left and right data sets using the left_on= and right_on= parameters.\n\n# Create new DataFrames where the column names are different\nnames2015_ = names2015.rename({\"Sex\": \"Gender\"}, axis=1)\n\n# This is how you merge them.\nnames1995.merge(\n    names2015_,\n    left_on=(\"Name\", \"Sex\"),\n    right_on=(\"Name\", \"Gender\")\n)\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Year_x\n      Gender\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      F\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      F\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      F\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      F\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      F\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      M\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      M\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      M\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      M\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      M\n      148\n      2015\n    \n  \n\n15680 rows × 7 columns\n\n\n\n\n\n5.3.1.3 One-to-One and Many-to-One Relationships\nIn the example above, there was at most one combination of Name and Sex in the 2015 data set for each combination of Name and Sex in the 1995 data set. These two data sets are thus said to have a one-to-one relationship. The same would be true of combining two GapMinder datasets.\nHowever, two data sets need not have a one-to-one relationship! Two datasets could have a many-to-one relationship. In general, it’s extremely important to think carefully about what variables each of your two datasets have to begin with, and what variables you want your merged dataset to have…and what that merged dataset will represent with respect to your data.\n\n\n5.3.1.4 Many-to-Many Relationships: A Cautionary Tale\nIt is also possible for multiple rows in the left DataFrame to match multiple rows in the right DataFrame. In this case, the two data sets are said to have a many-to-many relationship. Many-to-many joins can lead to misleading analyses, so it is important to exercise caution when working with many-to-many relationships.\nFor example, in the baby names data set, the Name variable is not uniquely identifying. For example, there are both males and females with the name “Jessie”.\n\njessie1995 = names1995[names1995[\"Name\"] == \"Jessie\"]\njessie1995\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      248\n      Jessie\n      F\n      1138\n      1995\n    \n    \n      16047\n      Jessie\n      M\n      903\n      1995\n    \n  \n\n\n\n\n\njessie2015 = names2015[names2015[\"Name\"] == \"Jessie\"]\njessie2015\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      615\n      Jessie\n      F\n      469\n      2015\n    \n    \n      20009\n      Jessie\n      M\n      233\n      2015\n    \n  \n\n\n\n\nIf we join these two DataFrames on Name, then we will end up with a many-to-many join, since each “Jessie” row in the 1995 data will be paired with each “Jessie” row in the 2015 data.\n\njessie1995.merge(jessie2015, on=[\"Name\"])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex_x\n      Count_x\n      Year_x\n      Sex_y\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessie\n      F\n      1138\n      1995\n      F\n      469\n      2015\n    \n    \n      1\n      Jessie\n      F\n      1138\n      1995\n      M\n      233\n      2015\n    \n    \n      2\n      Jessie\n      M\n      903\n      1995\n      F\n      469\n      2015\n    \n    \n      3\n      Jessie\n      M\n      903\n      1995\n      M\n      233\n      2015\n    \n  \n\n\n\n\nNotice that Jessie ends up appearing four times:\n\nFemale Jessies from 1995 are matched with female Jessies from 2015. (Good!)\nMale Jessies from 1995 are matched with male Jessies from 2015. (Good!)\nFemale Jessies from 1995 are matched with male Jessies from 2015. (This is perhaps undesirable.)\nMale Jessies from 1995 are matched with female Jessies from 2015. (Also unexpected and undesirable.)\n\nIf we had used a data set like this to determine the number of Jessies in 1995, then we would end up with the wrong answer, since we would have double-counted both female and male Jessies as a result of the many-to-many join. This is why it is important to exercise caution when working with (potential) many-to-many relationships.\n\n\n\n5.3.2 Types of Joins\nAbove, we saw how to merge (or join) two data sets by matching on certain variables. But what happens when a row in one DataFrame has no match in the other?\nFirst, let’s investigate how pandas handles this situation by default. The name “Nevaeh”, which is “heaven” spelled backwards, took after Sonny Sandoval of the band P.O.D. gave his daughter the name in 2000. Let’s look at how common this name was five years earlier and five years after.\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\n\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\nnames2005 = pd.read_csv(data_dir + \"yob2005.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\n\nnames1995[names1995.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n  \n\n\n\n\n\nnames2005[names2005.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      68\n      Nevaeh\n      F\n      4552\n    \n    \n      21353\n      Nevaeh\n      M\n      56\n    \n  \n\n\n\n\nIn 1995, there were no girls (at least fewer than 5) named Nevaeh; just eight years later, there were over 4500 girls (and even 56 boys) with the name. It seems like Sonny Sandoval had a huge effect.\nWhat happens to the name “Nevaeh” when we merge the two data sets?\n\nnames = names1995.merge(names2005, on=[\"Name\", \"Sex\"])\nnames[names.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n  \n\n\n\n\nBy default, pandas only includes combinations that are present in both DataFrames. If it cannot find a match for a row in one DataFrame, then the combination is simply dropped.\nBut in this context, the fact that a name does not appear in one data set is informative. It means that no babies were born in that year with that name. We might want to include names that appeared in only one of the two DataFrames, rather than just the names that appeared in both.\nThere are four types of joins, distinguished by whether they include the rows from the left DataFrame, the right DataFrame, both, or neither:\n\ninner join (default): only values that are present in both DataFrames are included in the result\nouter join: any value that appears in either DataFrame is included in the result\nleft join: any value that appears in the left DataFrame is included in the result, whether or not it appears in the right DataFrame\nright join: any value that appears in the right DataFrame is included in the result, whether or not it appears in the left DataFrame.\n\nOne way to visualize the different types of joins is using Venn diagrams. The shaded region indicates which rows that are included in the output. For example, only rows that appear in both the left and right DataFrames are included in the output of an inner join.\n\nIn pandas, the join type is specified using the how= argument.\nNow let’s look at the examples of each of these types of joins.\n\n# inner join\nnames_inner = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"inner\")\nnames_inner\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      8108\n    \n    \n      1\n      Ashley\n      F\n      26603\n      13270\n    \n    \n      2\n      Emily\n      F\n      24378\n      23930\n    \n    \n      3\n      Samantha\n      F\n      21646\n      13633\n    \n    \n      4\n      Sarah\n      F\n      21369\n      11527\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      19119\n      Zeppelin\n      M\n      5\n      7\n    \n    \n      19120\n      Zerek\n      M\n      5\n      8\n    \n    \n      19121\n      Zhen\n      M\n      5\n      7\n    \n    \n      19122\n      Ziggy\n      M\n      5\n      6\n    \n    \n      19123\n      Zyon\n      M\n      5\n      102\n    \n  \n\n19124 rows × 4 columns\n\n\n\n\n# outer join\nnames_outer = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"outer\")\nnames_outer\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935.0\n      8108.0\n    \n    \n      1\n      Ashley\n      F\n      26603.0\n      13270.0\n    \n    \n      2\n      Emily\n      F\n      24378.0\n      23930.0\n    \n    \n      3\n      Samantha\n      F\n      21646.0\n      13633.0\n    \n    \n      4\n      Sarah\n      F\n      21369.0\n      11527.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      39490\n      Zymiere\n      M\n      NaN\n      5.0\n    \n    \n      39491\n      Zyrell\n      M\n      NaN\n      5.0\n    \n    \n      39492\n      Zyrian\n      M\n      NaN\n      5.0\n    \n    \n      39493\n      Zyshon\n      M\n      NaN\n      5.0\n    \n    \n      39494\n      Zytavious\n      M\n      NaN\n      5.0\n    \n  \n\n39495 rows × 4 columns\n\n\n\nNames like “Zyrell” and “Zyron” appeared in the 2005 data but not the 1995 data. For this reason, their count in 1995 is NaN. In general, there will be missing values in DataFrames that result from an outer join. Any time a value appears in one DataFrame but not the other, there will be NaNs in the columns from the DataFrame missing that value.\n\nnames_inner.isnull().sum()\n\nName       0\nSex        0\nCount_x    0\nCount_y    0\ndtype: int64\n\n\nLeft and right joins preserve data from one DataFrame but not the other. For example, if we were trying to calculate the percentage change for each name from 1995 to 2005, we would want to include all of the names that appeared in the 1995 data. If the name did not appear in the 2005 data, then that is informative.\n\n# left join\nnames_left = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"left\")\nnames_left\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      8108.0\n    \n    \n      1\n      Ashley\n      F\n      26603\n      13270.0\n    \n    \n      2\n      Emily\n      F\n      24378\n      23930.0\n    \n    \n      3\n      Samantha\n      F\n      21646\n      13633.0\n    \n    \n      4\n      Sarah\n      F\n      21369\n      11527.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      26075\n      Zerek\n      M\n      5\n      8.0\n    \n    \n      26076\n      Zhen\n      M\n      5\n      7.0\n    \n    \n      26077\n      Ziggy\n      M\n      5\n      6.0\n    \n    \n      26078\n      Zuberi\n      M\n      5\n      NaN\n    \n    \n      26079\n      Zyon\n      M\n      5\n      102.0\n    \n  \n\n26080 rows × 4 columns\n\n\n\nThe result of the left join has NaNs in the columns from the right DataFrame.\n\nnames_left.isnull().sum()\n\nName          0\nSex           0\nCount_x       0\nCount_y    6956\ndtype: int64\n\n\nThe result of the right join, on the other hand, has NaNs in the column from the left DataFrame.\n\n# right join\nnames_right = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"right\")\nnames_right\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Emily\n      F\n      24378.0\n      23930\n    \n    \n      1\n      Emma\n      F\n      5041.0\n      20335\n    \n    \n      2\n      Madison\n      F\n      9775.0\n      19562\n    \n    \n      3\n      Abigail\n      F\n      7821.0\n      15747\n    \n    \n      4\n      Olivia\n      F\n      7624.0\n      15691\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32534\n      Zymiere\n      M\n      NaN\n      5\n    \n    \n      32535\n      Zyrell\n      M\n      NaN\n      5\n    \n    \n      32536\n      Zyrian\n      M\n      NaN\n      5\n    \n    \n      32537\n      Zyshon\n      M\n      NaN\n      5\n    \n    \n      32538\n      Zytavious\n      M\n      NaN\n      5\n    \n  \n\n32539 rows × 4 columns\n\n\n\n\nnames_right.isnull().sum()\n\nName           0\nSex            0\nCount_x    13415\nCount_y        0\ndtype: int64\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload a second GapMinder dataset and merge it with the population dataset from above. Did you have to pivot first? Which order of operations makes the most sense? Is your resulting dataset tidy?\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nClick here to solve a riddle using data manipulation."
  },
  {
    "objectID": "05-function_writing.html",
    "href": "05-function_writing.html",
    "title": "6  Writing Custom Functions",
    "section": "",
    "text": "A function is a set of actions that we group together and name. Throughout this course, you’ve already used a bunch of different functions in python that are built into the language or added through packages: mean, ggplot, merge, etc. In this chapter, we’ll be writing our own functions."
  },
  {
    "objectID": "05-function_writing.html#defining-a-function",
    "href": "05-function_writing.html#defining-a-function",
    "title": "6  Writing Custom Functions",
    "section": "6.2 Defining a function",
    "text": "6.2 Defining a function\n\n6.2.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead of copy pasting. The motivation behind this is the “don’t repeat yourself” (DRY) principle. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\nConsider the following data analysis task done in python:\ndf = pd.DataFrame({'a':np.random.normal(1,1,10), 'b':np.random.normal(2,2,10), 'c':np.random.normal(3,3,10), 'd':np.random.normal(4,4,10)})\n\ndf['a'] = (df['a'] - min(df['a']))/(max(df['a']) - min(df['a']))\n\ndf['b'] = (df['b'] - min(df['a']))/(max(df['b']) - min(df['b']))\n\ndf['c'] = (df['c'] - min(df['c']))/(max(df['c']) - min(df['c']))\n\ndf['d'] = (df['d'] - min(df['d']))/(max(df['d']) - min(df['d']))\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat does this code do?\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nIn theory, the code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b!\nWriting a function will help us avoid these subtle copy-paste errors.\n\n\n\n\n\n6.2.2 Building up the function\nTo write a function, we first analyze the code to determine how many inputs it has\ndf['a'] = (df['a'] - min(df['a']))/(max(df['a']) - min(df['a']))\nThis code has only one input: df['a'].\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat is the object structure of this input?\n\n\nNow we choose an argument name for our new input. It’s nice if the argument name reminds the user of what type or structure of object is expected.\nIn this case, it might help to replace df$a with vec.\n\nvec = df['a'] \n\n(vec - min(vec))/(max(vec) - min(vec))\n\n0    0.712148\n1    0.440887\n2    0.711946\n3    0.868801\n4    1.000000\n5    0.397482\n6    0.160347\n7    0.662889\n8    0.000000\n9    0.975713\nName: a, dtype: float64\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times) or separating steps to avoid nested parentheses (for instance, computing max first).\n\nvec = df['a'] \nmin_vec = min(vec)\nmax_vec = max(vec)\n\n(vec - min_vec)/(max_vec - min_vec)\n\n0    0.712148\n1    0.440887\n2    0.711946\n3    0.868801\n4    1.000000\n5    0.397482\n6    0.160347\n7    0.662889\n8    0.000000\n9    0.975713\nName: a, dtype: float64\n\n\nFinally, we turn this code into a function with the def command:\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled_vec = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled_vec\n\nThe name of the function, rescale_vector, describes what the function does - it rescales a vector (pandas Series or numpy array).\nThe function takes one argument, named vec; any references to this value within the function will use vec as the name.\nThe code that actually does what your function is supposed to do goes in the body of the function, after the :. It is important for the body of the function to be indented.\nThe function returns the computed object you want to hand back to the user: in this case, rescaled_vec.\n\n\n\n\n\n\n\nNote\n\n\n\nSome people prefer to create a final object and then return: that object, as we have done above with rescaled_vec.\nOthers prefer fewer objects and longer lines of code, i.e.,\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  return (vec - min_vec)/(max_vec - min_vec)\nThese two approaches will work identically; it’s a matter of personal preference.\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.\nNow, we are able to use our function to avoid the repetition:\ndf['a'] = rescale_vector(df['a'])\ndf['b'] = rescale_vector(df['b'])\ndf['c'] = rescale_vector(df['c'])\ndf['d'] = rescale_vector(df['d'])\nYou probably notice there is still a little bit of repetition here, with df['a'] appearing on both the left and right side of the =. But this is good repetition! When we assign or reassign an object or column, we want that to be an obvious and deliberate choice.\nIt’s also possible that you might have preferred to keep your original column untouched, and to make new columns for the rescaled data:\ndf['a_scaled'] = rescale_vector(df['a'])\ndf['b_scaled'] = rescale_vector(df['b'])\ndf['c_scaled'] = rescale_vector(df['c'])\ndf['d_scaled'] = rescale_vector(df['d'])\n\n\n6.2.3 Documenting your function\nWhen you want to use a function in python, but you can’t quite remember exactly how it works, you might be in the habit of typing ?fun_name or help(fun_name) to be able to see the documentation for that function.\nWhen you write your own function - whether for yourself or for others - it’s important to provide reminders of what the function is for and how it works.\nWe do this by adding text in a very specific structure into the body of our function:\ndef rescale_vector(vec):\n  \n  \"\"\"\n  Rescales a numeric vector to have a max of 1 and min of 0\n  \n  Parameter\n  ---------\n  vec : num\n    A list, numpy array, or pandas Series of numeric data.\n\n  Returns\n  -------\n  array \n    A numpy array containing the rescaled values.\n  \"\"\"\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled_vec = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled_vec\nA few important aspects of the above to note:\n\nThe use of three quotation marks, \"\"\" is necessary for this text to be recognized as official documentation.\nThe exact structure of the Parameters and Returns sections, with underlines, is important for consistency with other documentation. There are a few other formatting styles that are generally accepted; we’ll stick with the one in this example in this course.\nWhen listing the arguments, a.k.a. parameters of our function, we specify the name of the argument, the data type that is expected, and a brief description.\nWhen listing the returns of our function, we specify what object structure is being returned, and a brief description.\nThe blank line after the final \"\"\" is important!\n\n\n\n\n\n\n\nCheck-in\n\n\n\nDefine the function using the code above, then run help(rescale_vector). Pretty satisfying, right?"
  },
  {
    "objectID": "05-function_writing.html#scope",
    "href": "05-function_writing.html#scope",
    "title": "6  Writing Custom Functions",
    "section": "6.3 Scope",
    "text": "6.3 Scope\nIn the previous example, you might have expected\nrescale_vector(df['a'])\nto change the column of the df object automatically - but it does not, unless you explicitly reassign the results! This is because everything that happens “inside” the function cannot change the status of objects outside the function.\nThis critical programming concept - determining when objects are changed and when they can be accessed - is called scope.\nIn general, an object is only available within the environment where it was created. Right now, as you work in your Jupyter notebook or Quarto document, you are interacting with your global environment. Run the globals() function to see all the objects you have created and libraries you have loaded so far.\nJust as someone sitting next to you on a different computer can’t access an object in your own global environment, the body of a function is its own function environment.\nAnything that is created or altered in the function environment does not impact the global environment - locally, you only “see” what the function returns.\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\nLike Las Vegas, what happens in a function stays in that function\n\n\n\n\n\n6.3.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function.\nNote that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\nConsider the following code:\na = 10\n\ndef myfun(a):\n  \n  a = a + 10\n  \n  return a\n\n\nmyfun(a)\n\na + 5\n\n\n\n\n\n\nCheck-in\n\n\n\nWithout running the code, what do you think will be printed out by the last two lines of code?\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\n\nThe object named a within the function environment, i.e. the parameter of the function, was altered to be equal to 20.\nThen, the function returned the value of its parameter a, which was 20.\nHere is a sketch of that idea:\n\nHowever, the object named a in our global environment is not impacted. The code a + 5, outside the function, still refers to the object in the global environment, which is equal to 10.\n\n\n\n\n\n6.3.2 Nested environments and scope\nOne thing to notice is that each time the function is run, it creates a new local environment. That is, previous running of a function can’t impact future runs of that function.\nFor example, this code gives the same answer over and over; it does not continue to add 10 to a copy of a, because it never alters the object a in the global environment or the parameters of the other functions’ local environments.\n\ndef myfun(a):\n  \n  a = a + 10\n  \n  return a\n\nmyfun(a)\nmyfun(a)\nmyfun(a)\n\nNameError: name 'a' is not defined\n\n\nHowever, all of the local environments are considered to be inside of your global environment. That is, while they cannot change objects in the global environment, they can “see” those objects.\nNotice in the below code that we don’t pass any arguments to myfun(). But it is still able to compute b + 1 and return an answer!\n\nb = 10\n\ndef myfun():\n  return b + 1\n\nmyfun()\n\n11\n\n\nA function will always look in its local environment first; then check the global for backups:\n\nb = 10\n\ndef myfun():\n  b = 20\n  return b + 1\n\nmyfun()\n\n21\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWriting a function that relies on global objects is in general terrible programming practice!!!\nWhat if you accidentally change or delete that global object? Suddenly the exact same code, i.e. myfun() runs differently or not at all.\n\n\nAlas, although this is bad practice, we quite often “cheat” in Data Science and use global references to our dataset in function shortcuts, e.g.\n\npenguins = load_penguins()\n\ndef plot_my_data(cat_var):\n  \n  plot = (ggplot(penguins, aes(x = cat_var)) + geom_bar())\n  \n  return plot\n  \n  \nplot_my_data('species')\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nThis “trick” is sometimes called Dynamic Lookup. You should mostly avoid it; but when you use it carefully and deliberately in the context of a specific data analysis, it can be a convenient approach.\n\n\n\n\n\n\nCheck-in\n\n\n\nWrite a custom function that does the following:\n\nLimit the penguins dataset to a user-chosen species and/or island.\nMakes a scatterplot of the penguin bill length and depth.\n\nTry writing a version of this function using dynamic lookup, and a version where everything the function needs is passed in directly."
  },
  {
    "objectID": "05-function_writing.html#unit-tests",
    "href": "05-function_writing.html#unit-tests",
    "title": "6  Writing Custom Functions",
    "section": "6.4 Unit tests",
    "text": "6.4 Unit tests\nSo: You have now written your first custom function. Hooray!\n\nNow, before you move on to further analysis, you have an important job to do. You need to make sure that your function will work - or will break in a helpful way - when users give unexpected input.\n\n6.4.1 Unit testing\nThe absolute most important thing to do after defining a function is to run some unit tests.\nThis refers to snippets of code that will try your function out on both “ordinary” user input, and strange or unexpected user input.\nFor example, consider the plot_my_data function defined above. We immediately unit tested it by running\n\nplot_my_data('species')\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nBut what if someone tried to enter the name of a variable that is not categorical? Or a variable that doesn’t exist in the penguins dataset? Or an input that is not a variable name (string)? Or no input at all?\n\nplot_my_data('bill_length_mm')\n\nplot_my_data('name')\n\nplot_my_data(5)\n\nplot_my_data(True)\n\nplot_my_data()\n\nTypeError: plot_my_data() missing 1 required positional argument: 'cat_var'\n\n\n\n\n\n\n\n\nExample\n\n\n\nAre all of these outputs what you expected? Why or why not? Can you explain why the unexpected behavior happened?\n\n\n\n\n6.4.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But as we saw above, you can’t guarantee that the person using your function knows that they need a certain type of input, and they might be confused by how your function handles that input. In these cases, it’s best to validate your function input.\n\n6.4.2.1 if-else and sys.exit\nGenerally your approach to validation will be to check some conditions, and exit the function if they are not met. The function exit() from the sys library is a good approach. You want to make sure you write an informative error statement in your exit, so that the user knows how to fix what went wrong.\n\n\n\n\n\n\nLearn-more\n\n\n\nThis article provides a short guide to writing informative error messages.\n\n\nThe most common condition checking is to make sure the object type is correct - i.e., that the user inputs a string, and that the string refers to a categorical (a.k.a. “object”) variable in the penguins dataset.\nRecall that you can “reverse” a boolean (True or False) value using the not statement. Sometimes, it is easier to check if a condition is not met than to list all the “good” conditions.\nPutting these together, we can check if our user input to plot_my_data is what we expect:\nfrom sys import exit\n\ndef plot_my_data(cat_var):\n    \n  if not isinstance(cat_var, str):\n    exit(\"Please provide a variable name in the form of a string.\")\n    \n  if not (cat_var in penguins.columns):\n    exit(\"The variable provided is not found in the penguins dataset.\")\n    \n  if not penguins[cat_var].dtypes == 'object':\n    exit(\"Please provide the name of a categorical (object type) variable.\")\n  \n  plot = (ggplot(penguins, aes(x = cat_var)) + geom_bar())\n  \n  return plot\n\n\n\n\n\n\nWarning\n\n\n\nNotice that we have used the isinstance base python function to check that the user inputted a string; but we have used the dtype method to check the data type of the column of data in a pandas dataframe.\nWhen checking types and structures, be careful about “special” object types from packages, like pandas data frames or numpy arrays - they each unfortunately have their own type checking functions and their own names for types.\n\n\nNow, let’s retry our unit tests:\n\nplot_my_data('bill_length_mm')\n\nplot_my_data('name')\n\nplot_my_data(5)\n\nplot_my_data(True)\n\nplot_my_data()\n\nSystemExit: Please provide the name of a categorical (object type) variable.\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused. If you’re interested, Wikipedia has more about defensive programming."
  },
  {
    "objectID": "05-function_writing.html#debugging",
    "href": "05-function_writing.html#debugging",
    "title": "6  Writing Custom Functions",
    "section": "6.5 Debugging",
    "text": "6.5 Debugging\nNow that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll find newer and more exciting ways to break your code!).\n\n\n\nThe faces of debugging (by Allison Horst)\n\n\nLet’s start with the basics: print debugging.\n\n6.5.1 Print Debugging\nThis technique is basically exactly what it sounds like. You insert a ton of print statements to give you an idea of what is happening at each step of the function.\nLet’s try it out on the rescale_vector function.\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\nSuppose we try the following:\n\nmy_vec = [-1,0,1,2,3]\n\nmy_vec = np.sqrt(my_vec)\n\nrescale_vector(my_vec)\n\narray([nan, nan, nan, nan, nan])\n\n\nYou probably have spotted the issue here, but what if it wasn’t obvious, and we wanted to know why our function was returning an array of nan values.\nIs the culprit the min? The max?\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  print(\"min: \" + str(min_vec))\n  \n  max_vec = max(vec)\n  print(\"max: \" + str(max_vec))\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\nrescale_vector(my_vec)\n\nmin: nan\nmax: nan\n\n\narray([nan, nan, nan, nan, nan])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how the print() statements cause information to get printed out as the function ran, but did not change the return value of the function!\n\n\nHmmm, both the min and the max were nan. This explains why our rescaling introduced all missing values!\nSo, the issue must be with the user input itself. Let’s rewrite our function to take a look at that.\ndef rescale_vector(vec):\n  \n  print(vec)\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\nrescale_vector(my_vec)\n\n[       nan 0.         1.         1.41421356 1.73205081]\n\n\narray([nan, nan, nan, nan, nan])\n\n\nAh-ha! The first value of the input vector is a nan.\nIdeally, the user would not input a vector with missing values. But it’s our job to make sure the function is prepared to handle them.\n\n\n\n\n\n\nCheck-in\n\n\n\nAdd code to the rescale_vector function definition to check if the vector has any nan values, and give an informative error message if so.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nThink of other options for handling nans in user input in this function. What are the pros and cons of writing functions that are opinionated - i.e., that give errors unless the user input is perfect - versus functions that try to work with imperfect input?\n\n\n\n\n6.5.2 Beyond print statements: breakpoints\nWhile print() statements work fine as a quick-and-dirty debugging strategy, you will soon get tired of using them, since you have to change your function and reload it every time you want to check something.\nA more elegant - and ultimately easier - approach is to “dive in” to the environment of the function itself, where you can interact with the parameters in the local environment the same way you might interact with your global environment.\nTo do this, we will set a breakpoint in our function. This will cause the function to run until that point, and then stop and let us play around in the environment.\ndef rescale_vector(vec):\n  \n  breakpoint()\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\n\n\n\n\n\nCheck-in\n\n\n\nSet a breakpoint in your rescale_vector code, as above, and then run the function on a vector.\nPlay around with the interface of the local environment until you get used to this.\n\n\n\n\n6.5.3 pdb (“python de-bugger”)\nAlthough setting breakpoints can be much cleaner and more convenient than several print() statements, using breakpoint() still required us to modify and reload the function.\nThe most advanced and clean approach to debugging is to use the pdb library to dive straight in to the local environment.\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\nimport pdb\npdb.run(\"rescale_vector(my_vec)\")\n\n\n\n\n\n\nCheck-in\n\n\n\nTry the above debugging approach."
  },
  {
    "objectID": "05-function_writing.html#general-debugging-strategies",
    "href": "05-function_writing.html#general-debugging-strategies",
    "title": "6  Writing Custom Functions",
    "section": "6.6 General Debugging Strategies",
    "text": "6.6 General Debugging Strategies\n\nDebugging: Being the detective in a crime movie where you are also the murderer.\n\nThe overall process of addressing a bug is:\n\nRealize that you have a bug\nGoogle! Generally Googling the error + the programming language + any packages you think are causing the issue is a good strategy.\nMake the error repeatable: This makes it easier to figure out what the error is, faster to re-try to see if you fixed it, and easier to ask for help. Unit tests are perfect for this.\nFigure out where it is. Print statements and debuggers help you dig into the function to find the problem area.\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation.\n\n\n6.6.1 Rubber Duck debugging\nHave you ever called a friend or teacher over for help with an issue, only to find that by explaining it to them, you solved it yourself?\nTalking through your code out loud is an extremely effective way to spot problems. In programming, we call this Rubber Duck Debugging, because coders will sometimes keep a small toy like a rubber duck by their computer, and talk to it while they are alone.\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\nThis is the original RickRoll. Yes, really.\n\n\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\nA more thorough explanation of rubber duck debugging can be found at gitduck.com.\n\n\n\n\n6.6.2 Refactoring your code\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solveable.\nWhen we redesign our functions to consist of many smaller functions, this is called refactoring. Consider the following function:\ndef rescale_all_variables(df):\n  \n  for col in df.columns:\n    \n    min_vec = min(df[col])\n    max_vec = max(df[col])\n  \n    df[col] = (df[col] - min_vec)/(max_vec - min_vec)\n    \n  \n  return df\nA much cleaner and easier to read way to use this function would be to use the smaller function rescale_vector inside of rescale_all_variables\ndef rescale_all_variables(df):\n  \n  for col in df.columns:\n    df[col] = rescale_vector(df[col])\n  \n  return df\nThis not only makes the code more readable to humans, it also helps us track down whether the error is happening in the outside function (rescale_all_variables) or the inside one (rescale_vector)\n\n\n6.6.3 Taking a break!\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s so well-known it has an xkcd comic.\nAt some point, getting up and going for a walk may help!\n\n\n\n\n\n\nPractice-exercise\n\n\n\nClick here to answer a few questions about function code."
  },
  {
    "objectID": "06-iteration.html",
    "href": "06-iteration.html",
    "title": "7  Iteration",
    "section": "",
    "text": "In this chapter, we will learn strategies for iteration, or performing a repeated task over many values.\nConsider the following task: Suppose I want to get a computer to print out the lyrics to the song 99 Bottles of Beer on the Wall.\nNow, we could certainly simply type up all the lyrics ourselves:\n\nprint(\n  \"99 Bottles of beer on the wall, 99 Bottles of beer. Take one down, pass it around, 98 Bottles of beer on the wall. 98 Bottles of beer on the wall, 98 Bottles of beer. Take one down, pass it around, 97 Bottles of beer on the wall.\" #... etc etc etc\n)\n\n99 Bottles of beer on the wall, 99 Bottles of beer. Take one down, pass it around, 98 Bottles of beer on the wall. 98 Bottles of beer on the wall, 98 Bottles of beer. Take one down, pass it around, 97 Bottles of beer on the wall.\n\n\nThat sounds like… not much fun to do.\nBut we notice that in the song, there is a ton of repetition! Every verse is almost identical, except for the number of bottles of beer.\nThis is a great time for some iteration.\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nFortunately, we aren’t going to try to sing this version: https://www.youtube.com/watch?v=R30DnFfVtUw"
  },
  {
    "objectID": "06-iteration.html#for-loops",
    "href": "06-iteration.html#for-loops",
    "title": "7  Iteration",
    "section": "7.2 For loops",
    "text": "7.2 For loops\nThe most basic approach to iteration is a simple for loop. At each step of the loop the value of our placeholder, i, changes to the next step in the provided list, range(100,97,-1).\n\nfor i in range(100,97,-1):\n  print(str(i) + \" bottles of beer on the wall\")\n  print(str(i) + \" bottles of beer\")\n  print(\" take one down, pass it around,\")\n  print(str(i-1) + \" bottles of beer on the wall\")\n\n100 bottles of beer on the wall\n100 bottles of beer\n take one down, pass it around,\n99 bottles of beer on the wall\n99 bottles of beer on the wall\n99 bottles of beer\n take one down, pass it around,\n98 bottles of beer on the wall\n98 bottles of beer on the wall\n98 bottles of beer\n take one down, pass it around,\n97 bottles of beer on the wall\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nAfter last chapter, hopefully we immediately see that this is a great opportunity to write a function, to make our code clearer and avoid repetition.\nWrite a function called sing_verse() to replace the body of the for loop.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\n\ndef sing_verse(num):\n  song = str(num) + \" bottles of beer on the wall \\n\" + str(num) + \" bottles of beer \\n\" + \" take one down, pass it around, \\n\" + str(num-1) + \" bottles of beer on the wall \\n\"\n  \n  return song\n\n\n\nNotice that in this function, instead of print-ing out the lines of the song, we return the an object consisting as one long string.\nOften, when running a for loop, you want to end the loop process with a single object. To do this, we start with an empty object, and then add to it at each loop.\n\nsong = \"\"\n\nfor i in range(100,97,-1):\n  song = song + sing_verse(i)\n  \nprint(song)\n\n100 bottles of beer on the wall \n100 bottles of beer \n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of beer on the wall \n99 bottles of beer \n take one down, pass it around, \n98 bottles of beer on the wall \n98 bottles of beer on the wall \n98 bottles of beer \n take one down, pass it around, \n97 bottles of beer on the wall \n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nModify the code above so that instead of creating one long string, we create a list, where each element is one of the verses of the song.\n\n\n\n7.2.1 Vectorized functions\nThe function that we wrote above can be described as not vectorized. What we mean by that it is designed to only take one value, num. If we instead try to input a list or vector of numbers, we get an error:\n\nsing_verse([100,99,98])\n\nTypeError: unsupported operand type(s) for -: 'list' and 'int'\n\n\nThis is why, in order to get results for a list of number, we needed to iterate.\nHowever, plenty of functions are designed to work well for single numbers or lists and vectors. For example:\n\na_num = 5\na_vec = [1,3,5,7]\n\nnp.sqrt(a_num)\nnp.sqrt(a_vec)\n\narray([1.        , 1.73205081, 2.23606798, 2.64575131])\n\n\nWhen we want to perform a function over many values - whether it’s one we wrote or not - we first need to ask ourselves if the function is vectorized or not. Using a for loop over a vectorized function is unnecessarily complicated and computationally slow!\n\nresult = []\nfor i in a_vec:\n  result = result + [np.sqrt(i)]\n  \nresult\n\n[1.0, 1.7320508075688772, 2.23606797749979, 2.6457513110645907]\n\n\n\n\n7.2.2 Vectorizing and booleans\nA common reason why a custom function is written in an unvectorized way is that it makes use of if statements. For example, consider the task of taking the square root of only the positive numbers in a list.\nHere is an approach that does not work:\n\na_vec = np.array([-2, 1, -3, -9, 7])\n\nif a_vec > 0:\n  a_vec = np.sqrt(a_vec)\n\na_vec\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nThe statement if a_vec > 0 makes no sense for a vector! The if statement needs either a single True or a single False to determine if the subsequent code will be run - but a_vec > 0 returns a list of five booleans.\nInstead, we would need to iterate over the values:\n\na_vec = np.array([-2, 1, -3, -9, 7])\n\nfor val in a_vec:\n  if val > 0:\n    val = np.sqrt(val)\n  print(val)\n\n-2\n1.0\n-3\n-9\n2.6457513110645907\n\n\nHowever, there is a nicer approach to this variety of problem, which is to use boolean masking:\n\nis_pos = a_vec > 0\n\na_vec[is_pos] = np.sqrt(a_vec[is_pos])\n\na_vec\n\narray([-2,  1, -3, -9,  2])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWrite two functions:\n\nsqrt_pos_unvec() takes in a single value as an argument, and returns the square root if the value is positive. Then, write a for loop that uses this function to construct a new vector where the positive values are square rooted.\nsqrt_pos_vec() takes in a vector of values, and returns a vector with the positive values square rooted. Do not use a for loop inside your function."
  },
  {
    "objectID": "06-iteration.html#iterable-functions",
    "href": "06-iteration.html#iterable-functions",
    "title": "7  Iteration",
    "section": "7.3 Iterable functions",
    "text": "7.3 Iterable functions\nAlthough for loops are a clear and basic procedure, it can become very tedious to use them frequently. This is especially true if you want to save the results of the iteration as a new object.\nHowever, it will not be possible or convenient to write every function in a vectorized way.\nInstead, we can use iterable functions, which perform the same iteration as a for loop in shorter and more elegant code.\n\n7.3.1 map()\nThe map() function requires the same information as a for loop: what values we want to iterate over, and what we want to do with each value.\n\nsong = map(sing_verse, range(100, 97, -1))\nsong = list(song)\nprint(\"\".join(song))\n\n100 bottles of beer on the wall \n100 bottles of beer \n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of beer on the wall \n99 bottles of beer \n take one down, pass it around, \n98 bottles of beer on the wall \n98 bottles of beer on the wall \n98 bottles of beer \n take one down, pass it around, \n97 bottles of beer on the wall \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the output of the map() function is a special object structure, of the type “map object”. We automatically convert this to a list with the list() function.\nThen, we make use of the join() string method to turn the list into one long string. Finally, we print() our final string out so that it looks nice.\n\n\n\n7.3.1.1 Double mapping\nSometimes, we want to loop through multiple sets of values at once. The map() function has the ability to take as many iterables, or lists of values, as you want.\nSuppose we want to change our sing_verse() function so that it has two arguments, the number of bottles and the type of drink.\ndef sing_verse_2(num, drink):\n  song = str(num) + \" bottles of \" + drink + \" on the wall \\n\"\n  song = song + str(num) + \" bottles of \" + drink + \"\\n\" \n  song = song + \" take one down, pass it around, \\n\"\n  song = song + str(num-1) + \" bottles of \" + drink + \" on the wall \\n\"\n  \n  return song\nNow, we use map() to switch the number and the drink at each iteration:\n\nnums = range(100, 97, -1)\ndrinks = [\"beer\", \"milk\", \"lemonade\"]\nsong = map(sing_verse_2, nums, drinks)\nprint(\"\".join(list(song)))\n\n100 bottles of beer on the wall \n100 bottles of beer\n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of milk on the wall \n99 bottles of milk\n take one down, pass it around, \n98 bottles of milk on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWrite a sing_verse_3() function that also lets us change the container (e.g. bottle, can, …) at each step of the loop.\nUse map() to sing a few verses.\nWhat happens if you supply three different drinks, but only two different types of containers?"
  },
  {
    "objectID": "06-iteration.html#lambda-functions",
    "href": "06-iteration.html#lambda-functions",
    "title": "7  Iteration",
    "section": "7.4 Lambda functions",
    "text": "7.4 Lambda functions\nWhat would you do if you still wanted to count down the number of bottles, but you wanted them all to be lemonade?\nIn this case, we want one of the arguments of our function to be iterated over many values, and the other one to stay consistent.\nOne rather inelegant way we could accomplish this is with a new function:\n\ndef sing_verse_lemonade(num):\n  return sing_verse_2(num, \"lemonade\")\n\nsong = map(sing_verse_lemonade, nums)\nprint(\"\".join(list(song)))\n\n100 bottles of lemonade on the wall \n100 bottles of lemonade\n take one down, pass it around, \n99 bottles of lemonade on the wall \n99 bottles of lemonade on the wall \n99 bottles of lemonade\n take one down, pass it around, \n98 bottles of lemonade on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\nThis is a lot of extra lines of code, though, for a task that should be straightforward - and we’ll probably never use sing_verse_lemonade() again, so it’s a bit of a waste to create it.\nInstead, we will use what is called a lambda function, which is like making a new sing_verse_lemonade wrapper function for temporary use:\n\nsong = map(lambda i: sing_verse_2(i, \"lemonade\"), nums)\nprint(\"\".join(list(song)))\n\n100 bottles of lemonade on the wall \n100 bottles of lemonade\n take one down, pass it around, \n99 bottles of lemonade on the wall \n99 bottles of lemonade on the wall \n99 bottles of lemonade\n take one down, pass it around, \n98 bottles of lemonade on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\nThe code lambda i: sing_verse_2(i, \"lemonade\") made a new anonymous function - sometimes called a headless function - that takes in one argument, i, and sends that argument into sing_verse_2.\n\n\n\n\n\n\nCheck-in\n\n\n\nUse a lambda function with sing_verse_3() to sing a few verses about milk in glasses."
  },
  {
    "objectID": "06-iteration.html#iterating-on-datasets",
    "href": "06-iteration.html#iterating-on-datasets",
    "title": "7  Iteration",
    "section": "7.5 Iterating on datasets",
    "text": "7.5 Iterating on datasets\nThis task of repeating a calculation with many inputs has a natural application area: datasets!\nIt is extremely common that we want to perform some calculation involving many variables of the dataset, and we want to repeat that same calculation over the values in each row.\nFor this situation, we use an iterable function that is very similar to map(): the .apply() method from pandas.\nAt its core, the .apply() method is meant for repeating a calculation over columns:\n\ndat = pd.DataFrame({\"x\": [99, 50, 2], \"y\": [1, 2, 3]})\n\ndat.apply(np.sqrt)\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      9.949874\n      1.000000\n    \n    \n      1\n      7.071068\n      1.414214\n    \n    \n      2\n      1.414214\n      1.732051\n    \n  \n\n\n\n\nIn this chapter, though, we are more interested in using it to repeat a function, using each row as input:\n\ndat.apply(np.sum, axis=1)\n\n0    100\n1     52\n2      5\ndtype: int64\n\n\nSuppose we have a pandas dataframe consisting of all the numbers, drinks, and containers that we are interested in singing about:\n\ndat = pd.DataFrame({\"num\": [99, 50, 2], \"drink\": [\"beer\", \"soda\", \"Capri Sun\"], \"container\": [\"bottles\", \"cans\", \"pouches\"]})\n\ndat\n\n\n\n\n\n  \n    \n      \n      num\n      drink\n      container\n    \n  \n  \n    \n      0\n      99\n      beer\n      bottles\n    \n    \n      1\n      50\n      soda\n      cans\n    \n    \n      2\n      2\n      Capri Sun\n      pouches\n    \n  \n\n\n\n\nOur goal is to apply the sing_verse_3 function over all these combinations of values.\nUnfortunately, this doesn’t happen automatically:\n\ndat.apply(sing_verse_3, axis=1)\n\nTypeError: sing_verse_3() missing 2 required positional arguments: 'drink' and 'container'\n\n\nThis is because .apply doesn’t “know” which columns below with which arguments of the sing_verse_3 function. We’ll need to use a lambda function to help it out:\n\ndat.apply(lambda x: sing_verse_3(x['num'], x['drink'], x['container']), axis=1)\n\n0    99 bottles of beer on the wall \\n99 bottles of...\n1    50 cans of soda on the wall \\n50 cans of soda\\...\n2    2 pouches of Capri Sun on the wall \\n2 pouches...\ndtype: object\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nClick here to open the practice activity."
  },
  {
    "objectID": "07-webscraping.html",
    "href": "07-webscraping.html",
    "title": "8  Webscraping",
    "section": "",
    "text": "This document demonstrates the use of the BeautifulSoup library in Python to do webscraping.\nWeb scraping is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.\nIf you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Sill, it’s a good idea to do some research on your own and make sure that you’re not violating any Terms of Service before you start a large-scale project.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the beautifulsoup4 library installed then you will need to run\npip install beautifulsoup4\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).\n\n\nimport pandas as pd"
  },
  {
    "objectID": "07-webscraping.html#an-alternative-to-web-scraping-apis",
    "href": "07-webscraping.html#an-alternative-to-web-scraping-apis",
    "title": "8  Webscraping",
    "section": "8.2 An Alternative to Web Scraping: APIs",
    "text": "8.2 An Alternative to Web Scraping: APIs\nSome website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\nWhen you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\n\n8.2.1 The Tasty API\n\n\n\n\n\n\nCheck-in\n\n\n\nTasty.co is a website and app that offers food recipes. They have made these recipes available through a REST API. You do need authentication, though.\nSpecifically, you will need to sign up for free account. You will then be provided with an API key that will need to be supplied with every request you make. This is used to track and limit usage.\n\n\nFor our example, we’ll us the recipes/list endpoint.\nMake sure you are logged into the account you are created, and select the recipes/list endpoint from the menu at the left of the documentation here. We’ll use the requests Python library to make use of this.\nLet’s search for recipes containing “daikon” (an Asian radish). Which one is the cheapest per portion?\nimport requests\n\nurl = \"https://tasty.p.rapidapi.com/recipes/list\"\n\nquerystring = {\"from\":\"0\",\"size\":\"20\",\"q\":\"daikon\"}\n\nheaders = {\n    \"X-RapidAPI-Key\": <your key here>,\n    \"X-RapidAPI-Host\": \"tasty.p.rapidapi.com\"\n}\n\nresponse = requests.get(url, headers=headers, params=querystring)\n\nprint(response.json())\nNotice that there are two elements to this object we got back, and we only really want the results piece.\ndaikon_recipes = pd.json_normalize(response.json(), \"results\")\ndaikon_recipes\n\n\n\n\n\n\nCheck-in\n\n\n\nWith the 2-3 people around you, look up what the JSON format is and what it looks like. Then look up the json_normalize function and discuss the differences between the results of this and the JSON format.\n\n\nThe JSON format is extremely common, but can be somewhat easily worked with because it’s highly structured.\nBefore we move on from APIs it’s again important to note that gathering data from websites often comes with constraints. For example, the Tasty API only returns 20 results by default and only 40 results maximum, even if you specify the size= parameter. So, to gather more than 40 results we might need to use some form of iteration while still respecting the API’s rate limits."
  },
  {
    "objectID": "07-webscraping.html#html-and-web-scraping",
    "href": "07-webscraping.html#html-and-web-scraping",
    "title": "8  Webscraping",
    "section": "8.3 HTML and Web Scraping",
    "text": "8.3 HTML and Web Scraping\nHTML, which stands for “hypertext markup language”, is an XML-like language for specifying the appearance of web pages. Each tag in HTML corresponds to a specific page element. For example:\n\n<img> specifies an image. The path to the image file is specified in the src= attribute.\n<a> specifies a hyperlink. The text enclosed between <a> and </a> is the text of the link that appears, while the URL is specified in the href= attribute of the tag.\n<table> specifies a table. The rows of the table are specified by <tr> tags nested inside the <table> tag, while the cells in each row are specified by <td> tages nested inside each <tr> tag.\n\nOur goal is not to teach you HTML to make a web page. You will learn just enough HTML to be able to scrape data programmatically from a web page.\n\n8.3.1 Inspecting HTML Source Code\nSuppose we want to scrape faculty information from the Cal Poly Statistics Department directory. Once we have identified a web page that we want to scrape, the next step is to study the HTML source code. All web browsers have a “View Source” or “Page Source” feature that will display the HTML source of a web page.\n\n\n\n\n\n\nCheck-in\n\n\n\nVisit the web page above, and view the HTML source of that page. (You may have to search online to figure out how to view the page source in your favorite browser.) Scroll down until you find the HTML code for the table containing information about the name, office, phone, e-mail, and office hours of the faculty members.\n\n\nNotice how difficult it can be to find a page element in the HTML source. Many browsers allow you to right-click on a page element and jump to the part of the HTML source corresponding to that element.\n\n\n8.3.2 Scraping an HTML Table with pandas\nThe pandas command read_html can be used to scrape information from an HTML table on a webpage.\nWe can call read_html on the URL.\npd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\nHowever, this scrapes all the tables on the webpage, not just the one we want. As we will see with Beautiful Soup, we can narrow the search by specifying the table attributes.\npd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", attrs = {'class': 'wikitable sortable', \"style\": \"text-align:center\"})\n\n\n\n\n\n\nCheck-in\n\n\n\nWhere did the attrs details in the code above come from? With the 2-3 people around you, inspect the HTML source code for this page and see if you can identify this.\n\n\nThis still returns 3 tables. The table that we want is the first one!\ndf_cities2 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", attrs = {'class': 'wikitable sortable', \"style\": \"text-align:center\"})[0]\ndf_cities2\nThis is a good first pass at scraping information from a webpage and it returns it to us well in the form of a data frame. This works well for HTML tables. Unfortunately, you often want to scrape information from a webpage that isn’t conveniently stored in an HTML table, in which case read_html won’t work. (It only searches for <table>, <th>, <tr>, and <td> tags, but there are many other HTML tags.)\n\n\n8.3.3 Web Scraping Using BeautifulSoup\nBeautifulSoup is a Python library that makes it easy to navigate an HTML document. Like with XML, we can query tags by name or attribute, and we can narrow our search to the ancestors and descendants of specific tags. Also, many web sites have malformed HTML, which BeautifulSoup is able to handle gracefully.\nFirst we issue an HTTP request to the URL to get the HTML source code.\nimport requests\nresponse = requests.get(\"https://statistics.calpoly.edu/content/directory\")\nThe HTML source is stored in the .content attribute of the response object. We pass this HTML source into BeautifulSoup to obtain a tree-like representation of the HTML document.\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n8.3.3.1 Find Elements by ID\nIn an HTML web page, every element can have an id attribute assigned. As the name already suggests, that id attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID.\n\n\n\n\n\n\nCheck-in\n\n\n\nVisit the web page above, and view the HTML source of that page. Locate an element of interest and identify its id attribute. Then run the following to try extracting it\nresults = soup.find(id=\"your id here\")\nprint(results.prettify())\n\n\n\n\n8.3.3.2 Find Elements by HTML Class Name\nYou will often see that every similar piece of a web page is wrapped in the same HTML element, like <div> with a particular class. We’re able to extract all of the parts of the HTML of interest to us, also, by specifying a containing HTML element and the specific classes we want with code like the following:\nresults.find_all(\"element name\", class_=\"class name\")\n\n\n8.3.3.3 Find Elements by Class Name and Text Content\nIt’s often the case that we only want pieces of a web page’s content that match certain criteria. We can refine our search using the string option of .find_all().\nrefined_results = results.find_all(\"class name\", string=\"search text\")\nThis code finds all class name elements where the contained string matches \"search text\" exactly. We’ll discuss later how to be more robust in the specification of string matches like this!\n\n\n8.3.3.4 Find Elements by Tag\nNow we can search for tags within this HTML document, using functions like .find_all(). For example, we can find all tables on this page.\ntables = soup.find_all(\"table\")\nlen(tables)\nAs a visual inspection of the web page would confirm, there are 3 tables on the page (chair and staff, faculty, emeritus faculty), and we are interested in the second one (for faculty).\ntable = tables[1]\ntable\nThere is one faculty member per row (<tr>), except for the first row, which is the header. We iterate over all rows except for the first, extracting the information about each faculty to append to rows, which we will eventually turn into a DataFrame. As you read the code below, refer to the HTML source above, so that you understand what each line is doing.\n\n\n\n\n\n\nNote\n\n\n\nYou are encouraged to add print() statements inside the for loop to check your understanding of each line of code.\n\n\n# initialize an empty list\nrows = []\n\n# iterate over all rows in the faculty table\nfor faculty in table.find_all(\"tr\")[1:]:\n\n    # Get all the cells (<td>) in the row.\n    cells = faculty.find_all(\"td\")\n\n    # The information we need is the text between tags.\n\n    # Find the the name of the faculty in cell[0]\n    # which for most faculty is contained in the <strong> tag\n    name_tag = cells[0].find(\"strong\") or cells[0]\n    name = name_tag.text\n\n    # Find the office of the faculty in cell[1]\n    # which for most faculty is contained in the <a> tag\n    link = cells[1].find(\"a\") or cells[1]\n    office = link.text\n\n    # Find the email of the faculty in cell[3]\n    # which for most faculty is contained in the <a> tag\n    email_tag = cells[3].find(\"a\") or cells[3]\n    email = email_tag.text\n\n    # Append this data.\n    rows.append({\n        \"name\": name,\n        \"office\": office,\n        \"email\": email\n    })\n\n\n\n\n\n\nCheck-in\n\n\n\nWith the 2-3 people around you do the following referring to the code above:\n\nWhat does cells look like for the first faculty (i.e. first iteration of the loop)?\nWhy do we need to call .find(\"strong\") within cells[0] (i.e. a <td> tag) to get the name of the faculty member?\nCould you extract the phone information for each faculty as well? If so, then add the code necessary to do this.\n\n\n\nIn the code above, observe that .find_all() returns a list with all matching tags, while .find() returns only the first matching tag. If no matching tags are found, then .find_all() will return an empty list [], while .find() will return None.\nFinally, we turn rows into a DataFrame.\npd.DataFrame(rows)\nNow this data is ready for further processing.\n\n\n\n\n\n\nPractice-exercise\n\n\n\nPractice Activity notebook here."
  }
]