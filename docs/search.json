[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GSB 544: Data Science and Machine Learning with Python",
    "section": "",
    "text": "This text was created for the Cal Poly course “GSB 544: Data Science and Machine Learning with Python” by Dr. Kelly Bodwin and Dr. Hunter Glanz. Some parts of the material and text are borrowed from Dr. Emily Robinson’s R course and Dr. Dennis Sun’s python course\nThis text is not meant to be a complete course or textbook by itself; rather, think of it as “long-form” class slides. We will summarize the main concepts in each chapter, show you examples, point you to more in-depth readings from outside sources, and ask you to try out short tasks in python as you go.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n\n\n\n\n\n\n\nExample\n\n\n\nExample sections contain code and other information. Don’t skip them!\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote sections contain clarification points (anywhere I would normally say “note that ….). Make sure to read them to avoid any common pitfalls or misconceptions.\n\n\n\n\n\n\n\n\nRequired-reading\n\n\n\nConsider these sections to be required readings. This is where we will direct you to existing materials to explain or introduce a concept.\n\n\n\n\n\n\n\n\nRequired-video\n\n\n\nSimilarly, consider these sections to be required viewing for the course material.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nCheck-in sections contain small tasks that you need to do throughout the reading, to practice or prepare. Although they are not graded, please treat them as required!\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nEach chapter will have a longer practice exercise to complete and turn in. These are intended to be done with help from instructors and peers.\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\nConsider these to be optional readings/viewings. The world of python programming has so many interesting tidbits, we can’t possibly teach them all - but we want to share them with you nonetheless!\nWe will usually make these “click to expand” so that they don’t distract from your reading.\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nThese are personal opinion comments from the authors. Take them with a grain of salt; we aren’t the only python programmers worth listening to, we are just sharing what has worked for us.\nWe will usually make these “click to expand” so that they don’t distract from your reading.\n\n\n\n\n\n\nReferences or additional readings may come from the following texts:\n\nPython Data Science Handbook by Jake VanderPlas\nPython for Data Analysis by Wes McKinney\nPrinciples of Data Science by Dennis Sun\n\nFor extra practice with python programming, we recommend the DataQuest interactive tutorials or working through some lessons on Python for Everybody."
  },
  {
    "objectID": "00-setup.html",
    "href": "00-setup.html",
    "title": "1  Intro and Workflow Setup",
    "section": "",
    "text": "While most students will arrive having taken an introductory programming course and/or Summer python intensive workshop, it is important to start this class with some computer fundamentals and some setup and workflow preparation.\nThis chapter is meant to provide a resource for some basic computer skills and a guide to the workflow we expect you to follow when managing your code and data.\nIn this chapter you will:\n\nLearn the basics of a computer system.\nCreate a GitHub account and practice using it for managing code and data artifacts.\nPractice using Google Colab notebooks for quick tasks and activities.\nInstall python locally on your machine via Anaconda.\nPractice opening and using jupyter notebooks in Anaconda.\nInstall the quarto document rendering system.\nPractice using quarto to render nicely formatted html documents from jupyter notebooks.\n\nIf you already feel comfortable with your own file organization system; you prefer GitLab over GitHub; or you prefer to use another python distribution and IDE (like VSCode), that is acceptable. Just know that we may be less able to help you and troubleshoot if you deviate from the recommended workflow.\nFor grading consistency, we will require that you submit quarto-rendered documents for all labs and projects."
  },
  {
    "objectID": "00-setup.html#computer-basics",
    "href": "00-setup.html#computer-basics",
    "title": "1  Intro and Workflow Setup",
    "section": "1.1 Computer Basics",
    "text": "1.1 Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\n\n1.1.1 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\n\n\nLearn-more\n\n\n\n{{ <video “https://www.youtube.com/embed/Rdm8E59L8Og >}}\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\n\n\n\n1.1.2 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n\nLearn-more\n\n\n\n{{ <video https://www.youtube.com/embed/RhHMgkUdhdk” > }}\n\n\n\n\n1.1.3 File Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\n\n\n\n\n\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BV0-EPUYuQc >}}\nStop watching at 4:16.\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BMT3JUWmqYY >}}\n\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nRecommend watching - helpful for understanding file paths!\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home."
  },
  {
    "objectID": "00-setup.html#git-and-github",
    "href": "00-setup.html#git-and-github",
    "title": "1  Intro and Workflow Setup",
    "section": "1.2 Git and GitHub",
    "text": "1.2 Git and GitHub\nOne of the most important parts of a data scientist’s workflow is version tracking: the process of making sure that you have a record of the changes and updates you have made to your code.\n\n1.2.1 Git\nGit is a computer program that lives on your local computer. Once you designate a folder as a Git Repository, the program will automatically tracks changes to the files in side that folder.\n\n\n\n\n\n\nCheck-in\n\n\n\nClick here to install Git on your computer.\n\n\n\n\n1.2.2 GitHub\nGitHub, and the less used alternate GitLab, are websites where Git Repositories can be stored online. This is useful for sharing your repository (“repo”) with others, for multiple people collaborating on the same repository, and for yourself to be able to access your files from anywhere.\n\n\n\n\n\n\nCheck-in\n\n\n\nClick here to make a GitHub account, if you do not already have one.\nYou do not have to use your school email for this account.\n\n\n\n\n1.2.3 Practice with Repos\nIf you are already familiar with how to use Git and GitHub, you can skip the rest of this section, which will walk us through some practice making and editing repositories.\nFirst, watch this 15-minute video, which nicely illustrates the basics of version tracking:\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/BCQHnlnPusY?si=L9C5waHxDzib-VwY >}}\n\n\nThen, watch this 10-minute video, which introduces the idea of branches, and important habit for collaborating with others (or your future self!)\n\n\n\n\n\n\nRequired-video\n\n\n\n{{ <video https://www.youtube.com/embed/oPpnCh7InLY?si=Yzezgt3R4n1OYBdV >}}\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nAlthough Git can sometimes be a headache, it is worth the struggle. Never again will you have to deal with a folder full of documents that looks like:\nProject-Final\nProject-Final-1\nProject-Final-again\nProject-Final-1-1\nProject-Final-for-real\n\n\n\nWorking with Git and GitHub can be made a lot easier by helper tools and apps. We recommend GitHub Desktop for your committing and pushing.\n\n\n1.2.4 Summary\nFor our purposes, it will be sufficient for you to learn to:\n\ncommit your work frequently as you make progress; about as often as you might save a document\npush your work every time you step away from your project\nbranch your repo when you want to try something and you aren’t sure it will work.\n\nIt will probably take you some time to get used to a workflow that feels natural to you - that’s okay! As long as you are trying out version control, you’re doing great."
  },
  {
    "objectID": "00-setup.html#anaconda-and-jupyter",
    "href": "00-setup.html#anaconda-and-jupyter",
    "title": "1  Intro and Workflow Setup",
    "section": "1.3 Anaconda and Jupyter",
    "text": "1.3 Anaconda and Jupyter\nNow, let’s talk about getting python actually set up and running.\n\n\n\n\n\n\n1.3.1 Anaconda\nOne downside of python is that it can sometimes be complicated to keep track of installs and updates.\n\n\n\n\n\nUnless you already have a python environment setup that works for you, we will suggest that you use Anaconda, which bundles together an installation of the most recent python version as well as multiple tools for interacting with the code.\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload Anaconda here\n\n\n\n\n1.3.2 Jupyter\nWhen you are writing ordinary text, you choose what type of document to use - Microsoft Word, Google Docs, LaTeX, etc.\nSimilarly, there are many types of files where you can write python code. By far the most common and popular is the jupyter notebook.\nThe advantage of a jupyter notebook is that ordinary text and “chunks” of code can be interspersed.\n\n\n\n\n\nJupyter notebooks have the file extension .ipynb for “i python notebook”.\n\n1.3.2.1 Google Colab\nOne way you may have seen the Jupyter notebooks before is on Google’s free cloud service, Google Colab.\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, to practice using Jupyter notebooks.\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nColab is an amazing data science tool that allows for easy collaboration.\nHowever, there is a limited amount of free compute time offered by Google, and not as much flexibility or control over the documents.\nThis is why we need Anaconda or similar local installations."
  },
  {
    "objectID": "00-setup.html#quarto",
    "href": "00-setup.html#quarto",
    "title": "1  Intro and Workflow Setup",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nAlthough jupyter and Colab are fantastic tools for data analysis, one major limitation is that the raw notebooks themselves are not the same as a final clear report.\nTo convert our interactive notebooks into professionally presented static documents, we will use a program called Quarto.\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload Quarto here\n\n\nOnce quarto is installed, converting a .ipynb file requires running only a single line in the Terminal:\nquarto render my_file.ipynb\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload the .ipynb file from your practice Colab notebook, open it using Anaconda, and render it using Quarto.\n\n\nHowever, there are also many, many options to make the final rendered document look even more visually pleasing and professional. Have a look at the Quarto documentation if you want to play around with themes, fonts, layouts, and so on.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nhttps://quarto.org/docs/get-started/hello/jupyter.html"
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "2  Programming Basics",
    "section": "",
    "text": "In this chapter, we will review some basics of general computer programming, and how they appear in python.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nSome of you may find this material to be unneeded review - if so, great!\nBut if you are new to programming, or it has been a while, this tutorial may help you refresh your knowledge."
  },
  {
    "objectID": "01-basics.html#basic-data-types",
    "href": "01-basics.html#basic-data-types",
    "title": "2  Programming Basics",
    "section": "2.1 Basic Data Types",
    "text": "2.1 Basic Data Types\nIt is important to have a base grasp on the types of data you might see in data analyses.\n\n2.1.1 Values and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn python, there are some very basic data types:\n\nlogical or boolean - False/True or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\ndouble or float - decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).1\n\nnumeric - python uses the name numeric to indicate a decimal value, regardless of precision.\ncharacter or string or object - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, python has a function to help you with that.\n\ntype(False)\ntype(2) # by default, python treats whole numbers as integers\ntype(2.0)  # to force it not to be an integer, add a .0\ntype(\"Hello, programmer!\")\n\nstr\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn python, boolean values are True and False. Capitalization matters a LOT.\nOther details: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\n\n\n\n2.1.2 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn python, we assign variables values using the syntax object_name = value You can read this as “object name gets value” in your head.\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\nmessage + \", sang the dolphins.\"\n\nyear + the_answer\n\nnot earth_demolished\n\nTrue\n\n\n\n2.1.2.1 Valid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, and _.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right.\n\n1st_thing = \"No starting with numbers\"\n\nfirst~thing = \"No other symbols\"\n\nfirst.thing = \"Periods have a particular meaning!\"\n\nSyntaxError: invalid syntax (3761243318.py, line 1)\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\n\n\nThere are a few different conventions for naming things that may be useful:\n\nsome_people_use_snake_case, where words are separated by underscores\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nA few people mix conventions with variables_thatLookLike_this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\n\n\n\n2.1.3 Type Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\ntype(2 + 3.14159) # add integer 2 and pi\ntype(2 + True) # add integer 2 and TRUE\ntype(True + False) # add TRUE and FALSE\n\nint\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how python handles type conversions.\n\n\n\n\n\n\nCheck-in\n\n\n\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “NA” or ” ” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell python that we want our string to be treated as a number, or vice-versa.\nIn python, we can explicitly convert a variable’s type using functions (int, float, str, etc.).\n\nx = 3\ny = \"3.14159\"\n\nx + y\n\nx + float(y)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'"
  },
  {
    "objectID": "01-basics.html#operators-and-functions",
    "href": "01-basics.html#operators-and-functions",
    "title": "2  Programming Basics",
    "section": "2.2 Operators and Functions",
    "text": "2.2 Operators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\n\nOperation\npython symbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n//\n\n\nModular Division\n%\n\n\nExponentiation\n**\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 // 3 would be 4, and 14 % 3 would be 2.\n\n14 // 3\n14 % 3\n\n2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\n\n2.2.1 Order of Operations\npython operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)**(5-2) # 2 ^ 3 = 8\n1 + 2**3 * 4 # 1 + (8 * 4)\n3*1**3 # 3 * 1\n\n3\n\n\n\n\n2.2.2 String Operations\nThe + operator also works on strings. Just remember that python doesn’t speak English - it neither knows nor cares if your strings are words, sentences, etc. So if you want to create good punctuation or spacing, that needs to be done in the code.\n\ngreeting = \"howdy\"\nperson = \"pardner\"\n\ngreeting + person\ngreeting + \", \" + person\n\n'howdy, pardner'\n\n\n\n\n2.2.3 Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. We only need to know how to use functions. Let’s look at the official documentation for the function round()`.\nround(number, ndigits=None)\n\nReturn number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input.\nThis tells us that the function requires one argument, number, a number to round. You also have the option to include a second argument, ndigits, if you want to round to something other than a whole number.\nWhen you call a function, you can either use the names of the arguments, or simply provide the information in the expected order.\nBy convention, we usually use names for optional arguments but not required ones.\n\nround(number = 2.718)\nround(2.718, 2)\n\n\nround(2.718)\nround(2.718, ndigits = 2)\n\n2.72\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe names of functions and their arguments are chosen by the developer who created them. You should never simply assume what a function or argument will do based on the name; always check documentation or try small test examples if you aren’t sure."
  },
  {
    "objectID": "01-basics.html#data-structures",
    "href": "01-basics.html#data-structures",
    "title": "2  Programming Basics",
    "section": "2.3 Data Structures",
    "text": "2.3 Data Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\nMethods or attributes are a special type of function that operate only on a specific data structure When using a method in python, you can use a period . to apply the function to an object.\nmy_nums = [1,2,3,4,5]\nmy_nums.sort()\nCareful, though! If a function is not specifically designed to be an attribute of the structure, this . trick won’t work.\n\nmy_nums.round()\n\nAttributeError: 'list' object has no attribute 'round'\n\n\n\n2.3.1 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nx = [\"a\", 3, True]\nx\n\n['a', 3, True]\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n2.3.1.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)3.\nIn python, we count from 0.\n\nx = [\"a\", 3, True]\n\nx[0] # This returns a list\nx[0:2] # This returns multiple elements in the list\n\nx.pop(0)\n\n'a'\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use the .pop attribute. The only downside to .pop is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n\n\n2.3.2 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \nBase python does not actually have a vector-type object! However, in data analysis we often have reasons to want a single-type data structure, so we will load an extra function called array from the numpy library to help us out. (More on libraries later!)\n\nfrom numpy import array\n\ndigits_pi = array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n# Access individual entries\ndigits_pi[1]\n\n# Print out the vector\ndigits_pi\n\narray([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific elements as well:\n\nfavorite_cats = array([\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"])\n\nfavorite_cats\n\nfavorite_cats[2] = \"Nyan Cat\"\n\nfavorite_cats\n\narray(['Grumpy', 'Garfield', 'Nyan Cat', 'Jean'], dtype='<U8')\n\n\nIf you’re curious about any of these cats, see the footnotes4.\n\n2.3.2.1 Boolean masking\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that tells us which elements of a corresponding vector we want to keep.\n\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\n\n\n\nNote that for boolean masking to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\n\n\n\n\nExample\n\n\n\n\n# Define a character vector\nweekdays = array([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"])\nweekend = array([\"Sunday\", \"Saturday\"])\n\n# Create logical vectors manually\nrelax_days = array([True, False, False, False, False, False, True])\n\n# Create logical vectors automatically\nfrom numpy import isin     # get a special function for arrays\nrelax_days = isin(weekdays, weekend) \n\nrelax_days\n\n# Using logical vectors to index the character vector\nweekdays[relax_days] \n\n# Using ~ to reverse the True and False\nweekdays[~relax_days] \n\narray(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n      dtype='<U9')\n\n\n\n\n\n\n2.3.2.2 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\n\n\n\n\n\nExample\n\n\n\n\narray([2, False, 3.1415, \"animal\"]) # all converted to strings\n\narray([2, False, 3.1415]) # converted to numerics\n\narray([2, False]) # converted to integers\n\narray([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - python decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\n\n\n\n\nWarning\n\n\n\nImplicit type conversions may seem convenient, but they are dangerous! Imagine that you created one of the arrays above, expecting it to be numeric, and only found out later that python had made it into strings.\n\n\n\n\n\n2.3.3 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\nOnce again, we need to use the numpy package to allow the matrix type to exist in python.\n\nfrom numpy import matrix\n\nmatrix([[1,2,3], [4,5,6]])\n\nmatrix([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how we give the matrix() function an argument that is a “list of lists”. That is, the first item in the list is [1,2,3] which is itself a list.\nYou can always think of lists as the most “neutral” data structure - if you don’t know what you want to use, it’s reasonably to start with the list, and then adjust from there, as we have with the array() and matrix() functions from numpy.\n\n\n\n2.3.3.1 Indexing in Matrices\npython uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [2,0] to get to the third row and first column entry (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\n\n\n\n\nExample\n\n\n\n\nmy_mat = matrix([[1,2,3,4], [4,5,6,7], [7,8,9,10]])\n\nmy_mat\n\nmy_mat[2,0] = 500\n\nmy_mat\n\nmatrix([[  1,   2,   3,   4],\n        [  4,   5,   6,   7],\n        [500,   8,   9,  10]])\n\n\n\n\nWe will not use matrices often in this class, but there are many math operations that are very specific to matrices. If you continue on in your data science journey, you will probably eventually need to do matrix algebra in python.\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nTutorial: Linear Algebra in python"
  },
  {
    "objectID": "01-basics.html#libraries-and-open-source",
    "href": "01-basics.html#libraries-and-open-source",
    "title": "2  Programming Basics",
    "section": "2.4 Libraries and Open-Source",
    "text": "2.4 Libraries and Open-Source\n\n2.4.1 Open-source languages\nOne of the great things about python is that it is an open-source language. This means that it was and is developed by individuals in a community rather than a private company, and the core code of it is visible to everyone.\nThe major consequences are:\n\nIt is free for anyone to use, rather than behind a paywall. (SAS or Java are examples of languages produced by private companies that require paid licenses to use.)\nThe language grows quickly, and in many diverse ways, because anyone at all can write their own programs. (You will write functions in a couple weeks!)\nYou are not allowed to sell your code for profit. (You can still write private code to help your company with a task - but you may not charge money to others for the programs themselves.)\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nWe believe very strongly in the philosophy of open-source. However, it does have its downsides: mainly, that nearly all progress in the language is on a volunteer, community basis.\nAs a user of open source tools, we hope you will give back in whatever ways you can - sharing your work publicly, helping others learn, and encouraging private companies to fund open-source work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nThis very recent article, about the role of open source in today’s world of AI and social media, is quite interesting!\n\n\n\n\n\n2.4.2 Libraries\nWhen an open-source developer creates a new collection of functions and capabilities for python, and they want it to be easily usable and accessible to others, they bundle their code into a library. (You will sometimes here this called a package.)\nPackages that meet certain standards of quality and formatting are added to the Python Package Index, after which they can be esailly installed with pip (“package installer for python”).\nMost of the packages we will use in this class actually come pre-installed with Anaconda, so we won’t have to worry about this too much.\n\n\n\n\n\n\nCheck-in\n\n\n\nOne package we need that is not pre-installed is plotnine.\nOpen up either a terminal or a Jupyter notebook in Anaconda. Then type\npip install plotnine\n\n\n\n\n\n\n\n\nOpinion\n\n\n\n\n\nPython is notoriously frustrating for managine package installs. We will keep things simple in this class, but if you reach a point where you are struggling with libraries, know that you are not alone.\n\n\n\n\n\n\n\n\n\n\n2.4.3 Using library functions\nWhen you want to use functions from a library in your current code project, you have two options:\n\n2.4.3.1 1. Import the whole library\nIt is possible to load the full functionality of a library into your notebook project by adding an import statement in your very first code chunk.\nThe downside of this is that you then need to reference all those functions using the package name:\n\nimport numpy\n\nmy_nums = numpy.array([1,2,3,4,5])\nnumpy.sum(my_nums)\n\n15\n\n\nBecause this can get tedious, it’s common practice to give the package a “nickname” that is shorter:\n\nimport numpy as np\nmy_nums = np.array([1,2,3,4,5])\nnp.sum(my_nums)\n\n15\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nThe reason for needing to use the library names is that nothing stops two developers from choosing the same name for their function. Python needs a way to know which library’s function you intended to use.\n\n\n\n\n\n2.4.3.2 2. Import only the functions you need.\nIf you only need a handful of functions from the library, and you want to avoid the extra typing of including the package name/nickname, you can pull those functions in directly:\n\nfrom numpy import array, sum\n\nmy_nums = array([1,2,3,4,5])\nsum(my_nums)\n\n15"
  },
  {
    "objectID": "01-basics.html#data-frames",
    "href": "01-basics.html#data-frames",
    "title": "2  Programming Basics",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\nSince we are interested in using python specifically for data analysis, we will mention one more important Data Structure: a data frame.\nUnlike lists (which can contain anything at all) or matrices (which must store all the same type of data), data frames are restricted by column. That is, every data entry within a single column must be the same type; but two columns in the same data frame can have two different types.\nOne way to think of a data frame is as a list of vectors that all have the same length.\n\n2.5.0.1 Pandas\nAs with vectors and matrices, we need help from an external package to construct and work efficiently with data frames. This library is called pandas, and you will learn many of its functions next week.\nFor now, let’s just look at a pandas data frame:\n\nimport pandas as pd\n\ndat = pd.read_csv(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\")\n\ndat\n\ndat.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 9 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   rowid              344 non-null    int64  \n 1   species            344 non-null    object \n 2   island             344 non-null    object \n 3   bill_length_mm     342 non-null    float64\n 4   bill_depth_mm      342 non-null    float64\n 5   flipper_length_mm  342 non-null    float64\n 6   body_mass_g        342 non-null    float64\n 7   sex                333 non-null    object \n 8   year               344 non-null    int64  \ndtypes: float64(4), int64(2), object(3)\nmemory usage: 24.3+ KB\n\n\nNotice how the columns all have specific types: integers, floats, or strings (“object”). They also each have names. We can access the vector of information in one column like so…\n\ndat.body_mass_g\n\n0      3750.0\n1      3800.0\n2      3250.0\n3         NaN\n4      3450.0\n        ...  \n339    4000.0\n340    3400.0\n341    3775.0\n342    4100.0\n343    3775.0\nName: body_mass_g, Length: 344, dtype: float64\n\n\n… which then lets us do things to that column vector just as we might for standalone vectors:\n\n## using methods\ndat.body_mass_g.mean()\n\n## editing elements\ndat.body_mass_g[0] = 10000000\ndat.body_mass_g\n\n## boolean masking\nbig_penguins = dat.body_mass_g > 6000\ndat.loc[big_penguins]\n\n\n\n\n\n  \n    \n      \n      rowid\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      1\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      10000000.0\n      male\n      2007\n    \n    \n      169\n      170\n      Gentoo\n      Biscoe\n      49.2\n      15.2\n      221.0\n      6300.0\n      male\n      2007\n    \n    \n      185\n      186\n      Gentoo\n      Biscoe\n      59.6\n      17.0\n      230.0\n      6050.0\n      male\n      2007"
  },
  {
    "objectID": "01-basics.html#summary",
    "href": "01-basics.html#summary",
    "title": "2  Programming Basics",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nWhew! How’s that for an overview?\n\n\n\n\n\nThe most important takeaways from this chapter are:\n\nObjects in python have types, and sometimes functions and operators behave differently based on the type.\nFunctions have both optional and required arguments. They take input and produce output.\nData can be stored in multiple different structures. The choice of structure depends on the dimensionality (1D or 2D) and the homogeneity (do all elements need to be the same type?)\nWe use indexing to access (and edit) individual elements or sections of data structures.\nWe use boolean masking to find only the elements of a vector, matrix, or data frame that meet a particular qualification.\npython is an open-source language. We will import many different libraries to add to our basic functionality.\n\n\n2.6.1 Practice Exercise\n\n\n\n\n\n\nPractice-exercise\n\n\n\nUse your new knowledge of objects, types, and common coding errors to solve this puzzle"
  },
  {
    "objectID": "02-plotnine.html",
    "href": "02-plotnine.html",
    "title": "3  Data Visualization in Python",
    "section": "",
    "text": "This document demonstrates the use of the plotnine library in Python to visualize data via the grammar of graphics framework.\nThe functions in plotnine originate from the ggplot2 R package, which is the R implementation of the grammar of graphics."
  },
  {
    "objectID": "02-plotnine.html#grammar-of-graphics",
    "href": "02-plotnine.html#grammar-of-graphics",
    "title": "3  Data Visualization in Python",
    "section": "3.2 Grammar of Graphics",
    "text": "3.2 Grammar of Graphics\nThe grammar of graphics is a framework for creating data visualizations.\nA visualization consists of:\n\nThe aesthetic: Which variables are dictating which plot elements.\nThe geometry: What shape of plot your are making.\n\nFor example, the plot below displays some of the data from the Palmer Penguins data set.\nFirst, though, we need to load the Palmer Penguins dataset.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Same for any other libraries you haven’t installed.\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom plotnine import ggplot, geom_point, aes, geom_boxplot\n\npenguins = load_penguins()\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nThe aesthetic is species on the x-axis, bill_length_mm on the y-axis, colored by species.\nThe geometry is a boxplot.\n\n\n\n\n\n\nCheck-in\n\n\n\nTake a look at the first page of the optional reading for plotnine. In groups of 3-4, discuss the differences between how they use plotnine and the way we used it in the code chunk above."
  },
  {
    "objectID": "02-plotnine.html#plotnine-i.e.-ggplot",
    "href": "02-plotnine.html#plotnine-i.e.-ggplot",
    "title": "3  Data Visualization in Python",
    "section": "3.3 plotnine (i.e. ggplot)",
    "text": "3.3 plotnine (i.e. ggplot)\nThe plotnine library implements the grammar of graphics in Python.\nCode for the previous example:\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n3.3.1 The aesthetic\n\n(ggplot(penguins, \naes(                           # <1>\n  x = \"species\",               # <2>\n  y = \"bill_length_mm\",        # <2>\n  fill = \"species\"))           # <2>\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\nThe aes() function is the place to specify aesthetics.\nx, y, and fill are three possible aesthetics that can be specified, that map variables in our data set to plot elements.\n\n\n\n3.3.2 The geometry\n\n(ggplot(penguins, \naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"species\"))\n+ geom_boxplot() # <1>\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\nA variety of geom_* functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)\n\n\n\n3.3.3 Other optional elements:\n\nThe scales of the x- and y-axes.\nThe color of elements that are not mapped to aesthetics.\nThe theme of the plot\n\n…and many more!\n\n\n3.3.4 Scales\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\nfrom plotnine import scale_y_reverse\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ scale_y_reverse()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.3.5 Non-aesthetic colors\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot(fill = \"cornflowerblue\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat will this show?\n\n\n(ggplot(penguins, \naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"cornflowerblue\"))\n+ geom_boxplot()\n)\n\n\n3.3.6 Themes\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nversus\n\nfrom plotnine import theme_classic\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ theme_classic()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nExample\n\n\n\nWhat are the differences between the two plots above? What did the theme change?\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat are the aesthetics, geometry, scales, and other options in the cartoon plot below?\n\n\n\nAn xkcd comic of time spent going up the down escalator\n\n\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\nScales: https://ggplot2-book.org/scale-position.html\nThemes: https://ggplot2-book.org/polishing.html"
  },
  {
    "objectID": "02-plotnine.html#geometries-the-big-five",
    "href": "02-plotnine.html#geometries-the-big-five",
    "title": "3  Data Visualization in Python",
    "section": "3.4 Geometries: The “Big Five”",
    "text": "3.4 Geometries: The “Big Five”\n\n3.4.1 1. Bar Plots\nMost often used for showing counts of a categorical variable:\n\nfrom plotnine import geom_bar\n(ggplot(penguins,\naes(\n  x = \"species\"\n))\n+ geom_bar()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n… or relationships between two categorical variables:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"sex\"\n))\n+ geom_bar()\n)\n\nTypeError: '<' not supported between instances of 'str' and 'float'\n\n\nWould we rather see percents?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"fill\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nOr side-by-side?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"dodge\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nExample\n\n\n\nCompare and contrast the plots above? What information is lost or gained between each of them?\n\n\n\n\n3.4.2 2. Boxplots\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nSide-by-side using a categorical variable:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"sex\"\n))\n+ geom_boxplot()\n)\n\nTypeError: '<' not supported between instances of 'str' and 'float'\n\n\n\n\n3.4.3 3. Histograms\n\nfrom plotnine import geom_histogram\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 100)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 10)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.4 3.5 Densities\nSuppose you want to compare histograms by category:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_histogram()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nCleaner: smoothed histogram, or density:\n\nfrom plotnine import geom_density\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nEven cleaner: The alpha option:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density(alpha = 0.5)\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.5 4. Scatterplots\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\"\n))\n+ geom_point()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\nColors for extra information:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\",\n  color = \"species\"\n))\n+ geom_point()\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n3.4.6 5. Line Plots\n\nfrom plotnine import geom_line\npenguins2 = penguins.groupby(by = [\"species\", \"sex\"]).mean()\n\n(ggplot(penguins2,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  color = \"sex\"\n))\n+ geom_point()\n+ geom_line()\n)\n\nTypeError: agg function failed [how->mean,dtype->object]\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\nggplot2 cheatsheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nplotnine: https://plotnine.readthedocs.io/en/stable/"
  },
  {
    "objectID": "02-plotnine.html#multiple-plots",
    "href": "02-plotnine.html#multiple-plots",
    "title": "3  Data Visualization in Python",
    "section": "3.5 Multiple Plots",
    "text": "3.5 Multiple Plots\n\n3.5.1 Facet Wrapping\n\nfrom plotnine import facet_wrap\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n+ facet_wrap(\"sex\")\n)\n\n\n\n\n<Figure Size: (640 x 480)>\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question."
  },
  {
    "objectID": "03-basic_data_operations.html",
    "href": "03-basic_data_operations.html",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "",
    "text": "This document demonstrates the use of the pandas library in Python to do basic data wrangling and summarization.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session, for packages that don’t come pre-installed)."
  },
  {
    "objectID": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "href": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.2 Reading Tabular Data into Python",
    "text": "4.2 Reading Tabular Data into Python\nWe’re going to be exploring pandas in the context of the famous Titanic dataset. We’ll work with a subset of this dataset, but more information about it all can be found here.\nWe start by loading the numpy and pandas libraries. Most of our data wrangling work will happen with functions from the pandas library, but the numpy library will be useful for performing certain mathematical operations should we choose to transform any of our data.\nimport numpy as np\nimport pandas as pd\ndata_dir = \"https://dlsun.github.io/pods/data/\"\ndf_titanic = pd.read_csv(data_dir + \"titanic.csv\")\n\n\n\n\n\n\nExample\n\n\n\nWe’ve already seen read_csv() used many times for importing CSV files into Python, but it bears repeating.\n\n\nData files of many different types and shapes can be read into Python with similar functions, but we will focus on tabular data.\n\n4.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you, navigate to the GapMinder Data site and download a single CSV file of your choice. Open it up in Excel or your application of choice. Is this dataset tidy? If not, then what would have to change to make it tidy?\n\n\n\n\n\n\n\n\nLearn-more\n\n\n\nThe term “tidy data” was first popularized in this paper by R developer Hadley Wickham.\n\n\nYou may have noticed that plotnine (ggplot) is basically built to take tidy data. Variables are specified in the aesthetics function to map them (i.e. columns) in our dataset to plot elements. This type of behavior is EXTREMELY common among functions that work with data in all languages, and so the importance of getting our data into a tidy format cannot be overstated.\nIn Python, there are at least two quick ways to view a dataset we’ve read in:\n\ndf_titanic\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.11\n      0\n    \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      2\n      Abbott, Mr. Rossmore Edward\n      male\n      16.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.13\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2202\n      Wynn, Mr. Walter\n      male\n      41.0\n      deck crew\n      B\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2203\n      Yearsley, Mr. Harry\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2204\n      Young, Mr. Francis James\n      male\n      32.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2205\n      Zanetti, Sig. Minio\n      male\n      20.0\n      restaurant staff\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2206\n      Zarracchi, Sig. L.\n      male\n      26.0\n      restaurant staff\n      S\n      England\n      NaN\n      NaN\n      0\n    \n  \n\n2207 rows × 9 columns\n\n\n\n\ndf_titanic.head()\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.11\n      0\n    \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      2\n      Abbott, Mr. Rossmore Edward\n      male\n      16.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.13\n      1\n    \n  \n\n\n\n\nThe latter (.head()) is usually preferred in case the dataset is large.\n\n\n\n\n\n\nCheck-in\n\n\n\nDoes the titanic dataset appear to be in tidy format?"
  },
  {
    "objectID": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "href": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.3 The “Big Five” Verbs of Data Wrangling",
    "text": "4.3 The “Big Five” Verbs of Data Wrangling\nData wrangling can involve a lot of different steps and operations to get data into a tidy format and ready for analysis and visualization. The vast majority of these fall under the umbrella one the following five operations:\n\nSelect columns/variables of interest\nFilter rows/observations of interest\nArrange the rows of a dataset by column(s) of interest (i.e. order or sort)\nMutate the columns of a dataset (i.e. create or transform variables)\nSummarize the rows of a dataset for column(s) of interest\n\n\n4.3.1 Select Columns/Variables\nSuppose we want to select the age variable from the titanic DataFrame. There are three ways to do this.\n\nUse .loc, specifying both the rows and columns. (The colon : is Python shorthand for “all”.)\n\ndf_titanic.loc[:, \"age\"]\n\nAccess the column as you would a key in a dict.\n\ndf_titanic[\"age\"]\n\nAccess the column as an attribute of the DataFrame.\n\ndf_titanic.age\nMethod 3 (attribute access) is the most concise. However, it does not work if the variable name contains spaces or special characters, begins with a number, or matches an existing attribute of the DataFrame. So, methods 1 and 2 are usually safer and preferred.\nTo select multiple columns, you would pass in a list of variable names, instead of a single variable name. For example, to select both age and fare, either of the two methods below would work (and produce the same result):\n# Method 1\ndf_titanic.loc[:, [\"age\", \"fare\"]].head()\n\n# Method 2\ndf_titanic[[\"age\", \"fare\"]].head()\n\n\n4.3.2 Filter Rows/Observations\n\n4.3.2.1 Selecting Rows/Observations by Location\nBefore we see how to filter (i.e. subset) the rows of dataset based on some condition, let’s see how to select rows by explicitly identifying them.\nWe can select a row by its position using the .iloc attribute. Keeping in mind that the first row is actually row 0, the fourth row could be extracted as:\n\ndf_titanic.iloc[3]\n\nname        Abbott, Mrs. Rhoda Mary 'Rosa'\ngender                              female\nage                                   39.0\nclass                                  3rd\nembarked                                 S\ncountry                            England\nticketno                            2673.0\nfare                                 20.05\nsurvived                                 1\nName: 3, dtype: object\n\n\nNotice that a single row from a DataFrame is no longer a DataFrame but a different data structure, called a Series.\nWe can also select multiple rows by passing a list of positions to .iloc.\n\ndf_titanic.iloc[[1, 3]]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      1\n      Abbott, Mr. Eugene Joseph\n      male\n      13.0\n      3rd\n      S\n      United States\n      2673.0\n      20.05\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.05\n      1\n    \n  \n\n\n\n\nNotice that when we select multiple rows, we get a DataFrame back.\nSo a Series is used to store a single observation (across multiple variables), while a DataFrame is used to store multiple observations (across multiple variables).\nIf selecting consecutive rows, we can use Python’s slice notation. For example, the code below selects all rows from the fourth row, up to (but not including) the tenth row.\n\ndf_titanic.iloc[3:9]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      4\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      3rd\n      S\n      Norway\n      348125.0\n      7.1300\n      1\n    \n    \n      5\n      Abelseth, Mr. Olaus Jørgensen\n      male\n      25.0\n      3rd\n      S\n      United States\n      348122.0\n      7.1300\n      1\n    \n    \n      6\n      Abelson, Mr. Samuel\n      male\n      30.0\n      2nd\n      C\n      France\n      3381.0\n      24.0000\n      0\n    \n    \n      7\n      Abelson, Mrs. Hannah\n      female\n      28.0\n      2nd\n      C\n      France\n      3381.0\n      24.0000\n      1\n    \n    \n      8\n      Abī-Al-Munà, Mr. Nāsīf Qāsim\n      male\n      27.0\n      3rd\n      C\n      Lebanon\n      2699.0\n      18.1509\n      1\n    \n  \n\n\n\n\n\n\n4.3.2.2 Selecting Rows/Observations by Condition\nWe’ll often want to filter or subset the rows of a dataset based on some condition. To do this we’ll take advantage of vectorization and boolean masking.\nRecall that we can compare the values of a variable/column to a particular value in the following way, and observe the result.\n\ndf_titanic[\"age\"] > 30\n\n0        True\n1       False\n2       False\n3        True\n4       False\n        ...  \n2202     True\n2203     True\n2204     True\n2205    False\n2206    False\nName: age, Length: 2207, dtype: bool\n\n\nWe can use these True and False values to filter/subset the dataset! The following subsets the titanic dataset down to only those individuals (rows) with ages over 30.\n\ndf_titanic[df_titanic[\"age\"] > 30]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      0\n      Abbing, Mr. Anthony\n      male\n      42.0\n      3rd\n      S\n      United States\n      5547.0\n      7.1100\n      0\n    \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      12\n      Ahlin, Mrs. Johanna Persdotter\n      female\n      40.0\n      3rd\n      S\n      Sweden\n      7546.0\n      9.0906\n      0\n    \n    \n      15\n      Aldworth, Mr. Augustus Henry\n      male\n      35.0\n      2nd\n      S\n      England\n      248744.0\n      13.0000\n      0\n    \n    \n      21\n      Allen, Mr. William Henry\n      male\n      39.0\n      3rd\n      S\n      England\n      373450.0\n      8.0100\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2197\n      Worthman, Mr. William Henry\n      male\n      37.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2200\n      Wright, Mr. William\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2202\n      Wynn, Mr. Walter\n      male\n      41.0\n      deck crew\n      B\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2203\n      Yearsley, Mr. Harry\n      male\n      40.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2204\n      Young, Mr. Francis James\n      male\n      32.0\n      engineering crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n  \n\n984 rows × 9 columns\n\n\n\nWe can combine multiple conditions using & (and) and | (or). The following subsets the titanic dataset down to females over 30 years of age.\n\ndf_titanic[(df_titanic[\"age\"] > 30) & (df_titanic[\"gender\"] == \"female\")]\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      3\n      Abbott, Mrs. Rhoda Mary 'Rosa'\n      female\n      39.0\n      3rd\n      S\n      England\n      2673.0\n      20.0500\n      1\n    \n    \n      12\n      Ahlin, Mrs. Johanna Persdotter\n      female\n      40.0\n      3rd\n      S\n      Sweden\n      7546.0\n      9.0906\n      0\n    \n    \n      35\n      Andersson, Miss. Ida Augusta Margareta\n      female\n      38.0\n      3rd\n      S\n      Sweden\n      347091.0\n      7.1506\n      0\n    \n    \n      40\n      Andersson, Mrs. Alfrida Konstantia Brogren\n      female\n      39.0\n      3rd\n      S\n      Sweden\n      347082.0\n      31.0506\n      0\n    \n    \n      44\n      Andrews, Miss. Kornelia Theodosia\n      female\n      62.0\n      1st\n      C\n      United States\n      13502.0\n      77.1902\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1997\n      Robinson, Mrs. Annie\n      female\n      41.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2059\n      Smith, Miss. Katherine Elizabeth\n      female\n      45.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2076\n      Stap, Miss. Sarah Agnes\n      female\n      47.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      1\n    \n    \n      2143\n      Wallis, Mrs. Catherine Jane\n      female\n      36.0\n      victualling crew\n      S\n      England\n      NaN\n      NaN\n      0\n    \n    \n      2145\n      Walsh, Miss. Catherine\n      female\n      32.0\n      victualling crew\n      S\n      Ireland\n      NaN\n      NaN\n      0\n    \n  \n\n206 rows × 9 columns\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith the 2-3 people around you, how would you find the just the names of the males under 20 years of age who survived (in the titanic dataset) with a single line of code?\n\n\n\n\n\n4.3.3 Arrange Rows\nAs part of exploratory data analysis and some reporting efforts, we will want to sort a dataset or set of results by one or more variables of interest.\nWe can do this with .sort_values in either ascending or descending order.\nThe following sorts the titanic dataset by age in decreasing order.\n\ndf_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n    \n  \n  \n    \n      1176\n      Svensson, Mr. Johan\n      male\n      74.000000\n      3rd\n      S\n      Sweden\n      347060.0\n      7.1506\n      0\n    \n    \n      820\n      Mitchell, Mr. Henry Michael\n      male\n      72.000000\n      2nd\n      S\n      England\n      24580.0\n      10.1000\n      0\n    \n    \n      53\n      Artagaveytia, Mr. Ramon\n      male\n      71.000000\n      1st\n      C\n      Argentina\n      17609.0\n      49.1001\n      0\n    \n    \n      456\n      Goldschmidt, Mr. George B.\n      male\n      71.000000\n      1st\n      C\n      United States\n      17754.0\n      34.1301\n      0\n    \n    \n      282\n      Crosby, Captain. Edward Gifford\n      male\n      70.000000\n      1st\n      S\n      United States\n      5735.0\n      71.0000\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      Tannūs, Master. As'ad\n      male\n      0.416667\n      3rd\n      C\n      Lebanon\n      2625.0\n      8.1004\n      1\n    \n    \n      296\n      Danbom, Master. Gilbert Sigvard Emanuel\n      male\n      0.333333\n      3rd\n      S\n      Sweden\n      347080.0\n      14.0800\n      0\n    \n    \n      316\n      Dean, Miss. Elizabeth Gladys 'Millvina'\n      female\n      0.166667\n      3rd\n      S\n      England\n      2315.0\n      20.1106\n      1\n    \n    \n      439\n      Gheorgheff, Mr. Stanio\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349254.0\n      7.1711\n      0\n    \n    \n      677\n      Kraeff, Mr. Theodor\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349253.0\n      7.1711\n      0\n    \n  \n\n2207 rows × 9 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that in these last few sections, we have not made any permanent changes to the df_titanic object. We have only asked python do some selecting/filtering/sorting and then to print out the results, not save them.\nIf we wanted df_titanic to become permanently sorted by age, we would re-assign the object:\ndf_titanic = df_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou will sometimes see object reassignment happen in a different way, using an inplace = True argument, like this:\ndf_titanic.sort_values(by = [\"age\"], ascending=False, inplace=True)\nWe strongly recommend against this approach, for two reason:\n\nWhen an object is “overwritten” via reassignment, that’s a major decision; you lose the old version of the object. It should be made deliberately and obviously. The inplace argument is easy to miss when copying/editing code, so it can lead to accidental overwriting that is hard to keep track of.\nNot all functions of DataFrames have an inplace option. It can be frustrating to get into the habit of using it, only to find out the hard way that it’s not available half the time!\n\n\n\n\n\n4.3.4 Mutate Column(s)\nThe variables available to us in our original dataset contain all of the information we have access to, but the best insights may instead come from transformations of those variables.\n\n4.3.4.1 Transforming Quantitative Variables\nOne of the simplest reasons to want to transform a quantitative variable is to change the measurement units.\nHere we change the age of passengers from a value in years to a value in decades.\ndf_titanic[\"age\"] = df_titanic[\"age\"] / 10\nIf we have a quantitative variable that is particularly skewed, then it might be a good idea to transform the values of that variable…like taking the log of the values.\n\n\n\n\n\n\nNote\n\n\n\nThis was a strategy you saw employed with the GapMinder data!\n\n\nBelow is an example of taking the log of the fare variable. Notice that we’re making use of the numpy here to take the log.\ndf_titanic[\"fare\"] = np.log(df_titanic[\"fare\"])\nRemember that we can take advantage of vectorization here too. The following operation wouldn’t really make physical sense, but it’s an example of creating a new variable out of existing variables.\ndf_titanic[\"nonsense\"] = df_titanic[\"fare\"] / df_titanic[\"age\"]\nNote that we created the new variable, nonsense, by specifying on the left side of the = here and populating that column/variable via the expression on the right side of the =.\nWe could want to create a new variable by categorizing (or discretizing) the values of a quantitative variable (i.e. convert a quantitative variable to a categorical variable). We can do so with cut.\nIn the following, we create a new age_cat variable which represents whether a person is a child or an adult.\ndf_titanic[\"age_cat\"] = pd.cut(df_titanic[\"age\"],\n                              bins = [0, 18, 100],\n                              labels = [\"child\", \"adult\"])\n\n\n\n\n\n\nCheck-in\n\n\n\nConsider the four mutations we just performed. In which ones did we reassign a column of the dataset, thus replacing the old values with new ones? In which ones did we create a brand-new column, thus retaining the old column(s) that were involved in the calculation?\n\n\n\n\n4.3.4.2 Transforming Categorical Variables\nIn some situations, especially later with modeling, we’ll need to convert categorical variables (stored as text) into quantitative (often coded) variables. Binary categorical variables can be converted into quantitative variables by coding one category as 1 and the other category as 0. (In fact, the survived column in the titanic dataset has already been coded this way.) The easiest way to do this is to create a boolean mask. For example, to convert gender to a quantitative variable female, which is 1 if the passenger was female and 0 otherwise, we can do the following:\ndf_titanic[\"female\"] = 1 * (df_titanic[\"gender\"] == \"female\")\nWhat do we do about a categorical variable with more than twwo categories, like embarked, which has four categories? In general, a categorical variable with K categories can be converted into K separate 0/1 variables, or dummy variables. Each of the K dummy variables is an indicator for one of the K categories. That is, a dummy variable is 1 if the observation fell into its particular category and 0 otherwise.\nAlthough it is not difficult to create dummy variables manually, the easiest way to create them is the get_dummies() function in pandas.\n\npd.get_dummies(df_titanic[\"embarked\"])\n\n\n\n\n\n  \n    \n      \n      B\n      C\n      Q\n      S\n    \n  \n  \n    \n      1176\n      False\n      False\n      False\n      True\n    \n    \n      820\n      False\n      False\n      False\n      True\n    \n    \n      53\n      False\n      True\n      False\n      False\n    \n    \n      456\n      False\n      True\n      False\n      False\n    \n    \n      282\n      False\n      False\n      False\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      False\n      True\n      False\n      False\n    \n    \n      296\n      False\n      False\n      False\n      True\n    \n    \n      316\n      False\n      False\n      False\n      True\n    \n    \n      439\n      False\n      True\n      False\n      False\n    \n    \n      677\n      False\n      True\n      False\n      False\n    \n  \n\n2207 rows × 4 columns\n\n\n\nWe may also want to change the levels of a categorical variable. A categorical variable can be transformed by mapping its levels to new levels. For example, we may only be interested in whether a person on the titanic was a passenger or a crew member. The variable class is too detailed. We can create a new variable, type, that is derived from the existing variable class. Observations with a class of “1st”, “2nd”, or “3rd” get a value of “passenger”, while observations with a class of “victualling crew”, “engineering crew”, or “deck crew” get a value of “crew”.\n\ndf_titanic[\"type\"] = df_titanic[\"class\"].map({\n    \"1st\": \"passenger\",\n    \"2nd\": \"passenger\",\n    \"3rd\": \"passenger\",\n    \"victualling crew\": \"crew\",\n    \"engineering crew\": \"crew\",\n    \"deck crew\": \"crew\"\n})\n\ndf_titanic\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      age\n      class\n      embarked\n      country\n      ticketno\n      fare\n      survived\n      nonsense\n      age_cat\n      female\n      type\n    \n  \n  \n    \n      1176\n      Svensson, Mr. Johan\n      male\n      7.400000\n      3rd\n      S\n      Sweden\n      347060.0\n      1.967196\n      0\n      0.265837\n      child\n      0\n      passenger\n    \n    \n      820\n      Mitchell, Mr. Henry Michael\n      male\n      7.200000\n      2nd\n      S\n      England\n      24580.0\n      2.312535\n      0\n      0.321185\n      child\n      0\n      passenger\n    \n    \n      53\n      Artagaveytia, Mr. Ramon\n      male\n      7.100000\n      1st\n      C\n      Argentina\n      17609.0\n      3.893861\n      0\n      0.548431\n      child\n      0\n      passenger\n    \n    \n      456\n      Goldschmidt, Mr. George B.\n      male\n      7.100000\n      1st\n      C\n      United States\n      17754.0\n      3.530180\n      0\n      0.497208\n      child\n      0\n      passenger\n    \n    \n      282\n      Crosby, Captain. Edward Gifford\n      male\n      7.000000\n      1st\n      S\n      United States\n      5735.0\n      4.262680\n      0\n      0.608954\n      child\n      0\n      passenger\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1182\n      Tannūs, Master. As'ad\n      male\n      0.041667\n      3rd\n      C\n      Lebanon\n      2625.0\n      2.091913\n      1\n      50.205923\n      child\n      0\n      passenger\n    \n    \n      296\n      Danbom, Master. Gilbert Sigvard Emanuel\n      male\n      0.033333\n      3rd\n      S\n      Sweden\n      347080.0\n      2.644755\n      0\n      79.342661\n      child\n      0\n      passenger\n    \n    \n      316\n      Dean, Miss. Elizabeth Gladys 'Millvina'\n      female\n      0.016667\n      3rd\n      S\n      England\n      2315.0\n      3.001247\n      1\n      180.074822\n      child\n      1\n      passenger\n    \n    \n      439\n      Gheorgheff, Mr. Stanio\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349254.0\n      1.970059\n      0\n      NaN\n      NaN\n      0\n      passenger\n    \n    \n      677\n      Kraeff, Mr. Theodor\n      male\n      NaN\n      3rd\n      C\n      Bulgaria\n      349253.0\n      1.970059\n      0\n      NaN\n      NaN\n      0\n      passenger\n    \n  \n\n2207 rows × 13 columns\n\n\n\n\n\n\n4.3.5 Summarizing Rows\nSummarization of the rows of a dataset for column(s) of interest can take many different forms. This introduction will not be exhaustive, but certainly cover the basics.\n\n4.3.5.1 Summarizing a Quantitative Variable\nThere are a few descriptive statistics that can be computed directly including, but not limited to, the mean and median.\n\ndf_titanic[\"age\"].mean()\n\ndf_titanic[\"age\"].median()\n\ndf_titanic[[\"age\", \"fare\"]].mean()\n\nage     3.043673\nfare    2.918311\ndtype: float64\n\n\nWe can ask for a slightly more comprehensive description using .describe()\n\ndf_titanic[\"age\"].describe()\n\ndf_titanic.describe()\n\n\n\n\n\n  \n    \n      \n      age\n      ticketno\n      fare\n      survived\n      nonsense\n      female\n    \n  \n  \n    \n      count\n      2205.000000\n      1.316000e+03\n      1291.000000\n      2207.000000\n      1289.000000\n      2207.000000\n    \n    \n      mean\n      3.043673\n      2.842157e+05\n      2.918311\n      0.322157\n      2.147877\n      0.221568\n    \n    \n      std\n      1.215968\n      6.334726e+05\n      0.974452\n      0.467409\n      7.237694\n      0.415396\n    \n    \n      min\n      0.016667\n      2.000000e+00\n      1.108728\n      0.000000\n      0.265837\n      0.000000\n    \n    \n      25%\n      2.200000\n      1.426225e+04\n      1.971383\n      0.000000\n      0.742371\n      0.000000\n    \n    \n      50%\n      2.900000\n      1.114265e+05\n      2.645480\n      0.000000\n      0.936833\n      0.000000\n    \n    \n      75%\n      3.800000\n      3.470770e+05\n      3.435945\n      1.000000\n      1.260935\n      0.000000\n    \n    \n      max\n      7.400000\n      3.101317e+06\n      6.238443\n      1.000000\n      180.074822\n      1.000000\n    \n  \n\n\n\n\nNote that, by default, .describe() provides descriptive statistics for only the quantitative variables in the dataset.\nWe can enhance numerical summaries with .groupby(), which allows us to specify one or more variables that we’d like to group our work by.\n\ndf_titanic[[\"age\", \"survived\"]].groupby(\"survived\").mean()\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      survived\n      \n    \n  \n  \n    \n      0\n      3.083194\n    \n    \n      1\n      2.960631\n    \n  \n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you, look up how you would compute the correlation between two quantitative variables in Python. Compute the correlation between the age and fare variables in the titanic dataset.\n\n\n\n\n4.3.5.2 Summarizing a Categorical Variable\nWhen it comes to categorical variables we’re most often interested in frequency distributions (counts), relative frequency distributions, and cross-tabulations.\n\ndf_titanic[\"class\"].unique()\n\ndf_titanic[\"class\"].describe()\n\ncount     2207\nunique       7\ntop        3rd\nfreq       709\nName: class, dtype: object\n\n\nThe .unique() here allows us to see the unique values of the class variable. Notice that the results of .describe() on a categorical variable are much different.\nTo completely summarize a single categorical variable, we report the number of times each level appeared, or its frequency.\n\ndf_titanic[\"class\"].value_counts()\n\nclass\n3rd                 709\nvictualling crew    431\n1st                 324\nengineering crew    324\n2nd                 284\nrestaurant staff     69\ndeck crew            66\nName: count, dtype: int64\n\n\nInstead of reporting counts, we can also report proportions or probabilities, or the relative frequencies. We can calculate the relative frequencies by specifying normalize=True in .value_counts().\n\ndf_titanic[\"class\"].value_counts(normalize=True)\n\nclass\n3rd                 0.321251\nvictualling crew    0.195288\n1st                 0.146806\nengineering crew    0.146806\n2nd                 0.128681\nrestaurant staff    0.031264\ndeck crew           0.029905\nName: proportion, dtype: float64\n\n\nCross-tabulations are one way we can investigate possible relationships between categorical variables. For example, what can we say about the relationship between gender and survival on the Titanic?\n\n\n\n\n\n\nCheck-in\n\n\n\nSummarize gender and survival individually by computing the frequency distributions of each.\n\n\nThis does not tell us how gender interacts with survival. To do that, we need to produce a cross-tabulation, or a “cross-tab” for short. (Statisticians tend to call this a contingency table or a two-way table.)\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"])\n\n\n\n\n\n  \n    \n      gender\n      female\n      male\n    \n    \n      survived\n      \n      \n    \n  \n  \n    \n      0\n      130\n      1366\n    \n    \n      1\n      359\n      352\n    \n  \n\n\n\n\nA cross-tabulation of two categorical variables is a two-dimensional array, with the levels of one variable along the rows and the levels of the other variable along the columns. Each cell in this array contains the number of observations that had a particular combination of levels. So in the Titanic data set, there were 359 females who survived and 1366 males who died. From the cross-tabulation, we can see that there were more females who survived than not, while there were more males who died than not. Clearly, gender had a strong influence on survival because of the Titanic’s policy of “women and children first”.\nTo get probabilities instead of counts, we specify normalize=True.\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"], normalize=True)\n\n\n\n\n\n  \n    \n      gender\n      female\n      male\n    \n    \n      survived\n      \n      \n    \n  \n  \n    \n      0\n      0.058903\n      0.618940\n    \n    \n      1\n      0.162664\n      0.159493\n    \n  \n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat about conditional proportions? With 2-3 people around you, discuss how you would compute the proportion of females that survived and the proportion of males that survived and then do it.\nNote, there are multiple ways to do this.\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question."
  },
  {
    "objectID": "04-pivoting_joining.html",
    "href": "04-pivoting_joining.html",
    "title": "5  Pivoting and Joining",
    "section": "",
    "text": "This document demonstrates the use of the pandas library in Python to do pivoting and joining of datasets.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session).\n\n\n\nimport pandas as pd\n\n\n# Population data from GapMinder\npopulation = pd.read_csv(\"/content/pop.csv\")"
  },
  {
    "objectID": "04-pivoting_joining.html#pivoting-data-in-python",
    "href": "04-pivoting_joining.html#pivoting-data-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.2 Pivoting Data in Python",
    "text": "5.2 Pivoting Data in Python\nData come in all shapes and forms! Rare is the day when we can open a dataset for the first time and it’s ready for every type of visualization or analysis that we could want to do with it.\nIn addition to the wrangling we discussed in the previous chapter, there may be a need to reshape the dataset entirely. For example, the column names might be values themselves that we want to make use of.\nRecall our introduction of tidy data in the previous chapter…\n\n5.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\nIn the previous chapter you were asked to open up a GapMinder dataset here and to comment on whether this dataset was tidy or not. The answer was no, this dataset is not tidy. These datasets come with a row representing a country, each column representing a year, and each cell representing the value of the global indicator selected. To be tidy these three variables (country, year, global indicator) should each have their own column, instead of the year variable taking values as the column headers.\n\n\n5.2.2 Wide to Long Format\nThe GapMinder dataset is an example of what’s commonly referred to as data in a wide format. To make this dataset tidy we aim for a dataset with columns for country, year, and global indicator (e.g. population). Three columns is many fewer than the current number of columns, and so we will convert this dataset from wide to long format.\n\n\n\n\n\n\nWarning\n\n\n\nIt often helps to physically draw/map out what our current dataset looks like and what the look of our target dataset is, before actually trying to write any code to do this. Writing the code can be extremely easier after this exercise, and only makes future pivot operations easier.\n\n\nIn order to convert our dataset from wide to long format we will use .melt() (or .wide_to_long()) in pandas.\n\nlong_population = population.melt(id_vars=[\"country\"], var_name=\"year\", value_name=\"population\")\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWith 2-3 people around you navigate to GapMinder, download the population dataset, and convert it from wide to long format. Does the result look how you expect? Is any other wrangling necessary?\n\n\n\n\n5.2.3 Long to Wide Format\nEven though certain data shapes are not considered tidy, they may be more conducive to performing certain operations than other shapes. For example, what if we were interested in the change in country population between 1950 and 2010? In the original wide shape of the GapMinder data this operation would have been a simple difference of columns like below.\n\npopulation[\"pop_diff\"] = population[\"2010\"] - population[\"1950\"]\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWhy doesn’t the above code work without further wrangling? What in the dataset needs to change for this operation to work?\n\n\nIn the long format of our Gapminder dataset (long_population), this operation is less straightforward. Sometimes datasets come to us in long format and to do things like the operation above we need to convert that dataset from long to wide format. We can go the reverse direction (i.e. long to wide format) with .pivot() in pandas.\n\nwide_population = long_population.pivot(index = \"country\", columns = \"year\", values = \"population\")\nwide_population = wide_population.reset_index()\n\n\n\n\n\n\n\nLearn-more\n\n\n\n\n\nWe haven’t spent much time discussing the index of a pandas DataFrame, but you can think of it like an address for data, or slices of data in a DataFrame. You can also think of an index (or indices) as row names, or axis labels, for your dataset. This can be useful for a number of functions in Python, and can enhance the look of results or visualizations.\nHowever, understanding them is not critical for what we will do in Python. Furthermore, variables that are indices for a DataFrame cannot be accessed or referenced in the same way as other variables in the DataFrame. So, we will avoid their use if possible."
  },
  {
    "objectID": "04-pivoting_joining.html#joining-datasets-in-python",
    "href": "04-pivoting_joining.html#joining-datasets-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.3 Joining Datasets in Python",
    "text": "5.3 Joining Datasets in Python\nThe information you need is often spread across multiple data sets, so you will need to combine multiple data sets into one. In this chapter, we discuss strategies for combining information from multiple (tabular) data sets.\nAs a working example, we will use a data set of baby names collected by the Social Security Administration. Each data set in this collection contains the names of all babies born in the United States in a particular year. This data is publicly available, and a copy has been made available at https://dlsun.github.io/pods/data/names/.\n\n5.3.1 Concatenating and Merging Data\n\n5.3.1.1 Concatenation\nSometimes, the rows of data are spread across multiple files, and we want to combine the rows into a single data set. The process of combining rows from different data sets is known as concatenation.\nVisually, to concatenate two DataFrames, we simply stack them on top of one another.\nFor example, suppose we want to understand how the popularity of different names evolved between 1995 and 2015. The 1995 names and the 2015 names are stored in two different files: yob1995.txt and yob2015.txt, respectively. To carry out this analysis, we will need to combine these two data sets into one.\n\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames1995\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n    \n    \n      1\n      Ashley\n      F\n      26603\n    \n    \n      2\n      Emily\n      F\n      24378\n    \n    \n      3\n      Samantha\n      F\n      21646\n    \n    \n      4\n      Sarah\n      F\n      21369\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      26075\n      Zerek\n      M\n      5\n    \n    \n      26076\n      Zhen\n      M\n      5\n    \n    \n      26077\n      Ziggy\n      M\n      5\n    \n    \n      26078\n      Zuberi\n      M\n      5\n    \n    \n      26079\n      Zyon\n      M\n      5\n    \n  \n\n26080 rows × 3 columns\n\n\n\n\nnames2015 = pd.read_csv(data_dir + \"yob2015.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames2015\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Emma\n      F\n      20455\n    \n    \n      1\n      Olivia\n      F\n      19691\n    \n    \n      2\n      Sophia\n      F\n      17417\n    \n    \n      3\n      Ava\n      F\n      16378\n    \n    \n      4\n      Isabella\n      F\n      15617\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      33116\n      Zykell\n      M\n      5\n    \n    \n      33117\n      Zyking\n      M\n      5\n    \n    \n      33118\n      Zykir\n      M\n      5\n    \n    \n      33119\n      Zyrus\n      M\n      5\n    \n    \n      33120\n      Zyus\n      M\n      5\n    \n  \n\n33121 rows × 3 columns\n\n\n\nTo concatenate the two, we use the pd.concat() function, which accepts a list of pandas objects (DataFrames or Series) and concatenates them.\n\npd.concat([names1995, names2015])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n    \n    \n      1\n      Ashley\n      F\n      26603\n    \n    \n      2\n      Emily\n      F\n      24378\n    \n    \n      3\n      Samantha\n      F\n      21646\n    \n    \n      4\n      Sarah\n      F\n      21369\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      33116\n      Zykell\n      M\n      5\n    \n    \n      33117\n      Zyking\n      M\n      5\n    \n    \n      33118\n      Zykir\n      M\n      5\n    \n    \n      33119\n      Zyrus\n      M\n      5\n    \n    \n      33120\n      Zyus\n      M\n      5\n    \n  \n\n59201 rows × 3 columns\n\n\n\n\nThere is no longer any way to distinguish the 1995 data from the 2015 data. To fix this, we can add a Year column to each DataFrame before we concatenate.\nThe indexes from the original DataFrames are preserved in the concatenated DataFrame. (To see this, observe that the last index in the DataFrame is about 33000, which corresponds to the number of rows in names2015, even though there are 59000 rows in the DataFrame.) That means that there are two rows with an index of 0, two rows with an index of 1, and so on. To force pandas to generate a completely new index for this DataFrame, ignoring the indices from the original DataFrames, we specify ignore_index=True.\n\n\nnames1995[\"Year\"] = 1995\nnames2015[\"Year\"] = 2015\nnames = pd.concat([names1995, names2015], ignore_index=True)\nnames\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      59196\n      Zykell\n      M\n      5\n      2015\n    \n    \n      59197\n      Zyking\n      M\n      5\n      2015\n    \n    \n      59198\n      Zykir\n      M\n      5\n      2015\n    \n    \n      59199\n      Zyrus\n      M\n      5\n      2015\n    \n    \n      59200\n      Zyus\n      M\n      5\n      2015\n    \n  \n\n59201 rows × 4 columns\n\n\n\nNow this is a DataFrame we can use!\n\n\n5.3.1.2 Merging (a.k.a Joining)\nMore commonly, the data sets that we want to combine actually contain different information about the same observations. In other words, instead of stacking the DataFrames on top of each other, as in concatenation, we want to stack them next to each other. The process of combining columns or variables from different data sets is known as merging or joining.\nThe observations may be in a different order in the two data sets, so merging is not as simple as placing the two DataFrames side-by-side.\nMerging is an operation on two DataFrames that returns a third DataFrame. By convention, the first DataFrame is referred to as the one on the “left”, while the second DataFrame is the one on the “right”.\nThis naming convention is reflected in the syntax of the .merge() function in pandas. In the code below, the “left” DataFrame, names1995, is quite literally on the left in the code, while the “right” DataFrame, names2015, is to the right. We also specify the variables to match across the two DataFrames.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Year_x\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      148\n      2015\n    \n  \n\n15680 rows × 6 columns\n\n\n\nThe most important component of merging two datasets is the presence of at least one key variable that both datasets share. This variable is sometimes referred to as an ID variable. It’s this variable that we will want to merge on, i.e. use to combine the two datasets intelligently.\nThe variables that we joined on (Name and Sex) appear once in the final DataFrame. The variable Count, which we did not join on, appears twice—since there was a column called Count in both of the original DataFrames. Notice that pandas automatically appended the suffix _x to the name of the variable from the left DataFrame and _y to the one from the right DataFrame. We can customize the suffixes by specifying the suffixes= parameter.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"], suffixes=(\"1995\", \"2015\"))\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count1995\n      Year1995\n      Count2015\n      Year2015\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      148\n      2015\n    \n  \n\n15680 rows × 6 columns\n\n\n\nIn the code above, we assumed that the columns that we joined on had the same names in the two data sets. What if they had different names? For example, suppose the variable had been called Sex in one data set and Gender in the other. We can specify which variables to use from the left and right data sets using the left_on= and right_on= parameters.\n\n# Create new DataFrames where the column names are different\nnames2015_ = names2015.rename({\"Sex\": \"Gender\"}, axis=1)\n\n# This is how you merge them.\nnames1995.merge(\n    names2015_,\n    left_on=(\"Name\", \"Sex\"),\n    right_on=(\"Name\", \"Gender\")\n)\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Year_x\n      Gender\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      1995\n      F\n      1587\n      2015\n    \n    \n      1\n      Ashley\n      F\n      26603\n      1995\n      F\n      3424\n      2015\n    \n    \n      2\n      Emily\n      F\n      24378\n      1995\n      F\n      11786\n      2015\n    \n    \n      3\n      Samantha\n      F\n      21646\n      1995\n      F\n      5340\n      2015\n    \n    \n      4\n      Sarah\n      F\n      21369\n      1995\n      F\n      4521\n      2015\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      15675\n      Zephan\n      M\n      5\n      1995\n      M\n      23\n      2015\n    \n    \n      15676\n      Zeppelin\n      M\n      5\n      1995\n      M\n      70\n      2015\n    \n    \n      15677\n      Zerek\n      M\n      5\n      1995\n      M\n      5\n      2015\n    \n    \n      15678\n      Ziggy\n      M\n      5\n      1995\n      M\n      44\n      2015\n    \n    \n      15679\n      Zyon\n      M\n      5\n      1995\n      M\n      148\n      2015\n    \n  \n\n15680 rows × 7 columns\n\n\n\n\n\n5.3.1.3 One-to-One and Many-to-One Relationships\nIn the example above, there was at most one combination of Name and Sex in the 2015 data set for each combination of Name and Sex in the 1995 data set. These two data sets are thus said to have a one-to-one relationship. The same would be true of combining two GapMinder datasets.\nHowever, two data sets need not have a one-to-one relationship! Two datasets could have a many-to-one relationship. In general, it’s extremely important to think carefully about what variables each of your two datasets have to begin with, and what variables you want your merged dataset to have…and what that merged dataset will represent with respect to your data.\n\n\n5.3.1.4 Many-to-Many Relationships: A Cautionary Tale\nIt is also possible for multiple rows in the left DataFrame to match multiple rows in the right DataFrame. In this case, the two data sets are said to have a many-to-many relationship. Many-to-many joins can lead to misleading analyses, so it is important to exercise caution when working with many-to-many relationships.\nFor example, in the baby names data set, the Name variable is not uniquely identifying. For example, there are both males and females with the name “Jessie”.\n\njessie1995 = names1995[names1995[\"Name\"] == \"Jessie\"]\njessie1995\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      248\n      Jessie\n      F\n      1138\n      1995\n    \n    \n      16047\n      Jessie\n      M\n      903\n      1995\n    \n  \n\n\n\n\n\njessie2015 = names2015[names2015[\"Name\"] == \"Jessie\"]\njessie2015\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      615\n      Jessie\n      F\n      469\n      2015\n    \n    \n      20009\n      Jessie\n      M\n      233\n      2015\n    \n  \n\n\n\n\nIf we join these two DataFrames on Name, then we will end up with a many-to-many join, since each “Jessie” row in the 1995 data will be paired with each “Jessie” row in the 2015 data.\n\njessie1995.merge(jessie2015, on=[\"Name\"])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex_x\n      Count_x\n      Year_x\n      Sex_y\n      Count_y\n      Year_y\n    \n  \n  \n    \n      0\n      Jessie\n      F\n      1138\n      1995\n      F\n      469\n      2015\n    \n    \n      1\n      Jessie\n      F\n      1138\n      1995\n      M\n      233\n      2015\n    \n    \n      2\n      Jessie\n      M\n      903\n      1995\n      F\n      469\n      2015\n    \n    \n      3\n      Jessie\n      M\n      903\n      1995\n      M\n      233\n      2015\n    \n  \n\n\n\n\nNotice that Jessie ends up appearing four times:\n\nFemale Jessies from 1995 are matched with female Jessies from 2015. (Good!)\nMale Jessies from 1995 are matched with male Jessies from 2015. (Good!)\nFemale Jessies from 1995 are matched with male Jessies from 2015. (This is perhaps undesirable.)\nMale Jessies from 1995 are matched with female Jessies from 2015. (Also unexpected and undesirable.)\n\nIf we had used a data set like this to determine the number of Jessies in 1995, then we would end up with the wrong answer, since we would have double-counted both female and male Jessies as a result of the many-to-many join. This is why it is important to exercise caution when working with (potential) many-to-many relationships.\n\n\n\n5.3.2 Types of Joins\nAbove, we saw how to merge (or join) two data sets by matching on certain variables. But what happens when a row in one DataFrame has no match in the other?\nFirst, let’s investigate how pandas handles this situation by default. The name “Nevaeh”, which is “heaven” spelled backwards, took after Sonny Sandoval of the band P.O.D. gave his daughter the name in 2000. Let’s look at how common this name was five years earlier and five years after.\n\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\n\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\nnames2005 = pd.read_csv(data_dir + \"yob2005.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\n\n\nnames1995[names1995.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n  \n\n\n\n\n\nnames2005[names2005.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n    \n  \n  \n    \n      68\n      Nevaeh\n      F\n      4552\n    \n    \n      21353\n      Nevaeh\n      M\n      56\n    \n  \n\n\n\n\nIn 1995, there were no girls (at least fewer than 5) named Nevaeh; just eight years later, there were over 4500 girls (and even 56 boys) with the name. It seems like Sonny Sandoval had a huge effect.\nWhat happens to the name “Nevaeh” when we merge the two data sets?\n\nnames = names1995.merge(names2005, on=[\"Name\", \"Sex\"])\nnames[names.Name == \"Nevaeh\"]\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n  \n\n\n\n\nBy default, pandas only includes combinations that are present in both DataFrames. If it cannot find a match for a row in one DataFrame, then the combination is simply dropped.\nBut in this context, the fact that a name does not appear in one data set is informative. It means that no babies were born in that year with that name. We might want to include names that appeared in only one of the two DataFrames, rather than just the names that appeared in both.\nThere are four types of joins, distinguished by whether they include the rows from the left DataFrame, the right DataFrame, both, or neither:\n\ninner join (default): only values that are present in both DataFrames are included in the result\nouter join: any value that appears in either DataFrame is included in the result\nleft join: any value that appears in the left DataFrame is included in the result, whether or not it appears in the right DataFrame\nright join: any value that appears in the right DataFrame is included in the result, whether or not it appears in the left DataFrame.\n\nOne way to visualize the different types of joins is using Venn diagrams. The shaded region indicates which rows that are included in the output. For example, only rows that appear in both the left and right DataFrames are included in the output of an inner join.\n\nIn pandas, the join type is specified using the how= argument.\nNow let’s look at the examples of each of these types of joins.\n\n# inner join\nnames_inner = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"inner\")\nnames_inner\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      8108\n    \n    \n      1\n      Ashley\n      F\n      26603\n      13270\n    \n    \n      2\n      Emily\n      F\n      24378\n      23930\n    \n    \n      3\n      Samantha\n      F\n      21646\n      13633\n    \n    \n      4\n      Sarah\n      F\n      21369\n      11527\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      19119\n      Zeppelin\n      M\n      5\n      7\n    \n    \n      19120\n      Zerek\n      M\n      5\n      8\n    \n    \n      19121\n      Zhen\n      M\n      5\n      7\n    \n    \n      19122\n      Ziggy\n      M\n      5\n      6\n    \n    \n      19123\n      Zyon\n      M\n      5\n      102\n    \n  \n\n19124 rows × 4 columns\n\n\n\n\n# outer join\nnames_outer = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"outer\")\nnames_outer\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935.0\n      8108.0\n    \n    \n      1\n      Ashley\n      F\n      26603.0\n      13270.0\n    \n    \n      2\n      Emily\n      F\n      24378.0\n      23930.0\n    \n    \n      3\n      Samantha\n      F\n      21646.0\n      13633.0\n    \n    \n      4\n      Sarah\n      F\n      21369.0\n      11527.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      39490\n      Zymiere\n      M\n      NaN\n      5.0\n    \n    \n      39491\n      Zyrell\n      M\n      NaN\n      5.0\n    \n    \n      39492\n      Zyrian\n      M\n      NaN\n      5.0\n    \n    \n      39493\n      Zyshon\n      M\n      NaN\n      5.0\n    \n    \n      39494\n      Zytavious\n      M\n      NaN\n      5.0\n    \n  \n\n39495 rows × 4 columns\n\n\n\nNames like “Zyrell” and “Zyron” appeared in the 2005 data but not the 1995 data. For this reason, their count in 1995 is NaN. In general, there will be missing values in DataFrames that result from an outer join. Any time a value appears in one DataFrame but not the other, there will be NaNs in the columns from the DataFrame missing that value.\n\nnames_inner.isnull().sum()\n\nName       0\nSex        0\nCount_x    0\nCount_y    0\ndtype: int64\n\n\nLeft and right joins preserve data from one DataFrame but not the other. For example, if we were trying to calculate the percentage change for each name from 1995 to 2005, we would want to include all of the names that appeared in the 1995 data. If the name did not appear in the 2005 data, then that is informative.\n\n# left join\nnames_left = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"left\")\nnames_left\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Jessica\n      F\n      27935\n      8108.0\n    \n    \n      1\n      Ashley\n      F\n      26603\n      13270.0\n    \n    \n      2\n      Emily\n      F\n      24378\n      23930.0\n    \n    \n      3\n      Samantha\n      F\n      21646\n      13633.0\n    \n    \n      4\n      Sarah\n      F\n      21369\n      11527.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      26075\n      Zerek\n      M\n      5\n      8.0\n    \n    \n      26076\n      Zhen\n      M\n      5\n      7.0\n    \n    \n      26077\n      Ziggy\n      M\n      5\n      6.0\n    \n    \n      26078\n      Zuberi\n      M\n      5\n      NaN\n    \n    \n      26079\n      Zyon\n      M\n      5\n      102.0\n    \n  \n\n26080 rows × 4 columns\n\n\n\nThe result of the left join has NaNs in the columns from the right DataFrame.\n\nnames_left.isnull().sum()\n\nName          0\nSex           0\nCount_x       0\nCount_y    6956\ndtype: int64\n\n\nThe result of the right join, on the other hand, has NaNs in the column from the left DataFrame.\n\n# right join\nnames_right = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"right\")\nnames_right\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count_x\n      Count_y\n    \n  \n  \n    \n      0\n      Emily\n      F\n      24378.0\n      23930\n    \n    \n      1\n      Emma\n      F\n      5041.0\n      20335\n    \n    \n      2\n      Madison\n      F\n      9775.0\n      19562\n    \n    \n      3\n      Abigail\n      F\n      7821.0\n      15747\n    \n    \n      4\n      Olivia\n      F\n      7624.0\n      15691\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32534\n      Zymiere\n      M\n      NaN\n      5\n    \n    \n      32535\n      Zyrell\n      M\n      NaN\n      5\n    \n    \n      32536\n      Zyrian\n      M\n      NaN\n      5\n    \n    \n      32537\n      Zyshon\n      M\n      NaN\n      5\n    \n    \n      32538\n      Zytavious\n      M\n      NaN\n      5\n    \n  \n\n32539 rows × 4 columns\n\n\n\n\nnames_right.isnull().sum()\n\nName           0\nSex            0\nCount_x    13415\nCount_y        0\ndtype: int64\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nDownload a second GapMinder dataset and merge it with the population dataset from above. Did you have to pivot first? Which order of operations makes the most sense? Is your resulting dataset tidy?\n\n\n\n\n\n\n\n\nPractice-exercise\n\n\n\nCollege Puzzle PA here…"
  }
]