[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GSB 544: Data Science and Machine Learning with Python",
    "section": "",
    "text": "Preface\nThis text was created for the Cal Poly course “GSB 544: Data Science and Machine Learning with Python” by Dr. Kelly Bodwin and Dr. Hunter Glanz. Some parts of the material and text are borrowed from Dr. Emily Robinson’s R course and Dr. Dennis Sun’s python course\nThis text is not meant to be a complete course or textbook by itself; rather, think of it as “long-form” class slides. We will summarize the main concepts in each chapter, show you examples, point you to more in-depth readings from outside sources, and ask you to try out short tasks in python as you go.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-text",
    "href": "index.html#how-to-use-this-text",
    "title": "GSB 544: Data Science and Machine Learning with Python",
    "section": "How to Use This Text",
    "text": "How to Use This Text\n\n\n\n\n\n\nWarning\n\n\n\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n\n\n\n\n\n\n\nExample\n\n\n\nExample sections contain code and other information. Don’t skip them!\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote sections contain clarification points (anywhere I would normally say “note that ….). Make sure to read them to avoid any common pitfalls or misconceptions.\n\n\n\n\n\n\n\n\nRequired Reading\n\n\n\nConsider these sections to be required readings. This is where we will direct you to existing materials to explain or introduce a concept.\n\n\n\n\n\n\n\n\nRequired Video\n\n\n\nSimilarly, consider these sections to be required viewing for the course material.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nCheck-in sections contain small tasks that you need to do throughout the reading, to practice or prepare. Although they are not graded, please treat them as required!\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nEach chapter will have a longer practice exercise to complete and turn in. These are intended to be done with help from instructors and peers.\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nConsider these to be optional readings/viewings. The world of python programming has so many interesting tidbits, we can’t possibly teach them all - but we want to share them with you nonetheless!\nWe will usually make these “click to expand” so that they don’t distract from your reading.\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nThese are personal opinion comments from the authors. Take them with a grain of salt; we aren’t the only python programmers worth listening to, we are just sharing what has worked for us.\nWe will usually make these “click to expand” so that they don’t distract from your reading.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "GSB 544: Data Science and Machine Learning with Python",
    "section": "Additional Resources",
    "text": "Additional Resources\nReferences or additional readings may come from the following texts:\n\nPython Data Science Handbook by Jake VanderPlas\nPython for Data Analysis by Wes McKinney\nPrinciples of Data Science by Dennis Sun\n\nFor extra practice with python programming, we recommend the DataQuest interactive tutorials or working through some lessons on Python for Everybody.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-setup.html",
    "href": "00-setup.html",
    "title": "1  Intro and Setup",
    "section": "",
    "text": "Objectives\nWhile most students will arrive having taken an introductory programming course and/or Summer python intensive workshop, it is important to start this class with some computer fundamentals and some setup and workflow preparation.\nThis chapter is meant to provide a resource for some basic computer skills and a guide to the workflow we expect you to follow when managing your code and data.\nIn this chapter you will:\nIf you already feel comfortable with your own file organization system; you prefer GitLab over GitHub; or you prefer to use another python distribution and IDE (like VSCode), that is acceptable. Just know that we may be less able to help you and troubleshoot if you deviate from the recommended workflow.\nFor grading consistency, we will require that you submit quarto-rendered documents for all labs and projects.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#ch0-objectives",
    "href": "00-setup.html#ch0-objectives",
    "title": "1  Intro and Setup",
    "section": "",
    "text": "Learn the basics of a computer system.\nCreate a GitHub account and practice using it for managing code and data artifacts.\nPractice using Google Colab notebooks for quick tasks and activities.\nInstall python locally on your machine with Anaconda.\nPractice opening and using jupyter notebooks in Positron or VSCode or another IDE.\nInstall the quarto document rendering system.\nPractice using quarto to render nicely formatted html documents from jupyter notebooks.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#computer-basics",
    "href": "00-setup.html#computer-basics",
    "title": "1  Intro and Setup",
    "section": "1.1 Computer Basics",
    "text": "1.1 Computer Basics\nIt is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\n\n1.1.1 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\n\n\nLearn More\n\n\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\n\n\n\n1.1.2 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n\nLearn More\n\n\n\n\n\n\n\n\n1.1.3 File Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\n\n\n\n\n\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\n\n\nRequired Video\n\n\n\n\nStop watching at 4:16.\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\n\n\nRequired Video\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nRecommend watching - helpful for understanding file paths!\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#git-and-github",
    "href": "00-setup.html#git-and-github",
    "title": "1  Intro and Setup",
    "section": "1.2 Git and GitHub",
    "text": "1.2 Git and GitHub\nOne of the most important parts of a data scientist’s workflow is version tracking: the process of making sure that you have a record of the changes and updates you have made to your code.\n\n1.2.1 Git\nGit is a computer program that lives on your local computer. Once you designate a folder as a Git Repository, the program will automatically tracks changes to the files in side that folder.\n\n\n\n\n\n\nCheck In\n\n\n\nClick here to install Git on your computer.\n\n\n\n\n1.2.2 GitHub\nGitHub, and the less used alternate GitLab, are websites where Git Repositories can be stored online. This is useful for sharing your repository (“repo”) with others, for multiple people collaborating on the same repository, and for yourself to be able to access your files from anywhere.\n\n\n\n\n\n\nCheck In\n\n\n\nClick here to make a GitHub account, if you do not already have one.\nYou do not have to use your school email for this account.\n\n\n\n\n1.2.3 Practice with Repos\nIf you are already familiar with how to use Git and GitHub, you can skip the rest of this section, which will walk us through some practice making and editing repositories.\nFirst, watch this 15-minute video, which nicely illustrates the basics of version tracking:\n\n\n\n\n\n\nRequired Video\n\n\n\n\n\n\nThen, watch this 10-minute video, which introduces the idea of branches, and important habit for collaborating with others (or your future self!)\n\n\n\n\n\n\nRequired Video\n\n\n\n\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nAlthough Git can sometimes be a headache, it is worth the struggle. Never again will you have to deal with a folder full of documents that looks like:\nProject-Final\nProject-Final-1\nProject-Final-again\nProject-Final-1-1\nProject-Final-for-real\n\n\nWorking with Git and GitHub can be made a lot easier by helper tools and apps. We recommend GitHub Desktop for your committing and pushing.\n\n\n1.2.4 Summary\nFor our purposes, it will be sufficient for you to learn to:\n\ncommit your work frequently as you make progress; about as often as you might save a document\npush your work every time you step away from your project\nbranch your repo when you want to try something and you aren’t sure it will work.\n\nIt will probably take you some time to get used to a workflow that feels natural to you - that’s okay! As long as you are trying out version control, you’re doing great.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#anaconda-and-jupyter",
    "href": "00-setup.html#anaconda-and-jupyter",
    "title": "1  Intro and Setup",
    "section": "1.3 Anaconda and Jupyter",
    "text": "1.3 Anaconda and Jupyter\nNow, let’s talk about getting python actually set up and running.\n\n\n\n\n\n\n1.3.1 Anaconda\nOne downside of python is that it can sometimes be complicated to keep track of installs and updates.\n\n\n\n\n\nUnless you already have a python environment setup that works for you, we will suggest that you use Anaconda, which bundles together an installation of the most recent python version as well as multiple tools for interacting with the code.\n\n\n\n\n\n\nCheck In\n\n\n\nDownload Anaconda here\n\n\n\n\n1.3.2 Jupyter\nWhen you are writing ordinary text, you choose what type of document to use - Microsoft Word, Google Docs, LaTeX, etc.\nSimilarly, there are many types of files where you can write python code. By far the most common and popular is the jupyter notebook.\nThe advantage of a jupyter notebook is that ordinary text and “chunks” of code can be interspersed.\n\n\n\n\n\nJupyter notebooks have the file extension .ipynb for “i python notebook”.\n\n1.3.2.1 Google Colab\nOne way you may have seen the Jupyter notebooks before is on Google’s free cloud service, Google Colab.\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, to practice using Jupyter notebooks.\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nColab is an amazing data science tool that allows for easy collaboration.\nHowever, there is a limited amount of free compute time offered by Google, and not as much flexibility or control over the documents.\nThis is why we need Anaconda or similar local installations.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#quarto",
    "href": "00-setup.html#quarto",
    "title": "1  Intro and Setup",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nAlthough jupyter and Colab are fantastic tools for data analysis, one major limitation is that the raw notebooks themselves are not the same as a final clear report.\nTo convert our interactive notebooks into professionally presented static documents, we will use a program called Quarto.\n\n\n\n\n\n\nCheck In\n\n\n\nDownload Quarto here\n\n\nOnce quarto is installed, converting a .ipynb file requires running only a single line in the Terminal:\nquarto render /path/to/my_file.ipynb\n\n\n\n\n\n\nCheck In\n\n\n\nDownload the .ipynb file from your practice Colab notebook and render it using Quarto.\n\n\nHowever, there are also many, many options to make the final rendered document look even more visually pleasing and professional. Have a look at the Quarto documentation if you want to play around with themes, fonts, layouts, and so on.\n\n\n\n\n\n\nLearn More\n\n\n\nhttps://quarto.org/docs/get-started/hello/jupyter.html",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#ides",
    "href": "00-setup.html#ides",
    "title": "1  Intro and Setup",
    "section": "1.5 IDEs",
    "text": "1.5 IDEs\nHave you ever watched an old movie or TV show, and the “hacker” character is typing on a screen that looks like this?\n\n\n\n\n\nNobody in today’s programming world interacts with code as plain green text on a black screen. Instead, we take advantage of the tools available to make coding and data analysis more user-friendly. This usually means using an Integrated Developer Environment, or IDE: an application on your computer that provides a nice user interface for writing code.\nIn this class, you may use any IDE you prefer. You might be able to get by with only using Google Colab - but for many projects and assignments, this approach will be extremely inconvenient and require some frustrating steps in your workflow.\nInstead, we would like to encourage you to use one of the following tools:\n\n1.5.1 VS Code\nWe mention this first because it is currently the most commonly used IDE for programming. VSCode (“Visual Studio Code”) is released by Microsoft, and provides a clean interface, with multiple panels to help you organize multiple files.\n\n\n\n\n\nPro: VSCode is very widely used; if you get comfortable in it now, you may be well prepared to work on teams that use it in the future.\nCon: VSCode is more targeted at software development than data science. It lacks several data-specific conveniences that other IDEs implement, such as the ability to explore a loaded dataset in the IDE.\n\n\n1.5.2 Positron (our current recommendation)\nPositron is a data science targeted IDE recently released by Posit, PBC. It is build on the original open-source structure used by VSCode, so it has many similarities.\n\n\n\n\n\nPro: Nearly all the great elements of VSCode, plus some extra conveniences for data analysis and multi-lingual coding.\nCon: It is currently in Beta stages, so the workflow you get used to now could change in the future.\n\n\n\n\n\n\nJust our opinion...\n\n\n\nWe are always hesitant to recommend a tool in class that is only in Beta testing phase. However, we firmly believe that Positron is the future go-to IDE for data science; and even in its current early form, it provides the smoothest workflow from jupyter notebook to Quarto document that we know of.\n\n\n\n\n\n\n\n\nNote\n\n\n\nImportant: If you are on a PC, you will need to enable the Quarto Extension in Positron for it to work.\n\n\n\n\n1.5.3 RStudio\nFor R users, there has historically only been one IDE worth using: RStudio. RStudio does support python and jupyter - so it is certainly a viable option for this class. However, there can be some frustrations in getting RStudio to recognize the correct installations of python and its packages. If you are already very familiar with RStudio, it might be worth working through the initial install challenges so you can stick with your comfort zone.\n\n\n\n\n\nPros: A very optimized data science IDE that is particularly great for R and for Quarto.\nCons: Can be a bit frustrating to use with python; most python users prefer VSCode or Positron.\n\n\n1.5.4 Jupyter\nTechnically speaking, jupyter does provide its own built-in IDE, which is essentially a plain jupyter notebook much like Google Colab.\nYou can access this through Anaconda.\n \nWe do not recommend this workflow; as it can get rather tedious to click through your folder structure and open individual documents, and none of the conveniences of an IDE are available.\n\n1.5.4.1 Try it!\n\n\n\n\n\n\nCheck In\n\n\n\nChoose an IDE to install. Then, download the .ipynb file from your practice Colab notebook and open it in that IDE. Play around with the notebook, and perhaps choose some color schemes that you prefer.\nThen, from your IDE, render the .ipynb file using Quarto.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "00-setup.html#the-new-world-of-genai",
    "href": "00-setup.html#the-new-world-of-genai",
    "title": "1  Intro and Setup",
    "section": "1.6 The new world of GenAI",
    "text": "1.6 The new world of GenAI\nEvery now and then, a technological advance comes along that changes the world very suddenly. These moments are exciting, but also particularly challenging for educators: how can we advise our students when this tool is as new to us as it is to them?\nOur goal for this class is for you to leave with a good idea of how to use GenAI tools responsibly and effectively. We want to think carefully about the ethical concerns with AI, but we also want to find ways that AI can make our data analysis process stronger, better, and easier.\nWhile we have put a lot of effort into the activities and resources in this textbook, we also hope that you will view this learning process as a collaborative one between us and you. As you encounter new ideas, new tools, or new challenges, we hope you will not hesitate to share those with us and your classmates.\n\n1.6.1 A brief history\nAlthough it is far from the first Generative AI Model, GenAI exploded on the public scene with the introduction of ChatGPT on November 30, 2022. ChatGPT stands for generative pre-trained transformer, and it is released by OpenAI, an organization founded in 2015. (The “original” ChatGPT - the first one released to the public - is actually version 3.5.)\nSince then, advances have been rapid. Several similar GenAI tools have popped up, including: Google’s Gemini, Anthropic’s Claude, Meta’s Llama, and Baidu’s Ernie. As of writing this in 2024, the most cutting-edge model is Chat-GPT 4o, the 3rd major public release from OpenAI.\nThese tools are often referred to as LLMs (Large Language Models), since they focus on generating human-like text; unlike other generative AI models that might produce images, sounds, numeric output, etc.\n\n\n1.6.2 The birth of an LLM\nThe principle behind an LLM is not complicated, and not different than the predictive models we will study in this class.\n\nGather some training data, which provides examples of possible inputs and the corresponding correct outputs.\nSpecify a model equation. For example, in simple linear regression, the equation is output = slope times input plus intercept, often written as y = mx + b\nUse the training data to train the model; i.e. to calculate the parameters. In linear regression, the two parameters are m and b, the slope and intercept.\nWhen new data becomes available, where we only have the input values, plug these values into the trained model equation to obtain predicted output values.\n\nSo, what makes an LLM so extremely different from a simple linear regression? Three elements:\n\nQuantity of training data. You might train a simple linear regression on a dataset with 100 rows and two columns. The dataset used to train ChatGPT 3.5 is unimaginably large: essentially, the entirely of all text written by humans available on the internet between 2016 and 2019, equating to roughly 500 billion rows of data.\nComplexity of the model. Note that a complex model is not the same as a complicated one: complexity means that we have many, many parameters. The model equations themselves remain relatively simple. A simple linear regression has two parameters: slope and intercept. The original ChatGPT 3.5, and most of the other models mentioned, have around 100-200 billion parameters. ChatGPT 4.0 is estimated to have 1.76 trillion parameters.\nGenerative vs predictive. A classic predictive model will typically input a single number \\(x\\) and output a single predicted number \\(y\\). A generative model takes human text and input, and instead outputs a probability distribution giving the probability of every possible word that could be a response to the human prompt. The LLM then samples from that distribution, randomly choosing a word, and then the next word, and then the next. This means that giving an LLM identical input won’t result in identical output.\n\n\n1.6.2.1 Cost and resources\nIf the procedure and equations for an LLM are not overly complicated, why did they not come on the scene until 2022?\nThe answer is quite simple: training an LLM, with such enormous data and complexity, requires an absolutely unbelievable amount of computing resources.\nThe cost of training ChatGPT-3.5 was approximately $5 million. It took several months to train, on a massive supercomputer running constantly the whole time. The amount of computation power used can be thought of like this: If you were to buy a top-of-the line personal laptop, for about $4000, you would need to run it continuously for almost 700 years to train ChatGPT-3.5. (source)\nThe size of ChatGPT-4 is not public; however, it is estimated to have cost $100 million.\nAll this is to say: to create an LLM on the scale of those popular today, an organization must have the funding, resources, and time to…\n\n… gather or purchase an inconceivable amount of data …\n… build a networked supercomputer …\n… run the supercomputer continuously for months …\n\n… before the LLM can be used for anything at all!\n\n\n1.6.2.2 The prediction step\nAll of this is what is needed to train the model. What about the prediction step? (Sometimes called inference in the AI world; although this term has a different meaning in statisics.)\nWell, once the model parameters are computed, putting in input and getting back output is a much lower computation load. But it is not totally free - and as you can imaging, there are a huge number of prompts and responses to the models every day.\nThe cost of a typical prompt-and-response to ChatGPT 3.5 is about $0.0025, i.e., a quarter of a penny. For ChatGPT 4, it is around 9 cents. For this reason, at present users must pay for access to ChatGPT 4 but not 3.5. (source)\n\n\n1.6.2.3 Environmental concerns\nAnother important consideration of the ongoing costs of GenAI is the use of earth’s resources.\nThe training and ongoing use of the model requires a supercomputer, housed in a large warehouse. These are located near water sources, both for the hydroelectric power, and to use water for cooling; leading to some concerns about the quantity of fresh water being used.\nPerhaps more prominently, the supercomputers of course use massive amounts of electricity, producing a carbon footprint of C02 emissions from the generation process. The carbon footprint from training ChatGPT-3.5 is the same as driving about 120 vehicles for a year. This is, arguably, not a massively impactful amount in the context of human life - but many have concerns about this footprint scaling up rapidly, as we train more different and more complex models, and as the use of these trained models increases. (source)\n\n\n\n1.6.3 Copyright and plagiarism issues\nIn the coming chapters, we will talk more about the responsibility of users of GenAI, like you and me, to make sure not to plagiarize. For now, though, we are focused on the creation of the LLMs.\nRecall that LLMs are essentially trained on text scraped from the internet. Text produced by an LLM is not new - it is simply the model’s rearrangement of the words and letters that it learned from the training data. So - is this ethical and legal use of the training data gathered from online?\nOne extreme opinion is that no use of any GenAI tool is ethical unless all training data was given with explicit permission of the author.\nIf I copy text from a website and share it without attribution, that is plagiarism. If I copy text from three websites, combine it together, and share it without attribution, that is plagiarism. If a GenAI copies text from thousands of websites and combines it, perhaps that too is always plagiarism.\nAn opinion on the other extreme is that any data available in public is acceptable to use for training.\nThis philosophy holds that, essentially, all human writing is the result of our brains processing “training data” from all the text we have ever read, and rearranging the words in new ways. Why is an LLM any different?\nThe core question is: Is training a model a fair use of Intellectual Property? It is a question without a clear answer at present, and it is being hotly debated in philosophical and legal settings.\n\n\n\n\n\n\nLearn More\n\n\n\nThis question is particularly relevant in the world of generative image models, where many artists feel that their work is being incorporated into generative AI art without permission. We won’t focus on image generation in this class, but this article\n\n\nIn this class, we will take the perspective that the current GenAI tools are, for better or for worse, legal and publicly available; and thus, we will make use of them in our work. At the same time, we will do our best to think critically about the ethical questions at stake, and to be responsible citizens of the new AI world.\n\n\n\n\n\n\nCheck In\n\n\n\nTake a moment to write a brief reflection on the following questions:\n\nWhat excites you the most about the current GenAI tools available? What scares you most?\nDo you currently pay for a GenAI tool? If so, what motivated you to do so?\nWhat is your current opinion on the copyright question for training data?",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro and Setup</span>"
    ]
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "2  Programming Basics",
    "section": "",
    "text": "Objectives\nIn this chapter, we will review some basics of general computer programming, and how they appear in python.\nWe will also introduce the role of GenAI in programming and how it will be covered in this course.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#ch0-objectives",
    "href": "01-basics.html#ch0-objectives",
    "title": "2  Programming Basics",
    "section": "",
    "text": "Learn More\n\n\n\nSome of you may find this material to be unneeded review - if so, great!\nBut if you are new to programming, or it has been a while, this tutorial may help you refresh your knowledge.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#basic-data-types",
    "href": "01-basics.html#basic-data-types",
    "title": "2  Programming Basics",
    "section": "2.1 Basic Data Types",
    "text": "2.1 Basic Data Types\nIt is important to have a base grasp on the types of data you might see in data analyses.\n\n2.1.1 Values and Types\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn python, there are some very basic data types:\n\nlogical or boolean - False/True or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\ndouble or float - decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).1\n\nnumeric - python uses the name numeric to indicate a decimal value, regardless of precision.\ncharacter or string or object - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, python has a function to help you with that.\n\ntype(False)\ntype(2) # by default, python treats whole numbers as integers\ntype(2.0)  # to force it not to be an integer, add a .0\ntype(\"Hello, programmer!\")\n\nstr\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn python, boolean values are True and False. Capitalization matters a LOT.\nOther details: if we try to write a million, we would write it 1000000 instead of 1,000,000. Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.\n\n\n\n\n2.1.2 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nIn python, we assign variables values using the syntax object_name = value You can read this as “object name gets value” in your head.\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\nmessage + \", sang the dolphins.\"\n\nyear + the_answer\n\nnot earth_demolished\n\nTrue\n\n\n\n2.1.2.1 Valid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, and _.\nWhat happens if we try to create a variable name that isn’t valid?\nStarting a variable name with a number will get you an error message that lets you know that something isn’t right.\n\n1st_thing = \"No starting with numbers\"\n\nfirst~thing = \"No other symbols\"\n\nfirst.thing = \"Periods have a particular meaning!\"\n\nSyntaxError: invalid syntax (3761243318.py, line 1)\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n\n\n\n\n\nLearn More\n\n\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\n\nThere are a few different conventions for naming things that may be useful:\n\nsome_people_use_snake_case, where words are separated by underscores\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nA few people mix conventions with variables_thatLookLike_this and they are almost universally hated.\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.\n\n\n\n2.1.3 Type Conversions\nWe talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. R is dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\ntype(2 + 3.14159) # add integer 2 and pi\ntype(2 + True) # add integer 2 and TRUE\ntype(True + False) # add TRUE and FALSE\n\nint\n\n\nAll of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how python handles type conversions.\n\n\n\n\n\n\nCheck In\n\n\n\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “NA” or ” ” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell python that we want our string to be treated as a number, or vice-versa.\nIn python, we can explicitly convert a variable’s type using functions (int, float, str, etc.).\n\nx = 3\ny = \"3.14159\"\n\nx + y\n\nx + float(y)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#operators-and-functions",
    "href": "01-basics.html#operators-and-functions",
    "title": "2  Programming Basics",
    "section": "2.2 Operators and Functions",
    "text": "2.2 Operators and Functions\nIn addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\n\nOperation\npython symbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nInteger Division\n//\n\n\nModular Division\n%\n\n\nExponentiation\n**\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nSo 14 // 3 would be 4, and 14 % 3 would be 2.\n\n14 // 3\n14 % 3\n\n2\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated.\n\n2.2.1 Order of Operations\npython operates under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n(1+1)**(5-2) # 2 ^ 3 = 8\n1 + 2**3 * 4 # 1 + (8 * 4)\n3*1**3 # 3 * 1\n\n3\n\n\n\n\n2.2.2 String Operations\nThe + operator also works on strings. Just remember that python doesn’t speak English - it neither knows nor cares if your strings are words, sentences, etc. So if you want to create good punctuation or spacing, that needs to be done in the code.\n\ngreeting = \"howdy\"\nperson = \"pardner\"\n\ngreeting + person\ngreeting + \", \" + person\n\n'howdy, pardner'\n\n\n\n\n2.2.3 Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. We only need to know how to use functions. Let’s look at the official documentation for the function round()`.\nround(number, ndigits=None)\n\nReturn number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input.\nThis tells us that the function requires one argument, number, a number to round. You also have the option to include a second argument, ndigits, if you want to round to something other than a whole number.\nWhen you call a function, you can either use the names of the arguments, or simply provide the information in the expected order.\nBy convention, we usually use names for optional arguments but not required ones.\n\nround(number = 2.718)\nround(2.718, 2)\n\n\nround(2.718)\nround(2.718, ndigits = 2)\n\n2.72\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe names of functions and their arguments are chosen by the developer who created them. You should never simply assume what a function or argument will do based on the name; always check documentation or try small test examples if you aren’t sure.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#data-structures",
    "href": "01-basics.html#data-structures",
    "title": "2  Programming Basics",
    "section": "2.3 Data Structures",
    "text": "2.3 Data Structures\nIn the previous section, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\nMethods or attributes are a special type of function that operate only on a specific data structure When using a method in python, you can use a period . to apply the function to an object.\n\nmy_nums = [1,2,3,4,5]\nmy_nums.sort()\n\nCareful, though! If a function is not specifically designed to be an attribute of the structure, this . trick won’t work.\n\nmy_nums.round()\n\nAttributeError: 'list' object has no attribute 'round'\n\n\n\n2.3.1 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nx = [\"a\", 3, True]\nx\n\n['a', 3, True]\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n2.3.1.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)3.\nIn python, we count from 0.\n\nx = [\"a\", 3, True]\n\nx[0] # This returns a list\nx[0:2] # This returns multiple elements in the list\n\nx.pop(0)\n\n'a'\n\n\nList indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use the .pop attribute. The only downside to .pop is that you can only access one thing at a time.\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n\n\n2.3.2 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \nBase python does not actually have a vector-type object! However, in data analysis we often have reasons to want a single-type data structure, so we will load an extra function called array from the numpy library to help us out. (More on libraries later!)\n\nfrom numpy import array\n\ndigits_pi = array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n# Access individual entries\ndigits_pi[1]\n\n# Print out the vector\ndigits_pi\n\narray([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific elements as well:\n\nfavorite_cats = array([\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"])\n\nfavorite_cats\n\nfavorite_cats[2] = \"Nyan Cat\"\n\nfavorite_cats\n\narray(['Grumpy', 'Garfield', 'Nyan Cat', 'Jean'], dtype='&lt;U8')\n\n\nIf you’re curious about any of these cats, see the footnotes4.\n\n2.3.2.1 Boolean masking\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that tells us which elements of a corresponding vector we want to keep.\n\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\n\n\n\nNote that for boolean masking to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\n\n\n\n\nExample\n\n\n\n\n# Define a character vector\nweekdays = array([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"])\nweekend = array([\"Sunday\", \"Saturday\"])\n\n# Create logical vectors manually\nrelax_days = array([True, False, False, False, False, False, True])\n\n# Create logical vectors automatically\nfrom numpy import isin     # get a special function for arrays\nrelax_days = isin(weekdays, weekend) \n\nrelax_days\n\n# Using logical vectors to index the character vector\nweekdays[relax_days] \n\n# Using ~ to reverse the True and False\nweekdays[~relax_days] \n\narray(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n      dtype='&lt;U9')\n\n\n\n\n\n\n2.3.2.2 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\n\n\n\n\n\nExample\n\n\n\n\narray([2, False, 3.1415, \"animal\"]) # all converted to strings\n\narray([2, False, 3.1415]) # converted to numerics\n\narray([2, False]) # converted to integers\n\narray([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - python decides what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\n\n\n\n\nWarning\n\n\n\nImplicit type conversions may seem convenient, but they are dangerous! Imagine that you created one of the arrays above, expecting it to be numeric, and only found out later that python had made it into strings.\n\n\n\n\n\n2.3.3 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\nOnce again, we need to use the numpy package to allow the matrix type to exist in python.\n\nfrom numpy import matrix\n\nmatrix([[1,2,3], [4,5,6]])\n\nmatrix([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how we give the matrix() function an argument that is a “list of lists”. That is, the first item in the list is [1,2,3] which is itself a list.\nYou can always think of lists as the most “neutral” data structure - if you don’t know what you want to use, it’s reasonably to start with the list, and then adjust from there, as we have with the array() and matrix() functions from numpy.\n\n\n\n2.3.3.1 Indexing in Matrices\npython uses [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix, we would use [2,0] to get to the third row and first column entry (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\n\n\n\n\nExample\n\n\n\n\nmy_mat = matrix([[1,2,3,4], [4,5,6,7], [7,8,9,10]])\n\nmy_mat\n\nmy_mat[2,0] = 500\n\nmy_mat\n\nmatrix([[  1,   2,   3,   4],\n        [  4,   5,   6,   7],\n        [500,   8,   9,  10]])\n\n\n\n\nWe will not use matrices often in this class, but there are many math operations that are very specific to matrices. If you continue on in your data science journey, you will probably eventually need to do matrix algebra in python.\n\n\n\n\n\n\nLearn More\n\n\n\nTutorial: Linear Algebra in python",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#libraries-and-open-source",
    "href": "01-basics.html#libraries-and-open-source",
    "title": "2  Programming Basics",
    "section": "2.4 Libraries and Open-Source",
    "text": "2.4 Libraries and Open-Source\n\n2.4.1 Open-source languages\nOne of the great things about python is that it is an open-source language. This means that it was and is developed by individuals in a community rather than a private company, and the core code of it is visible to everyone.\nThe major consequences are:\n\nIt is free for anyone to use, rather than behind a paywall. (SAS or Java are examples of languages produced by private companies that require paid licenses to use.)\nThe language grows quickly, and in many diverse ways, because anyone at all can write their own programs. (You will write functions in a couple weeks!)\nYou are not allowed to sell your code for profit. (You can still write private code to help your company with a task - but you may not charge money to others for the programs themselves.)\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nWe believe very strongly in the philosophy of open-source. However, it does have its downsides: mainly, that nearly all progress in the language is on a volunteer, community basis.\nAs a user of open source tools, we hope you will give back in whatever ways you can - sharing your work publicly, helping others learn, and encouraging private companies to fund open-source work.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThis very recent article, about the role of open source in today’s world of AI and social media, is quite interesting!\n\n\n\n\n2.4.2 Libraries\nWhen an open-source developer creates a new collection of functions and capabilities for python, and they want it to be easily usable and accessible to others, they bundle their code into a library. (You will sometimes here this called a package.)\nPackages that meet certain standards of quality and formatting are added to the Python Package Index, after which they can be esailly installed with pip (“package installer for python”).\nMost of the packages we will use in this class actually come pre-installed with Anaconda, so we won’t have to worry about this too much.\n\n\n\n\n\n\nCheck In\n\n\n\nOne package we need that is not pre-installed is plotnine.\nOpen up either a terminal or a Jupyter notebook in Anaconda. Then type\npip install plotnine\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nPython is notoriously frustrating for managine package installs. We will keep things simple in this class, but if you reach a point where you are struggling with libraries, know that you are not alone.\n\n\n\n\n\n\n\n\n\n2.4.3 Using library functions\nWhen you want to use functions from a library in your current code project, you have two options:\n\n2.4.3.1 1. Import the whole library\nIt is possible to load the full functionality of a library into your notebook project by adding an import statement in your very first code chunk.\nThe downside of this is that you then need to reference all those functions using the package name:\n\nimport numpy\n\nmy_nums = numpy.array([1,2,3,4,5])\nnumpy.sum(my_nums)\n\n15\n\n\nBecause this can get tedious, it’s common practice to give the package a “nickname” that is shorter:\n\nimport numpy as np\nmy_nums = np.array([1,2,3,4,5])\nnp.sum(my_nums)\n\n15\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThe reason for needing to use the library names is that nothing stops two developers from choosing the same name for their function. Python needs a way to know which library’s function you intended to use.\n\n\n\n\n2.4.3.2 2. Import only the functions you need.\nIf you only need a handful of functions from the library, and you want to avoid the extra typing of including the package name/nickname, you can pull those functions in directly:\n\nfrom numpy import array, sum\n\nmy_nums = array([1,2,3,4,5])\nsum(my_nums)\n\n15",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#data-frames",
    "href": "01-basics.html#data-frames",
    "title": "2  Programming Basics",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\nSince we are interested in using python specifically for data analysis, we will mention one more important Data Structure: a data frame.\nUnlike lists (which can contain anything at all) or matrices (which must store all the same type of data), data frames are restricted by column. That is, every data entry within a single column must be the same type; but two columns in the same data frame can have two different types.\nOne way to think of a data frame is as a list of vectors that all have the same length.\n\n2.5.0.1 Pandas\nAs with vectors and matrices, we need help from an external package to construct and work efficiently with data frames. This library is called pandas, and you will learn many of its functions next week.\nFor now, let’s just look at a pandas data frame:\n\nimport pandas as pd\n\ndat = pd.read_csv(\"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\")\n\ndat\n\ndat.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 9 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   rowid              344 non-null    int64  \n 1   species            344 non-null    object \n 2   island             344 non-null    object \n 3   bill_length_mm     342 non-null    float64\n 4   bill_depth_mm      342 non-null    float64\n 5   flipper_length_mm  342 non-null    float64\n 6   body_mass_g        342 non-null    float64\n 7   sex                333 non-null    object \n 8   year               344 non-null    int64  \ndtypes: float64(4), int64(2), object(3)\nmemory usage: 24.3+ KB\n\n\nNotice how the columns all have specific types: integers, floats, or strings (“object”). They also each have names. We can access the vector of information in one column like so…\n\ndat.body_mass_g\n\n0      3750.0\n1      3800.0\n2      3250.0\n3         NaN\n4      3450.0\n        ...  \n339    4000.0\n340    3400.0\n341    3775.0\n342    4100.0\n343    3775.0\nName: body_mass_g, Length: 344, dtype: float64\n\n\n… which then lets us do things to that column vector just as we might for standalone vectors:\n\n## using methods\ndat.body_mass_g.mean()\n\n## editing elements\ndat.body_mass_g[0] = 10000000\ndat.body_mass_g\n\n## boolean masking\nbig_penguins = dat.body_mass_g &gt; 6000\ndat.loc[big_penguins]\n\n\n\n\n\n\n\n\nrowid\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\n1\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n10000000.0\nmale\n2007\n\n\n169\n170\nGentoo\nBiscoe\n49.2\n15.2\n221.0\n6300.0\nmale\n2007\n\n\n185\n186\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nmale\n2007",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#summary",
    "href": "01-basics.html#summary",
    "title": "2  Programming Basics",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nWhew! How’s that for an overview?\n\n\n\n\n\nThe most important takeaways from this chapter are:\n\nObjects in python have types, and sometimes functions and operators behave differently based on the type.\nFunctions have both optional and required arguments. They take input and produce output.\nData can be stored in multiple different structures. The choice of structure depends on the dimensionality (1D or 2D) and the homogeneity (do all elements need to be the same type?)\nWe use indexing to access (and edit) individual elements or sections of data structures.\nWe use boolean masking to find only the elements of a vector, matrix, or data frame that meet a particular qualification.\npython is an open-source language. We will import many different libraries to add to our basic functionality.\n\n\n2.6.1 Practice Exercise\n\n\n\n\n\n\nPractice Activity\n\n\n\nUse your new knowledge of objects, types, and common coding errors to solve this puzzle",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#programming-with-genai",
    "href": "01-basics.html#programming-with-genai",
    "title": "2  Programming Basics",
    "section": "2.7 Programming with GenAI",
    "text": "2.7 Programming with GenAI\nWhile it is certainly important to think about responsible and ethical use of GenAI, that doesn’t mean we should avoid using it altogehter. In this class we will also focus on effective and correct ways to use it to support our data tasks.\nDo you feel safe in a car with some self-driving abilities? Despite the self-driving abilities, your answer probably still depends on how safe you feel with the driver. You would hope that your driver has been taught to drive thoroughly, including knowing how to do the things that the self-driving can do on its own.\nIt is similar with learning programming and machine learning. If we can use GenAI to do our work faster and with less frustration that is a good thing. But we can only really trust an analysis or program produced by GenAI if we are good drivers - if we know enough about programming without AI that we are able to review, edit, and test the output of the AI tool.\n\n\n\n\n\n\nJust our opinion...\n\n\n\nOur advice is that you challenge yourself to use no GenAI help on the Practice Exercises in this class. This will ensure that you get practice with new skills, and with finding other resources online, before you dive in to the lab assignments. Think of this like driving around in a parking lot before you hit the freeway!\nOn the other hand, we encourage you to treat the Lab Assignments like real-world data analysis projects. This might mean using a higher level of GenAI support - but of course, you will still need to take careful steps to ensure that the final results are correct and reliable, as we discuss below.\n\n\nIn this class, we will follow the WEIRD rule for checking that AI-produced work is acceptable. Anything produced or co-produced by GenAI needs a human contributor to ensure that it is:\n\nWell-Specified\nEditable\nInterpretable\nReproducible\nDependable\n\n\n2.7.1 Well-Specified\nA self-driving car can get you to your location, but it can’t help you decide what location you are driving to.\nThe blessing and the curse of computer programming is that computers can only do what they are told. The same is true for prompting GenAI content: an LLM will only respond to the exact prompt that it is given.\nThe very first step of every data analysis must come from a human being, who will define the problem and construct a problem statement, which may be used as an AI prompt.\nClick this link to see an example of a conversation with Chat-GPT 4o, where we give three different prompts for a particular data analysis question. Notice how the specificity of the prompt is important to get the results that the user is interested in.\n\n2.7.1.1 Brainstorming\nIf you don’t begin a data exploration with a specific analysis already in mind, GenAI can be excellent for brainstorming possible approaches.\nClick here again to continue the previous chat conversation, where we ask ChatGPT 4o to suggest some additional analyses. The AI tool is able to suggest some supporting tests for our original t-test, as well of some extensions of the original research question.\nUltimately, however, it is the responsibility of the human to sift through the AI’s suggestions - possibly researching further any suggested analyses they are not familiar with - and determine one final, carefully specified plan of action.\n\n\n\n2.7.2 Editable\nEvery few years, a group of programmers hosts the Obfusticated C Code Contest, a competition to see who can write the most unreadable, illogical, complicated program to do a straightforward task. Winners include code like the below, which, believe it or not, is a program to play chess:\nB,i,y,u,b,I[411],*G=I,x=10,z=15,M=1e4;X(w,c,h,e,S,s){int t,o,L,E,d,O=e,N=-M*M,K\n=78-h&lt;&lt;x,p,*g,n,*m,A,q,r,C,J,a=y?-x:x;y^=8;G++;d=w||s&&s&gt;=h&&v 0,0)&gt;M;do{_ o=I[\np=O]){q=o&z^y _ q&lt;7){A=q--&2?8:4;C=o-9&z?q[\"& .$  \"]:42;do{r=I[p+=C[l]-64]_!w|p\n==w){g=q|p+a-S?0:I+S _!r&(q|A&lt;3||g)||(r+1&z^y)&gt;9&&q|A&gt;2){_ m=!(r-2&7))P G[1]=O,\nK;J=n=o&z;E=I[p-a]&z;t=q|E-7?n:(n+=2,6^y);Z n&lt;=t){L=r?l[r&7]*9-189-h-q:0 _ s)L\n+=(1-q?l[p/x+5]-l[O/x+5]+l[p%x+6]*-~!q-l[O%x+6]+o/16*8:!!m*9)+(q?0:!(I[p-1]^n)+\n!(I[p+1]^n)+l[n&7]*9-386+!!g*99+(A&lt;2))+!(E^y^9)_ s&gt;h||1&lt;s&s==h&&L&gt;z|d){p[I]=n,O\n[I]=m?*g=*m,*m=0:g?*g=0:0;L-=X(s&gt;h|d?0:p,L-N,h+1,G[1],J=q|A&gt;1?0:p,s)_!(h||s-1|B\n-O|i-n|p-b|L&lt;-M))P y^=8,u=J;J=q-1|A&lt;7||m||!s|d|r|o&lt;z||v 0,0)&gt;M;O[I]=o;p[I]=r;m?\n*m=*g,*g=0:g?*g=9^y:0;}_ L&gt;N){*G=O _ s&gt;1){_ h&&c-L&lt;0)P L _!h)i=n,B=O,b=p;}N=L;}\nn+=J||(g=I+p,m=p&lt;O?g-3:g+2,*m&lt;z|m[O-p]||I[p+=p-O]);}}}}Z!r&q&gt;2||(p=O,q|A&gt;2|o&gt;z&\n!r&&++C*--A));}}}Z++O&gt;98?O=20:e-O);P N+M*M&&N&gt;-K+1924|d?N:0;}main(){Z++B&lt;121)*G\n++=B/x%x&lt;2|B%x&lt;2?7:B/x&4?0:*l++&31;Z B=19){Z B++&lt;99)putchar(B%x?l[B[I]|16]:x)_\nx-(B=F)){i=I[B+=(x-F)*x]&z;b=F;b+=(x-F)*x;Z x-(*G=F))i=*G^8^y;}else v u,5);v u,\n1);}}\nWhile GenAI doesn’t write code like the above, it can sometimes create programs that aren’t quite designed with human readability in mind. This makes it difficult for a human reviewer to edit the program when needed.\n\nIf your GenAI comes back with a program that you find difficult to understand - convoluted design, unclearly named objects, too many nested functions or loops, and so on - you will need to refactor the code so that it is easier to tweak and update.\n\n\n2.7.3 Interpretable\nEven if you use GenAI to produce code and/or analysis, ultimately the ownership of the project lies with you. It is extremely important that you understand, and are able to explain, the steps that the AI took; whether or not you directly performed them.\n\n\n\n\n\n\nNote\n\n\n\nIn this course, the interpretable element is also how we define academic integrity.\nWe expect that any AI-produced content you include in your work to be something you can completely and correctly explain without the AI’s help.\nIf we suspect that AI is being overused in inappropriate ways - such as homework questions being copy-pasted into the prompt and the AI response being copy-pasted back with no oversight - we will address this by sitting down with you and asking you to explain the content step by step.\nIt is your responsibility to take the time and ensure you truly understand each task that you “outsource” to a GenAI tool.\n\n\n\n\n2.7.4 Reproducible\nWe call a data analysis reproducible if it is structured and documented in such a way that it is easily repeatable with new data or by new researchers.\nA current weakness of GenAI tools is that they often produce code that is too specifically tailored to the data given; code that cannot be generalized to the full data problem.\nConsider, for example, the following suggestion from ChatGPT 4o, in response to the prompt “Write me a python program to find all the first names used in the lyrics of a given song.”\n\nimport re\n\n# A list of common first names directly embedded in the script\nFIRST_NAMES = {\n    'John', 'Paul', 'Ringo', 'Mary', 'Lucy', 'David', 'Sarah', 'James',\n    'Michael', 'Jennifer', 'Linda', 'Elizabeth', 'George', 'Emma',\n    'Liam', 'Olivia', 'Noah', 'Ava', 'Sophia', 'Isabella'\n    # Add more first names as needed\n}\n\ndef find_first_names_in_lyrics(lyrics, first_names):\n    \"\"\"Find all first names in the given song lyrics.\"\"\"\n    # Split lyrics into words and remove punctuation using regex\n    words = re.findall(r'\\b\\w+\\b', lyrics.lower())\n    \n    # Find intersection of words in lyrics with the first names\n    found_names = {word.capitalize() for word in words if word in first_names}\n    \n    return found_names\n\nHere we see that the LLM suggests an approach of manually making a list of common first names. As you can imagine, this is unlikely to be the design that a data scientist would use. Not only does it require manually making a list of common names, or finding a resource like that online, this approach would miss uncommon first names. We would be more likely to use strategies like looking for capitalized words, sentence syntax, context, and so on.\nThe LLM’s approach would not be reproducible outside of the example songs it seems to have used to select 'John', 'Paul', 'Ringo', 'Mary', 'Lucy' as common names. (What songs could this be, do you think?)\nWhen you generate code from an LLM, it is your responsibility to look for “hard-coded” or otherwise unreproducible elements of the program, and to instead implement a more generalizable approach.\n\n\n2.7.5 Dependable\nThe simplest and most important question to ask about your AI-produced code or analysis is: is it correct?\nIf we have met the previous four guidelines, we know our output is: - Answering the right question - Possible to change if needed - Following a process we understand - Generalized, not hard-coded\nBut… does it actually achieve the desired task in a proper way?\nIt is not enough for you to be convinced that your procedure works. You need to provide solid reassurances of its dependability.\n\n2.7.5.1 Unit testing\nIn programming, there is one way to do this: writing small unit tests that check whether your code outputs the values you wanted it to.\nUnit tests can be a very structured and formal, like the code checks that automatically run when a developer updates a particular library, to make sure the updates didn’t cause a breaking change in the software. But they can also be quick and informal, like putting a few different inputs into your function to make sure it behaves how you expect.\n\n\n\n\n\n\nImportant\n\n\n\nWe cannot emphasize this enough: Unit tests are the best and most important way to prove to yourself, to us, and to your clients that the code you created (with or without AI) works the way you claim it does.\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nHere is a conversation with ChatGPT 4o, in which it is asked to create a program to find all prime numbers in a certain range, and a program to find all perfect numbers in a certain range.\nYou may not understand every step of the code, nor the math procedure being used. That’s okay for this exercise.\nCopy the code into a Colab Notebook. Then write a few unit tests that check if it works. Make sure to include some unexpected or unusual inputs, to try to “challenge” the program to break!\n\n\n\n\n\n2.7.6 Even WEIRD-ER: Ethics and References\nThe WEIRD guidelines are focused on making sure the actual content and results of AI work is trustworthy.\nDon’t forget, though, to be even WEIRD-ER: we also expect you to consider the Ethical issues of your AI use, and to thoroughly Reference any AI contribution to your work.\nIn every Lab Assignment, we expect an appendix with an Ethics Statement and References, both of which might need to reference your AI use.\nAn example ethics statement might look something like:\n\nThis analysis includes a comparison of male and female sex penguins. It therefore omits penguins with unknown sex or with unidentifiable biological sex. This analysis also makes use of Generative AI to suggest comparisons across penguin sex, and these tools may overlook exceptions to the sex binary.\n\nAn example reference statement including AI might look something like:\n\nChat-GPT 4o was used to suggest several analyses, including the Two-Way ANOVA test for body mass across sex and habitat. It was also used to generate first-draft code to perform this ANOVA test and create the corresponding boxplot.\n\n\n\n2.7.7 Try it out\n\n\n\n\n\n\nPractice Activity\n\n\n\nComplete this activity with a partner.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "01-basics.html#footnotes",
    "href": "01-basics.html#footnotes",
    "title": "2  Programming Basics",
    "section": "",
    "text": "This means that doubles take up more memory but can store more decimal places. You don’t need to worry about this in anything we will do.↩︎\nIn some ways, this is like the difference between an automatic and a manual transmission - you have fewer things to worry about, but you also don’t know what’s going on under the hood nearly as well↩︎\nThroughout this section (and other sections), lego pictures are rendered using https://www.mecabricks.com/en/workshop. It’s a pretty nice tool for building stuff online!↩︎\nGrumpy cat, Garfield, Nyan cat. Jorts and Jean: The initial post and the update (both are worth a read because the story is hilarious). The cats also have a Twitter account where they promote workers rights.↩︎",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Programming Basics</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html",
    "href": "02-plotnine.html",
    "title": "3  Data Visualization in Python",
    "section": "",
    "text": "3.1 Introduction\nThis document demonstrates the use of the plotnine library in Python to visualize data via the grammar of graphics framework.\nThe functions in plotnine originate from the ggplot2 R package, which is the R implementation of the grammar of graphics.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html#grammar-of-graphics",
    "href": "02-plotnine.html#grammar-of-graphics",
    "title": "3  Data Visualization in Python",
    "section": "3.2 Grammar of Graphics",
    "text": "3.2 Grammar of Graphics\nThe grammar of graphics is a framework for creating data visualizations.\nA visualization consists of:\n\nThe aesthetic: Which variables are dictating which plot elements.\nThe geometry: What shape of plot your are making.\n\nFor example, the plot below displays some of the data from the Palmer Penguins data set.\nFirst, though, we need to load the Palmer Penguins dataset.\n\n\n\n\n\n\nNote\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Same for any other libraries you haven’t installed.\n\n\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\nfrom plotnine import ggplot, geom_point, aes, geom_boxplot\n\n\npenguins = load_penguins()\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nThe aesthetic is species on the x-axis, bill_length_mm on the y-axis, colored by species.\nThe geometry is a boxplot.\n\n\n\n\n\n\nCheck In\n\n\n\nTake a look at the first page of the optional reading for plotnine. In groups of 3-4, discuss the differences between how they use plotnine and the way we used it in the code chunk above.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html#plotnine-i.e.-ggplot",
    "href": "02-plotnine.html#plotnine-i.e.-ggplot",
    "title": "3  Data Visualization in Python",
    "section": "3.3 plotnine (i.e. ggplot)",
    "text": "3.3 plotnine (i.e. ggplot)\nThe plotnine library implements the grammar of graphics in Python.\nCode for the previous example:\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n3.3.1 The aesthetic\n\n(ggplot(penguins,\n1aes(\n2  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"species\"))\n+ geom_boxplot()\n)\n\n\n1\n\nThe aes() function is the place to specify aesthetics.\n\n2\n\nx, y, and fill are three possible aesthetics that can be specified, that map variables in our data set to plot elements.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 The geometry\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"species\"))\n1+ geom_boxplot()\n)\n\n\n1\n\nA variety of geom_* functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Other optional elements:\n\nThe scales of the x- and y-axes.\nThe color of elements that are not mapped to aesthetics.\nThe theme of the plot\n\n…and many more!\n\n\n3.3.4 Scales\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nversus\n\nfrom plotnine import scale_y_reverse\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ scale_y_reverse()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Non-aesthetic colors\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nversus\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot(fill = \"cornflowerblue\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat will this show?\n\n\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"cornflowerblue\"))\n+ geom_boxplot()\n)\n\n\n\n3.3.6 Themes\n\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nversus\n\nfrom plotnine import theme_classic\n(ggplot(penguins, aes(x = \"species\", y = \"bill_length_mm\", fill = \"species\"))\n+ geom_boxplot()\n+ theme_classic()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nWhat are the differences between the two plots above? What did the theme change?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat are the aesthetics, geometry, scales, and other options in the cartoon plot below?\n\n\n\nAn xkcd comic of time spent going up the down escalator\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\n\nScales: https://ggplot2-book.org/scale-position.html\nThemes: https://ggplot2-book.org/polishing.html",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html#geometries-the-big-five",
    "href": "02-plotnine.html#geometries-the-big-five",
    "title": "3  Data Visualization in Python",
    "section": "3.4 Geometries: The “Big Five”",
    "text": "3.4 Geometries: The “Big Five”\n\n3.4.1 1. Bar Plots\nMost often used for showing counts of a categorical variable:\n\nfrom plotnine import geom_bar\n(ggplot(penguins,\naes(\n  x = \"species\"\n))\n+ geom_bar()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n… or relationships between two categorical variables:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"sex\"\n))\n+ geom_bar()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nWould we rather see percents?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"fill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nOr side-by-side?\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  fill = \"island\"\n))\n+ geom_bar(position = \"dodge\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nCompare and contrast the plots above? What information is lost or gained between each of them?\n\n\n\n\n3.4.2 2. Boxplots\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nSide-by-side using a categorical variable:\n\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  fill = \"sex\"\n))\n+ geom_boxplot()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 3. Histograms\n\nfrom plotnine import geom_histogram\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 100)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\"\n))\n+ geom_histogram(bins = 10)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 3.5 Densities\nSuppose you want to compare histograms by category:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_histogram()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nCleaner: smoothed histogram, or density:\n\nfrom plotnine import geom_density\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nEven cleaner: The alpha option:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  fill = \"species\"\n))\n+ geom_density(alpha = 0.5)\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.4.5 4. Scatterplots\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\"\n))\n+ geom_point()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\nColors for extra information:\n\n(ggplot(penguins,\naes(\n  x = \"bill_length_mm\",\n  y = \"bill_depth_mm\",\n  color = \"species\"\n))\n+ geom_point()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n3.4.6 5. Line Plots\n\nfrom plotnine import geom_line\npenguins2 = penguins.groupby([\"species\", \"sex\"])[[\"bill_length_mm\"]].mean().reset_index()\n\n(ggplot(penguins2,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\",\n  color = \"sex\",\n  group = \"sex\"\n))\n+ geom_point()\n+ geom_line()\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\n\nggplot2 cheatsheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nplotnine: https://plotnine.readthedocs.io/en/stable/",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html#multiple-plots",
    "href": "02-plotnine.html#multiple-plots",
    "title": "3  Data Visualization in Python",
    "section": "3.5 Multiple Plots",
    "text": "3.5 Multiple Plots\n\n3.5.1 Facet Wrapping\n\nfrom plotnine import facet_wrap\n(ggplot(penguins,\naes(\n  x = \"species\",\n  y = \"bill_length_mm\"\n))\n+ geom_boxplot()\n+ facet_wrap(\"sex\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub. Then share this repository link in the quiz question.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "02-plotnine.html#visualization-and-genai",
    "href": "02-plotnine.html#visualization-and-genai",
    "title": "3  Data Visualization in Python",
    "section": "3.6 Visualization and GenAI",
    "text": "3.6 Visualization and GenAI\nIn our experience, generative AI can help with the data visualization process in two major ways:\n\n3.6.1 1. Brainstorming possible visualizations for a particular research question.\nSometimes, it can be hard to imagine what a plot will look like or which geometries to use - you sink time into writing out your code, only to be disappointed when the resulting image is not as compelling as you hoped.\nWith careful prompting, many genAI tools can suggest plot types and then “preview” these plot ideas for you. There are some limitations, however:\n\nWhen asking for this service, make sure to ask for the code output specifically. In one attempt to demonstrate this task, I carelessly used the phrase “sketch a plot”, and GPT-4o took the “sketch” command very seriously, as you can see below!\n\n\n\nThe GenAI does not have access to your specific dataset. That means the tool cannot fully preview how your plots might look on your data. What it can do, though, is show comparable examples on another dataset.\n\nThe goal here is not to fully produce your final visualization. The goal is to get a general sense of what geometry options might fit your research question, and how each of those would look.\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen a GenAI tool that includes a python environment, such as Chat GPT-4o or Gemini. Try a prompt of the following form:\n\nI would like to make a plot showing the different bill dimensions of different species of penguins. Can you use a similar dataset to show me a few examples of python code and corresponding output to make plots like this?\n\nThen, imagine another dataset (maybe one that doesn’t exist), and ask the genAI tool to show you plot examples for a particular research question. What happens?\n\n\n\n\n3.6.2 2. Building code layer by layer.\n\n3.6.2.1 Initial plot\nIf you find it psychologically easier to edit code than to start from scratch, genAI can be very adept at producing basic visualization code for you to build on. This chat shows a very quick example.\n\n\n3.6.2.2 Specific syntax to tweak your visual\nOnce you have your basic plot code, the genAI tool becomes an excellent reference/documentation for how to add layers and make tweaks. For example, suppose in the above example we wanted to see the bill lengths on a logarithmic scale. In this chat, we see how easily Chat GPT-4o is able to add the ggplot layer of + scale_y_log10()\n\n\n3.6.2.3 Principles\nSince this use of AI involves asking it to write actual code for you, remember the WEIRDER principles:\nWell-specified: The more specifically we can describe our plot, the better resulting code you will get. Make sure to mention which plotting library you want to use, what geometry you are using, and what your variable mappings are.\nEditable: Don’t try to get the AI tool to create your final perfect polished visualization from the first prompt; this can lead to overly complicated code that is hard to tweak. Instead, add complexity bit by bit, checking at each step for ways to improve or clarify the AI-generated code.\nInterpretable: The AI tool will sometimes leap to conclusions about the plot, making unprompted changes to the titles, the scales, or the theme. Make sure you review each layer of the ggplot process, and ensure that it is indeed what you intended.\nReproducible: Sometimes, when you ask for a particular small visual change, the AI will achieve this task manually. For example, if you ask for particular labels on the x-axis, it may choose to remove all labels and put numbers in “by hand”, rather than generally changing the scale. (Look for an example of this with the facet titles in the activity at the end of this section!)\nEven if the AI-generated code achieves the visual you hoped for, make sure to review the code for instances where you need to replace sloppy solutions with cleaner ones.\nDependable: The good news is, unit testing in visualization is easy: simply run the code and see if the output looks how you hoped!\nEthical: Just because an LLM suggests a visual doesn’t mean it is a responsible one. You, as the human creator, must review your visualizations to ensure they are not conveying any harmful information or impressions.\nReferences: If you use AI-generated code in your visualization, you absolutely must state this up front in your work, even if you heavily edit the initial code.\n\n\n\n3.6.3 Try it out\n\n\n\n\n\n\nPractice Activity\n\n\n\nTry to construct code to replicate the plot below by using only AI prompts. You may ask specifically for plotnine code, but beyond that, you may not mention any specific functions or argument names.\nInstead, use statements that reference the Grammar of Graphics, like “The x-axis is mapped to bill length” or “The color scale for the species is Red, Yellow, Orange.”\nDo your best to achieve this without un-folding the code to see how I constructed the plot; but if you are stuck you may look at it.\n\n\n\n\nCode\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins()\n\nfrom plotnine import *\n\n(\n    ggplot(penguins.dropna(), aes(x=\"species\", y=\"body_mass_g\", fill=\"species\"))\n    + geom_violin(alpha=0.5)\n    + geom_jitter()\n    + facet_wrap(\"sex\")\n    + scale_fill_discrete(guide=None)\n    + labs(x=\"\", y=\"\", title=\"Body mass (g) of different penguins species by sex.\")\n    + theme_bw()\n)\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nClick here to see my conversation with Chat GPT 4o. Notice how my results improved when I used more specific Grammar of Graphics language.\nCompare the resulting code, and its output, to my hand-written code that made the original plot. What is different? What ways did GenAI make things easier, and in what ways did it overcomplicate a step?",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization in Python</span>"
    ]
  },
  {
    "objectID": "03-basic_data_operations.html",
    "href": "03-basic_data_operations.html",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "",
    "text": "4.1 Introduction\nThis document demonstrates the use of the pandas library in Python to do basic data wrangling and summarization.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular Data and Basic Data Operations</span>"
    ]
  },
  {
    "objectID": "03-basic_data_operations.html#introduction",
    "href": "03-basic_data_operations.html#introduction",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session, for packages that don’t come pre-installed).",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular Data and Basic Data Operations</span>"
    ]
  },
  {
    "objectID": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "href": "03-basic_data_operations.html#reading-tabular-data-into-python",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.2 Reading Tabular Data into Python",
    "text": "4.2 Reading Tabular Data into Python\nWe’re going to be exploring pandas in the context of the famous Titanic dataset. We’ll work with a subset of this dataset, but more information about it all can be found here.\nWe start by loading the numpy and pandas libraries. Most of our data wrangling work will happen with functions from the pandas library, but the numpy library will be useful for performing certain mathematical operations should we choose to transform any of our data.\n\nimport numpy as np\nimport pandas as pd\n\n\ndata_dir = \"https://dlsun.github.io/pods/data/\"\ndf_titanic = pd.read_csv(data_dir + \"titanic.csv\")\n\n\n\n\n\n\n\nExample\n\n\n\nWe’ve already seen read_csv() used many times for importing CSV files into Python, but it bears repeating.\n\n\nData files of many different types and shapes can be read into Python with similar functions, but we will focus on tabular data.\n\n4.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith 2-3 people around you, navigate to the GapMinder Data site and download a single CSV file of your choice. Open it up in Excel or your application of choice. Is this dataset tidy? If not, then what would have to change to make it tidy?\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThe term “tidy data” was first popularized in this paper by R developer Hadley Wickham.\n\n\nYou may have noticed that plotnine (ggplot) is basically built to take tidy data. Variables are specified in the aesthetics function to map them (i.e. columns) in our dataset to plot elements. This type of behavior is EXTREMELY common among functions that work with data in all languages, and so the importance of getting our data into a tidy format cannot be overstated.\nIn Python, there are at least two quick ways to view a dataset we’ve read in:\n\ndf_titanic\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.11\n0\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.05\n1\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.13\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2202\nWynn, Mr. Walter\nmale\n41.0\ndeck crew\nB\nEngland\nNaN\nNaN\n1\n\n\n2203\nYearsley, Mr. Harry\nmale\n40.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2204\nYoung, Mr. Francis James\nmale\n32.0\nengineering crew\nS\nEngland\nNaN\nNaN\n0\n\n\n2205\nZanetti, Sig. Minio\nmale\n20.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\n0\n\n\n2206\nZarracchi, Sig. L.\nmale\n26.0\nrestaurant staff\nS\nEngland\nNaN\nNaN\n0\n\n\n\n\n2207 rows × 9 columns\n\n\n\n\ndf_titanic.head()\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.11\n0\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0\n\n\n2\nAbbott, Mr. Rossmore Edward\nmale\n16.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.05\n1\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.13\n1\n\n\n\n\n\n\n\nThe latter (.head()) is usually preferred in case the dataset is large.\n\n\n\n\n\n\nCheck In\n\n\n\nDoes the titanic dataset appear to be in tidy format?",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular Data and Basic Data Operations</span>"
    ]
  },
  {
    "objectID": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "href": "03-basic_data_operations.html#the-big-five-verbs-of-data-wrangling",
    "title": "4  Tabular Data and Basic Data Operations",
    "section": "4.3 The “Big Five” Verbs of Data Wrangling",
    "text": "4.3 The “Big Five” Verbs of Data Wrangling\nData wrangling can involve a lot of different steps and operations to get data into a tidy format and ready for analysis and visualization. The vast majority of these fall under the umbrella one the following five operations:\n\nSelect columns/variables of interest\nFilter rows/observations of interest\nArrange the rows of a dataset by column(s) of interest (i.e. order or sort)\nMutate the columns of a dataset (i.e. create or transform variables)\nSummarize the rows of a dataset for column(s) of interest\n\n\n4.3.1 Select Columns/Variables\nSuppose we want to select the age variable from the titanic DataFrame. There are three ways to do this.\n\nUse .loc, specifying both the rows and columns. (The colon : is Python shorthand for “all”.)\n\n\ndf_titanic.loc[:, \"age\"]\n\n\nAccess the column as you would a key in a dict.\n\n\ndf_titanic[\"age\"]\n\n\nAccess the column as an attribute of the DataFrame.\n\n\ndf_titanic.age\n\nMethod 3 (attribute access) is the most concise. However, it does not work if the variable name contains spaces or special characters, begins with a number, or matches an existing attribute of the DataFrame. So, methods 1 and 2 are usually safer and preferred.\nTo select multiple columns, you would pass in a list of variable names, instead of a single variable name. For example, to select both age and fare, either of the two methods below would work (and produce the same result):\n\n# Method 1\ndf_titanic.loc[:, [\"age\", \"fare\"]].head()\n\n# Method 2\ndf_titanic[[\"age\", \"fare\"]].head()\n\n\n\n4.3.2 Filter Rows/Observations\n\n4.3.2.1 Selecting Rows/Observations by Location\nBefore we see how to filter (i.e. subset) the rows of dataset based on some condition, let’s see how to select rows by explicitly identifying them.\nWe can select a row by its position using the .iloc attribute. Keeping in mind that the first row is actually row 0, the fourth row could be extracted as:\n\ndf_titanic.iloc[3]\n\nname        Abbott, Mrs. Rhoda Mary 'Rosa'\ngender                              female\nage                                   39.0\nclass                                  3rd\nembarked                                 S\ncountry                            England\nticketno                            2673.0\nfare                                 20.05\nsurvived                                 1\nName: 3, dtype: object\n\n\nNotice that a single row from a DataFrame is no longer a DataFrame but a different data structure, called a Series.\nWe can also select multiple rows by passing a list of positions to .iloc.\n\ndf_titanic.iloc[[1, 3]]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n1\nAbbott, Mr. Eugene Joseph\nmale\n13.0\n3rd\nS\nUnited States\n2673.0\n20.05\n0\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.05\n1\n\n\n\n\n\n\n\nNotice that when we select multiple rows, we get a DataFrame back.\nSo a Series is used to store a single observation (across multiple variables), while a DataFrame is used to store multiple observations (across multiple variables).\nIf selecting consecutive rows, we can use Python’s slice notation. For example, the code below selects all rows from the fourth row, up to (but not including) the tenth row.\n\ndf_titanic.iloc[3:9]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.0500\n1\n\n\n4\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n3rd\nS\nNorway\n348125.0\n7.1300\n1\n\n\n5\nAbelseth, Mr. Olaus Jørgensen\nmale\n25.0\n3rd\nS\nUnited States\n348122.0\n7.1300\n1\n\n\n6\nAbelson, Mr. Samuel\nmale\n30.0\n2nd\nC\nFrance\n3381.0\n24.0000\n0\n\n\n7\nAbelson, Mrs. Hannah\nfemale\n28.0\n2nd\nC\nFrance\n3381.0\n24.0000\n1\n\n\n8\nAbī-Al-Munà, Mr. Nāsīf Qāsim\nmale\n27.0\n3rd\nC\nLebanon\n2699.0\n18.1509\n1\n\n\n\n\n\n\n\n\n\n4.3.2.2 Selecting Rows/Observations by Condition\nWe’ll often want to filter or subset the rows of a dataset based on some condition. To do this we’ll take advantage of vectorization and boolean masking.\nRecall that we can compare the values of a variable/column to a particular value in the following way, and observe the result.\n\ndf_titanic[\"age\"] &gt; 30\n\n0        True\n1       False\n2       False\n3        True\n4       False\n        ...  \n2202     True\n2203     True\n2204     True\n2205    False\n2206    False\nName: age, Length: 2207, dtype: bool\n\n\nWe can use these True and False values to filter/subset the dataset! The following subsets the titanic dataset down to only those individuals (rows) with ages over 30.\n\ndf_titanic[df_titanic[\"age\"] &gt; 30]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n0\nAbbing, Mr. Anthony\nmale\n42.0\n3rd\nS\nUnited States\n5547.0\n7.1100\n0\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.0500\n1\n\n\n12\nAhlin, Mrs. Johanna Persdotter\nfemale\n40.0\n3rd\nS\nSweden\n7546.0\n9.0906\n0\n\n\n15\nAldworth, Mr. Augustus Henry\nmale\n35.0\n2nd\nS\nEngland\n248744.0\n13.0000\n0\n\n\n21\nAllen, Mr. William Henry\nmale\n39.0\n3rd\nS\nEngland\n373450.0\n8.0100\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2197\nWorthman, Mr. William Henry\nmale\n37.0\nengineering crew\nS\nEngland\nNaN\nNaN\n0\n\n\n2200\nWright, Mr. William\nmale\n40.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2202\nWynn, Mr. Walter\nmale\n41.0\ndeck crew\nB\nEngland\nNaN\nNaN\n1\n\n\n2203\nYearsley, Mr. Harry\nmale\n40.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2204\nYoung, Mr. Francis James\nmale\n32.0\nengineering crew\nS\nEngland\nNaN\nNaN\n0\n\n\n\n\n984 rows × 9 columns\n\n\n\nWe can combine multiple conditions using & (and) and | (or). The following subsets the titanic dataset down to females over 30 years of age.\n\ndf_titanic[(df_titanic[\"age\"] &gt; 30) & (df_titanic[\"gender\"] == \"female\")]\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n3\nAbbott, Mrs. Rhoda Mary 'Rosa'\nfemale\n39.0\n3rd\nS\nEngland\n2673.0\n20.0500\n1\n\n\n12\nAhlin, Mrs. Johanna Persdotter\nfemale\n40.0\n3rd\nS\nSweden\n7546.0\n9.0906\n0\n\n\n35\nAndersson, Miss. Ida Augusta Margareta\nfemale\n38.0\n3rd\nS\nSweden\n347091.0\n7.1506\n0\n\n\n40\nAndersson, Mrs. Alfrida Konstantia Brogren\nfemale\n39.0\n3rd\nS\nSweden\n347082.0\n31.0506\n0\n\n\n44\nAndrews, Miss. Kornelia Theodosia\nfemale\n62.0\n1st\nC\nUnited States\n13502.0\n77.1902\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1997\nRobinson, Mrs. Annie\nfemale\n41.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2059\nSmith, Miss. Katherine Elizabeth\nfemale\n45.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2076\nStap, Miss. Sarah Agnes\nfemale\n47.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n1\n\n\n2143\nWallis, Mrs. Catherine Jane\nfemale\n36.0\nvictualling crew\nS\nEngland\nNaN\nNaN\n0\n\n\n2145\nWalsh, Miss. Catherine\nfemale\n32.0\nvictualling crew\nS\nIreland\nNaN\nNaN\n0\n\n\n\n\n206 rows × 9 columns\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith the 2-3 people around you, how would you find the just the names of the males under 20 years of age who survived (in the titanic dataset) with a single line of code?\n\n\n\n\n\n4.3.3 Arrange Rows\nAs part of exploratory data analysis and some reporting efforts, we will want to sort a dataset or set of results by one or more variables of interest.\nWe can do this with .sort_values in either ascending or descending order.\nThe following sorts the titanic dataset by age in decreasing order.\n\ndf_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\n\n\n\n\n1176\nSvensson, Mr. Johan\nmale\n74.000000\n3rd\nS\nSweden\n347060.0\n7.1506\n0\n\n\n820\nMitchell, Mr. Henry Michael\nmale\n72.000000\n2nd\nS\nEngland\n24580.0\n10.1000\n0\n\n\n53\nArtagaveytia, Mr. Ramon\nmale\n71.000000\n1st\nC\nArgentina\n17609.0\n49.1001\n0\n\n\n456\nGoldschmidt, Mr. George B.\nmale\n71.000000\n1st\nC\nUnited States\n17754.0\n34.1301\n0\n\n\n282\nCrosby, Captain. Edward Gifford\nmale\n70.000000\n1st\nS\nUnited States\n5735.0\n71.0000\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1182\nTannūs, Master. As'ad\nmale\n0.416667\n3rd\nC\nLebanon\n2625.0\n8.1004\n1\n\n\n296\nDanbom, Master. Gilbert Sigvard Emanuel\nmale\n0.333333\n3rd\nS\nSweden\n347080.0\n14.0800\n0\n\n\n316\nDean, Miss. Elizabeth Gladys 'Millvina'\nfemale\n0.166667\n3rd\nS\nEngland\n2315.0\n20.1106\n1\n\n\n439\nGheorgheff, Mr. Stanio\nmale\nNaN\n3rd\nC\nBulgaria\n349254.0\n7.1711\n0\n\n\n677\nKraeff, Mr. Theodor\nmale\nNaN\n3rd\nC\nBulgaria\n349253.0\n7.1711\n0\n\n\n\n\n2207 rows × 9 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that in these last few sections, we have not made any permanent changes to the df_titanic object. We have only asked python do some selecting/filtering/sorting and then to print out the results, not save them.\nIf we wanted df_titanic to become permanently sorted by age, we would re-assign the object:\n\ndf_titanic = df_titanic.sort_values(by = [\"age\"], ascending=False)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou will sometimes see object reassignment happen in a different way, using an inplace = True argument, like this:\n\ndf_titanic.sort_values(by = [\"age\"], ascending=False, inplace=True)\n\nWe strongly recommend against this approach, for two reason:\n\nWhen an object is “overwritten” via reassignment, that’s a major decision; you lose the old version of the object. It should be made deliberately and obviously. The inplace argument is easy to miss when copying/editing code, so it can lead to accidental overwriting that is hard to keep track of.\nNot all functions of DataFrames have an inplace option. It can be frustrating to get into the habit of using it, only to find out the hard way that it’s not available half the time!\n\n\n\n\n\n4.3.4 Mutate Column(s)\nThe variables available to us in our original dataset contain all of the information we have access to, but the best insights may instead come from transformations of those variables.\n\n4.3.4.1 Transforming Quantitative Variables\nOne of the simplest reasons to want to transform a quantitative variable is to change the measurement units.\nHere we change the age of passengers from a value in years to a value in decades.\n\ndf_titanic[\"age\"] = df_titanic[\"age\"] / 10\n\nIf we have a quantitative variable that is particularly skewed, then it might be a good idea to transform the values of that variable…like taking the log of the values.\n\n\n\n\n\n\nNote\n\n\n\nThis was a strategy you saw employed with the GapMinder data!\n\n\nBelow is an example of taking the log of the fare variable. Notice that we’re making use of the numpy here to take the log.\n\ndf_titanic[\"fare\"] = np.log(df_titanic[\"fare\"])\n\nRemember that we can take advantage of vectorization here too. The following operation wouldn’t really make physical sense, but it’s an example of creating a new variable out of existing variables.\n\ndf_titanic[\"nonsense\"] = df_titanic[\"fare\"] / df_titanic[\"age\"]\n\nNote that we created the new variable, nonsense, by specifying on the left side of the = here and populating that column/variable via the expression on the right side of the =.\nWe could want to create a new variable by categorizing (or discretizing) the values of a quantitative variable (i.e. convert a quantitative variable to a categorical variable). We can do so with cut.\nIn the following, we create a new age_cat variable which represents whether a person is a child or an adult.\n\ndf_titanic[\"age_cat\"] = pd.cut(df_titanic[\"age\"],\n                              bins = [0, 18, 100],\n                              labels = [\"child\", \"adult\"])\n\n\n\n\n\n\n\nCheck In\n\n\n\nConsider the four mutations we just performed. In which ones did we reassign a column of the dataset, thus replacing the old values with new ones? In which ones did we create a brand-new column, thus retaining the old column(s) that were involved in the calculation?\n\n\n\n\n4.3.4.2 Transforming Categorical Variables\nIn some situations, especially later with modeling, we’ll need to convert categorical variables (stored as text) into quantitative (often coded) variables. Binary categorical variables can be converted into quantitative variables by coding one category as 1 and the other category as 0. (In fact, the survived column in the titanic dataset has already been coded this way.) The easiest way to do this is to create a boolean mask. For example, to convert gender to a quantitative variable female, which is 1 if the passenger was female and 0 otherwise, we can do the following:\n\ndf_titanic[\"female\"] = 1 * (df_titanic[\"gender\"] == \"female\")\n\nWhat do we do about a categorical variable with more than twwo categories, like embarked, which has four categories? In general, a categorical variable with K categories can be converted into K separate 0/1 variables, or dummy variables. Each of the K dummy variables is an indicator for one of the K categories. That is, a dummy variable is 1 if the observation fell into its particular category and 0 otherwise.\nAlthough it is not difficult to create dummy variables manually, the easiest way to create them is the get_dummies() function in pandas.\n\npd.get_dummies(df_titanic[\"embarked\"])\n\n\n\n\n\n\n\n\nB\nC\nQ\nS\n\n\n\n\n1176\nFalse\nFalse\nFalse\nTrue\n\n\n820\nFalse\nFalse\nFalse\nTrue\n\n\n53\nFalse\nTrue\nFalse\nFalse\n\n\n456\nFalse\nTrue\nFalse\nFalse\n\n\n282\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n1182\nFalse\nTrue\nFalse\nFalse\n\n\n296\nFalse\nFalse\nFalse\nTrue\n\n\n316\nFalse\nFalse\nFalse\nTrue\n\n\n439\nFalse\nTrue\nFalse\nFalse\n\n\n677\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n2207 rows × 4 columns\n\n\n\nWe may also want to change the levels of a categorical variable. A categorical variable can be transformed by mapping its levels to new levels. For example, we may only be interested in whether a person on the titanic was a passenger or a crew member. The variable class is too detailed. We can create a new variable, type, that is derived from the existing variable class. Observations with a class of “1st”, “2nd”, or “3rd” get a value of “passenger”, while observations with a class of “victualling crew”, “engineering crew”, or “deck crew” get a value of “crew”.\n\ndf_titanic[\"type\"] = df_titanic[\"class\"].map({\n    \"1st\": \"passenger\",\n    \"2nd\": \"passenger\",\n    \"3rd\": \"passenger\",\n    \"victualling crew\": \"crew\",\n    \"engineering crew\": \"crew\",\n    \"deck crew\": \"crew\"\n})\n\ndf_titanic\n\n\n\n\n\n\n\n\nname\ngender\nage\nclass\nembarked\ncountry\nticketno\nfare\nsurvived\nnonsense\nage_cat\nfemale\ntype\n\n\n\n\n1176\nSvensson, Mr. Johan\nmale\n7.400000\n3rd\nS\nSweden\n347060.0\n1.967196\n0\n0.265837\nchild\n0\npassenger\n\n\n820\nMitchell, Mr. Henry Michael\nmale\n7.200000\n2nd\nS\nEngland\n24580.0\n2.312535\n0\n0.321185\nchild\n0\npassenger\n\n\n53\nArtagaveytia, Mr. Ramon\nmale\n7.100000\n1st\nC\nArgentina\n17609.0\n3.893861\n0\n0.548431\nchild\n0\npassenger\n\n\n456\nGoldschmidt, Mr. George B.\nmale\n7.100000\n1st\nC\nUnited States\n17754.0\n3.530180\n0\n0.497208\nchild\n0\npassenger\n\n\n282\nCrosby, Captain. Edward Gifford\nmale\n7.000000\n1st\nS\nUnited States\n5735.0\n4.262680\n0\n0.608954\nchild\n0\npassenger\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1182\nTannūs, Master. As'ad\nmale\n0.041667\n3rd\nC\nLebanon\n2625.0\n2.091913\n1\n50.205923\nchild\n0\npassenger\n\n\n296\nDanbom, Master. Gilbert Sigvard Emanuel\nmale\n0.033333\n3rd\nS\nSweden\n347080.0\n2.644755\n0\n79.342661\nchild\n0\npassenger\n\n\n316\nDean, Miss. Elizabeth Gladys 'Millvina'\nfemale\n0.016667\n3rd\nS\nEngland\n2315.0\n3.001247\n1\n180.074822\nchild\n1\npassenger\n\n\n439\nGheorgheff, Mr. Stanio\nmale\nNaN\n3rd\nC\nBulgaria\n349254.0\n1.970059\n0\nNaN\nNaN\n0\npassenger\n\n\n677\nKraeff, Mr. Theodor\nmale\nNaN\n3rd\nC\nBulgaria\n349253.0\n1.970059\n0\nNaN\nNaN\n0\npassenger\n\n\n\n\n2207 rows × 13 columns\n\n\n\n\n\n\n4.3.5 Summarizing Rows\nSummarization of the rows of a dataset for column(s) of interest can take many different forms. This introduction will not be exhaustive, but certainly cover the basics.\n\n4.3.5.1 Summarizing a Quantitative Variable\nThere are a few descriptive statistics that can be computed directly including, but not limited to, the mean and median.\n\ndf_titanic[\"age\"].mean()\n\ndf_titanic[\"age\"].median()\n\ndf_titanic[[\"age\", \"fare\"]].mean()\n\nage     3.043673\nfare    2.918311\ndtype: float64\n\n\nWe can ask for a slightly more comprehensive description using .describe()\n\ndf_titanic[\"age\"].describe()\n\ndf_titanic.describe()\n\n\n\n\n\n\n\n\nage\nticketno\nfare\nsurvived\nnonsense\nfemale\n\n\n\n\ncount\n2205.000000\n1.316000e+03\n1291.000000\n2207.000000\n1289.000000\n2207.000000\n\n\nmean\n3.043673\n2.842157e+05\n2.918311\n0.322157\n2.147877\n0.221568\n\n\nstd\n1.215968\n6.334726e+05\n0.974452\n0.467409\n7.237694\n0.415396\n\n\nmin\n0.016667\n2.000000e+00\n1.108728\n0.000000\n0.265837\n0.000000\n\n\n25%\n2.200000\n1.426225e+04\n1.971383\n0.000000\n0.742371\n0.000000\n\n\n50%\n2.900000\n1.114265e+05\n2.645480\n0.000000\n0.936833\n0.000000\n\n\n75%\n3.800000\n3.470770e+05\n3.435945\n1.000000\n1.260935\n0.000000\n\n\nmax\n7.400000\n3.101317e+06\n6.238443\n1.000000\n180.074822\n1.000000\n\n\n\n\n\n\n\nNote that, by default, .describe() provides descriptive statistics for only the quantitative variables in the dataset.\nWe can enhance numerical summaries with .groupby(), which allows us to specify one or more variables that we’d like to group our work by.\n\ndf_titanic[[\"age\", \"survived\"]].groupby(\"survived\").mean()\n\n\n\n\n\n\n\n\nage\n\n\nsurvived\n\n\n\n\n\n0\n3.083194\n\n\n1\n2.960631\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith 2-3 people around you, look up how you would compute the correlation between two quantitative variables in Python. Compute the correlation between the age and fare variables in the titanic dataset.\n\n\n\n\n4.3.5.2 Summarizing a Categorical Variable\nWhen it comes to categorical variables we’re most often interested in frequency distributions (counts), relative frequency distributions, and cross-tabulations.\n\ndf_titanic[\"class\"].unique()\n\ndf_titanic[\"class\"].describe()\n\ncount     2207\nunique       7\ntop        3rd\nfreq       709\nName: class, dtype: object\n\n\nThe .unique() here allows us to see the unique values of the class variable. Notice that the results of .describe() on a categorical variable are much different.\nTo completely summarize a single categorical variable, we report the number of times each level appeared, or its frequency.\n\ndf_titanic[\"class\"].value_counts()\n\nclass\n3rd                 709\nvictualling crew    431\n1st                 324\nengineering crew    324\n2nd                 284\nrestaurant staff     69\ndeck crew            66\nName: count, dtype: int64\n\n\nInstead of reporting counts, we can also report proportions or probabilities, or the relative frequencies. We can calculate the relative frequencies by specifying normalize=True in .value_counts().\n\ndf_titanic[\"class\"].value_counts(normalize=True)\n\nclass\n3rd                 0.321251\nvictualling crew    0.195288\n1st                 0.146806\nengineering crew    0.146806\n2nd                 0.128681\nrestaurant staff    0.031264\ndeck crew           0.029905\nName: proportion, dtype: float64\n\n\nCross-tabulations are one way we can investigate possible relationships between categorical variables. For example, what can we say about the relationship between gender and survival on the Titanic?\n\n\n\n\n\n\nCheck In\n\n\n\nSummarize gender and survival individually by computing the frequency distributions of each.\n\n\nThis does not tell us how gender interacts with survival. To do that, we need to produce a cross-tabulation, or a “cross-tab” for short. (Statisticians tend to call this a contingency table or a two-way table.)\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"])\n\n\n\n\n\n\n\ngender\nfemale\nmale\n\n\nsurvived\n\n\n\n\n\n\n0\n130\n1366\n\n\n1\n359\n352\n\n\n\n\n\n\n\nA cross-tabulation of two categorical variables is a two-dimensional array, with the levels of one variable along the rows and the levels of the other variable along the columns. Each cell in this array contains the number of observations that had a particular combination of levels. So in the Titanic data set, there were 359 females who survived and 1366 males who died. From the cross-tabulation, we can see that there were more females who survived than not, while there were more males who died than not. Clearly, gender had a strong influence on survival because of the Titanic’s policy of “women and children first”.\nTo get probabilities instead of counts, we specify normalize=True.\n\npd.crosstab(df_titanic[\"survived\"], df_titanic[\"gender\"], normalize=True)\n\n\n\n\n\n\n\ngender\nfemale\nmale\n\n\nsurvived\n\n\n\n\n\n\n0\n0.058903\n0.618940\n\n\n1\n0.162664\n0.159493\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat about conditional proportions? With 2-3 people around you, discuss how you would compute the proportion of females that survived and the proportion of males that survived and then do it.\nNote, there are multiple ways to do this.\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tabular Data and Basic Data Operations</span>"
    ]
  },
  {
    "objectID": "04-pivoting_joining.html",
    "href": "04-pivoting_joining.html",
    "title": "5  Pivoting and Joining",
    "section": "",
    "text": "5.1 Introduction\nThis document demonstrates the use of the pandas library in Python to do pivoting and joining of datasets.\nimport pandas as pd\n# Population data from GapMinder\npopulation = pd.read_csv(\"/content/pop.csv\")",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pivoting and Joining</span>"
    ]
  },
  {
    "objectID": "04-pivoting_joining.html#introduction",
    "href": "04-pivoting_joining.html#introduction",
    "title": "5  Pivoting and Joining",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the pandas library installed then you will need to run\npip install pandas\nin the Jupyter terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pivoting and Joining</span>"
    ]
  },
  {
    "objectID": "04-pivoting_joining.html#pivoting-data-in-python",
    "href": "04-pivoting_joining.html#pivoting-data-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.2 Pivoting Data in Python",
    "text": "5.2 Pivoting Data in Python\nData come in all shapes and forms! Rare is the day when we can open a dataset for the first time and it’s ready for every type of visualization or analysis that we could want to do with it.\nIn addition to the wrangling we discussed in the previous chapter, there may be a need to reshape the dataset entirely. For example, the column names might be values themselves that we want to make use of.\nRecall our introduction of tidy data in the previous chapter…\n\n5.2.1 Tidy Data is Special Tabular Data\nFor most people, the image that comes to mind when thinking about data is indeed something tabular or spreadsheet-like in nature. Which is great!\nTabular data is a form preferred by MANY different data operations and work. However, we will want to take this one step further. In almost all data science work we want our data to be tidy\n\n\n\n\n\n\nNote\n\n\n\nA dataset is tidy if it adheres to following three characteristics:\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a single value\n\n\n\n\nIn the previous chapter you were asked to open up a GapMinder dataset here and to comment on whether this dataset was tidy or not. The answer was no, this dataset is not tidy. These datasets come with a row representing a country, each column representing a year, and each cell representing the value of the global indicator selected. To be tidy these three variables (country, year, global indicator) should each have their own column, instead of the year variable taking values as the column headers.\n\n\n5.2.2 Wide to Long Format\nThe GapMinder dataset is an example of what’s commonly referred to as data in a wide format. To make this dataset tidy we aim for a dataset with columns for country, year, and global indicator (e.g. population). Three columns is many fewer than the current number of columns, and so we will convert this dataset from wide to long format.\n\n\n\n\n\n\nWarning\n\n\n\nIt often helps to physically draw/map out what our current dataset looks like and what the look of our target dataset is, before actually trying to write any code to do this. Writing the code can be extremely easier after this exercise, and only makes future pivot operations easier.\n\n\nIn order to convert our dataset from wide to long format we will use .melt() (or .wide_to_long()) in pandas.\n\nlong_population = population.melt(id_vars=[\"country\"], var_name=\"year\", value_name=\"population\")\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith 2-3 people around you navigate to GapMinder, download the population dataset, and convert it from wide to long format. Does the result look how you expect? Is any other wrangling necessary?\n\n\n\n\n5.2.3 Long to Wide Format\nEven though certain data shapes are not considered tidy, they may be more conducive to performing certain operations than other shapes. For example, what if we were interested in the change in country population between 1950 and 2010? In the original wide shape of the GapMinder data this operation would have been a simple difference of columns like below.\n\npopulation[\"pop_diff\"] = population[\"2010\"] - population[\"1950\"]\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhy doesn’t the above code work without further wrangling? What in the dataset needs to change for this operation to work?\n\n\nIn the long format of our Gapminder dataset (long_population), this operation is less straightforward. Sometimes datasets come to us in long format and to do things like the operation above we need to convert that dataset from long to wide format. We can go the reverse direction (i.e. long to wide format) with .pivot() in pandas.\n\nwide_population = long_population.pivot(index = \"country\", columns = \"year\", values = \"population\")\nwide_population = wide_population.reset_index()\n\n\n\n\n\n\n\nLearn More\n\n\n\nWe haven’t spent much time discussing the index of a pandas DataFrame, but you can think of it like an address for data, or slices of data in a DataFrame. You can also think of an index (or indices) as row names, or axis labels, for your dataset. This can be useful for a number of functions in Python, and can enhance the look of results or visualizations.\nHowever, understanding them is not critical for what we will do in Python. Furthermore, variables that are indices for a DataFrame cannot be accessed or referenced in the same way as other variables in the DataFrame. So, we will avoid their use if possible.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pivoting and Joining</span>"
    ]
  },
  {
    "objectID": "04-pivoting_joining.html#joining-datasets-in-python",
    "href": "04-pivoting_joining.html#joining-datasets-in-python",
    "title": "5  Pivoting and Joining",
    "section": "5.3 Joining Datasets in Python",
    "text": "5.3 Joining Datasets in Python\nThe information you need is often spread across multiple data sets, so you will need to combine multiple data sets into one. In this chapter, we discuss strategies for combining information from multiple (tabular) data sets.\nAs a working example, we will use a data set of baby names collected by the Social Security Administration. Each data set in this collection contains the names of all babies born in the United States in a particular year. This data is publicly available, and a copy has been made available at https://dlsun.github.io/pods/data/names/.\n\n5.3.1 Concatenating and Merging Data\n\n5.3.1.1 Concatenation\nSometimes, the rows of data are spread across multiple files, and we want to combine the rows into a single data set. The process of combining rows from different data sets is known as concatenation.\nVisually, to concatenate two DataFrames, we simply stack them on top of one another.\nFor example, suppose we want to understand how the popularity of different names evolved between 1995 and 2015. The 1995 names and the 2015 names are stored in two different files: yob1995.txt and yob2015.txt, respectively. To carry out this analysis, we will need to combine these two data sets into one.\n\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames1995\n\n\n\n\n\n\n\n\nName\nSex\nCount\n\n\n\n\n0\nJessica\nF\n27935\n\n\n1\nAshley\nF\n26603\n\n\n2\nEmily\nF\n24378\n\n\n3\nSamantha\nF\n21646\n\n\n4\nSarah\nF\n21369\n\n\n...\n...\n...\n...\n\n\n26075\nZerek\nM\n5\n\n\n26076\nZhen\nM\n5\n\n\n26077\nZiggy\nM\n5\n\n\n26078\nZuberi\nM\n5\n\n\n26079\nZyon\nM\n5\n\n\n\n\n26080 rows × 3 columns\n\n\n\n\nnames2015 = pd.read_csv(data_dir + \"yob2015.txt\",\n                        header=None,\n                        names=[\"Name\", \"Sex\", \"Count\"])\nnames2015\n\n\n\n\n\n\n\n\nName\nSex\nCount\n\n\n\n\n0\nEmma\nF\n20455\n\n\n1\nOlivia\nF\n19691\n\n\n2\nSophia\nF\n17417\n\n\n3\nAva\nF\n16378\n\n\n4\nIsabella\nF\n15617\n\n\n...\n...\n...\n...\n\n\n33116\nZykell\nM\n5\n\n\n33117\nZyking\nM\n5\n\n\n33118\nZykir\nM\n5\n\n\n33119\nZyrus\nM\n5\n\n\n33120\nZyus\nM\n5\n\n\n\n\n33121 rows × 3 columns\n\n\n\nTo concatenate the two, we use the pd.concat() function, which accepts a list of pandas objects (DataFrames or Series) and concatenates them.\n\npd.concat([names1995, names2015])\n\n\n\n\n\n\n\n\nName\nSex\nCount\n\n\n\n\n0\nJessica\nF\n27935\n\n\n1\nAshley\nF\n26603\n\n\n2\nEmily\nF\n24378\n\n\n3\nSamantha\nF\n21646\n\n\n4\nSarah\nF\n21369\n\n\n...\n...\n...\n...\n\n\n33116\nZykell\nM\n5\n\n\n33117\nZyking\nM\n5\n\n\n33118\nZykir\nM\n5\n\n\n33119\nZyrus\nM\n5\n\n\n33120\nZyus\nM\n5\n\n\n\n\n59201 rows × 3 columns\n\n\n\n\nThere is no longer any way to distinguish the 1995 data from the 2015 data. To fix this, we can add a Year column to each DataFrame before we concatenate.\nThe indexes from the original DataFrames are preserved in the concatenated DataFrame. (To see this, observe that the last index in the DataFrame is about 33000, which corresponds to the number of rows in names2015, even though there are 59000 rows in the DataFrame.) That means that there are two rows with an index of 0, two rows with an index of 1, and so on. To force pandas to generate a completely new index for this DataFrame, ignoring the indices from the original DataFrames, we specify ignore_index=True.\n\n\nnames1995[\"Year\"] = 1995\nnames2015[\"Year\"] = 2015\nnames = pd.concat([names1995, names2015], ignore_index=True)\nnames\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n0\nJessica\nF\n27935\n1995\n\n\n1\nAshley\nF\n26603\n1995\n\n\n2\nEmily\nF\n24378\n1995\n\n\n3\nSamantha\nF\n21646\n1995\n\n\n4\nSarah\nF\n21369\n1995\n\n\n...\n...\n...\n...\n...\n\n\n59196\nZykell\nM\n5\n2015\n\n\n59197\nZyking\nM\n5\n2015\n\n\n59198\nZykir\nM\n5\n2015\n\n\n59199\nZyrus\nM\n5\n2015\n\n\n59200\nZyus\nM\n5\n2015\n\n\n\n\n59201 rows × 4 columns\n\n\n\nNow this is a DataFrame we can use!\n\n\n5.3.1.2 Merging (a.k.a Joining)\nMore commonly, the data sets that we want to combine actually contain different information about the same observations. In other words, instead of stacking the DataFrames on top of each other, as in concatenation, we want to stack them next to each other. The process of combining columns or variables from different data sets is known as merging or joining.\nThe observations may be in a different order in the two data sets, so merging is not as simple as placing the two DataFrames side-by-side.\nMerging is an operation on two DataFrames that returns a third DataFrame. By convention, the first DataFrame is referred to as the one on the “left”, while the second DataFrame is the one on the “right”.\nThis naming convention is reflected in the syntax of the .merge() function in pandas. In the code below, the “left” DataFrame, names1995, is quite literally on the left in the code, while the “right” DataFrame, names2015, is to the right. We also specify the variables to match across the two DataFrames.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"])\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nYear_x\nCount_y\nYear_y\n\n\n\n\n0\nJessica\nF\n27935\n1995\n1587\n2015\n\n\n1\nAshley\nF\n26603\n1995\n3424\n2015\n\n\n2\nEmily\nF\n24378\n1995\n11786\n2015\n\n\n3\nSamantha\nF\n21646\n1995\n5340\n2015\n\n\n4\nSarah\nF\n21369\n1995\n4521\n2015\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n15675\nZephan\nM\n5\n1995\n23\n2015\n\n\n15676\nZeppelin\nM\n5\n1995\n70\n2015\n\n\n15677\nZerek\nM\n5\n1995\n5\n2015\n\n\n15678\nZiggy\nM\n5\n1995\n44\n2015\n\n\n15679\nZyon\nM\n5\n1995\n148\n2015\n\n\n\n\n15680 rows × 6 columns\n\n\n\nThe most important component of merging two datasets is the presence of at least one key variable that both datasets share. This variable is sometimes referred to as an ID variable. It’s this variable that we will want to merge on, i.e. use to combine the two datasets intelligently.\nThe variables that we joined on (Name and Sex) appear once in the final DataFrame. The variable Count, which we did not join on, appears twice—since there was a column called Count in both of the original DataFrames. Notice that pandas automatically appended the suffix _x to the name of the variable from the left DataFrame and _y to the one from the right DataFrame. We can customize the suffixes by specifying the suffixes= parameter.\n\nnames1995.merge(names2015, on=[\"Name\", \"Sex\"], suffixes=(\"1995\", \"2015\"))\n\n\n\n\n\n\n\n\nName\nSex\nCount1995\nYear1995\nCount2015\nYear2015\n\n\n\n\n0\nJessica\nF\n27935\n1995\n1587\n2015\n\n\n1\nAshley\nF\n26603\n1995\n3424\n2015\n\n\n2\nEmily\nF\n24378\n1995\n11786\n2015\n\n\n3\nSamantha\nF\n21646\n1995\n5340\n2015\n\n\n4\nSarah\nF\n21369\n1995\n4521\n2015\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n15675\nZephan\nM\n5\n1995\n23\n2015\n\n\n15676\nZeppelin\nM\n5\n1995\n70\n2015\n\n\n15677\nZerek\nM\n5\n1995\n5\n2015\n\n\n15678\nZiggy\nM\n5\n1995\n44\n2015\n\n\n15679\nZyon\nM\n5\n1995\n148\n2015\n\n\n\n\n15680 rows × 6 columns\n\n\n\nIn the code above, we assumed that the columns that we joined on had the same names in the two data sets. What if they had different names? For example, suppose the variable had been called Sex in one data set and Gender in the other. We can specify which variables to use from the left and right data sets using the left_on= and right_on= parameters.\n\n# Create new DataFrames where the column names are different\nnames2015_ = names2015.rename({\"Sex\": \"Gender\"}, axis=1)\n\n# This is how you merge them.\nnames1995.merge(\n    names2015_,\n    left_on=(\"Name\", \"Sex\"),\n    right_on=(\"Name\", \"Gender\")\n)\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nYear_x\nGender\nCount_y\nYear_y\n\n\n\n\n0\nJessica\nF\n27935\n1995\nF\n1587\n2015\n\n\n1\nAshley\nF\n26603\n1995\nF\n3424\n2015\n\n\n2\nEmily\nF\n24378\n1995\nF\n11786\n2015\n\n\n3\nSamantha\nF\n21646\n1995\nF\n5340\n2015\n\n\n4\nSarah\nF\n21369\n1995\nF\n4521\n2015\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15675\nZephan\nM\n5\n1995\nM\n23\n2015\n\n\n15676\nZeppelin\nM\n5\n1995\nM\n70\n2015\n\n\n15677\nZerek\nM\n5\n1995\nM\n5\n2015\n\n\n15678\nZiggy\nM\n5\n1995\nM\n44\n2015\n\n\n15679\nZyon\nM\n5\n1995\nM\n148\n2015\n\n\n\n\n15680 rows × 7 columns\n\n\n\n\n\n5.3.1.3 One-to-One and Many-to-One Relationships\nIn the example above, there was at most one combination of Name and Sex in the 2015 data set for each combination of Name and Sex in the 1995 data set. These two data sets are thus said to have a one-to-one relationship. The same would be true of combining two GapMinder datasets.\nHowever, two data sets need not have a one-to-one relationship! Two datasets could have a many-to-one relationship. In general, it’s extremely important to think carefully about what variables each of your two datasets have to begin with, and what variables you want your merged dataset to have…and what that merged dataset will represent with respect to your data.\n\n\n5.3.1.4 Many-to-Many Relationships: A Cautionary Tale\nIt is also possible for multiple rows in the left DataFrame to match multiple rows in the right DataFrame. In this case, the two data sets are said to have a many-to-many relationship. Many-to-many joins can lead to misleading analyses, so it is important to exercise caution when working with many-to-many relationships.\nFor example, in the baby names data set, the Name variable is not uniquely identifying. For example, there are both males and females with the name “Jessie”.\n\njessie1995 = names1995[names1995[\"Name\"] == \"Jessie\"]\njessie1995\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n248\nJessie\nF\n1138\n1995\n\n\n16047\nJessie\nM\n903\n1995\n\n\n\n\n\n\n\n\njessie2015 = names2015[names2015[\"Name\"] == \"Jessie\"]\njessie2015\n\n\n\n\n\n\n\n\nName\nSex\nCount\nYear\n\n\n\n\n615\nJessie\nF\n469\n2015\n\n\n20009\nJessie\nM\n233\n2015\n\n\n\n\n\n\n\nIf we join these two DataFrames on Name, then we will end up with a many-to-many join, since each “Jessie” row in the 1995 data will be paired with each “Jessie” row in the 2015 data.\n\njessie1995.merge(jessie2015, on=[\"Name\"])\n\n\n\n\n\n\n\n\nName\nSex_x\nCount_x\nYear_x\nSex_y\nCount_y\nYear_y\n\n\n\n\n0\nJessie\nF\n1138\n1995\nF\n469\n2015\n\n\n1\nJessie\nF\n1138\n1995\nM\n233\n2015\n\n\n2\nJessie\nM\n903\n1995\nF\n469\n2015\n\n\n3\nJessie\nM\n903\n1995\nM\n233\n2015\n\n\n\n\n\n\n\nNotice that Jessie ends up appearing four times:\n\nFemale Jessies from 1995 are matched with female Jessies from 2015. (Good!)\nMale Jessies from 1995 are matched with male Jessies from 2015. (Good!)\nFemale Jessies from 1995 are matched with male Jessies from 2015. (This is perhaps undesirable.)\nMale Jessies from 1995 are matched with female Jessies from 2015. (Also unexpected and undesirable.)\n\nIf we had used a data set like this to determine the number of Jessies in 1995, then we would end up with the wrong answer, since we would have double-counted both female and male Jessies as a result of the many-to-many join. This is why it is important to exercise caution when working with (potential) many-to-many relationships.\n\n\n\n5.3.2 Types of Joins\nAbove, we saw how to merge (or join) two data sets by matching on certain variables. But what happens when a row in one DataFrame has no match in the other?\nFirst, let’s investigate how pandas handles this situation by default. The name “Nevaeh”, which is “heaven” spelled backwards, took after Sonny Sandoval of the band P.O.D. gave his daughter the name in 2000. Let’s look at how common this name was five years earlier and five years after.\n\ndata_dir = \"http://dlsun.github.io/pods/data/names/\"\n\nnames1995 = pd.read_csv(data_dir + \"yob1995.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\nnames2005 = pd.read_csv(data_dir + \"yob2005.txt\",\n                        header=None, names=[\"Name\", \"Sex\", \"Count\"])\n\n\nnames1995[names1995.Name == \"Nevaeh\"]\n\n\n\n\n\n\n\n\nName\nSex\nCount\n\n\n\n\n\n\n\n\n\n\nnames2005[names2005.Name == \"Nevaeh\"]\n\n\n\n\n\n\n\n\nName\nSex\nCount\n\n\n\n\n68\nNevaeh\nF\n4552\n\n\n21353\nNevaeh\nM\n56\n\n\n\n\n\n\n\nIn 1995, there were no girls (at least fewer than 5) named Nevaeh; just eight years later, there were over 4500 girls (and even 56 boys) with the name. It seems like Sonny Sandoval had a huge effect.\nWhat happens to the name “Nevaeh” when we merge the two data sets?\n\nnames = names1995.merge(names2005, on=[\"Name\", \"Sex\"])\nnames[names.Name == \"Nevaeh\"]\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nCount_y\n\n\n\n\n\n\n\n\n\nBy default, pandas only includes combinations that are present in both DataFrames. If it cannot find a match for a row in one DataFrame, then the combination is simply dropped.\nBut in this context, the fact that a name does not appear in one data set is informative. It means that no babies were born in that year with that name. We might want to include names that appeared in only one of the two DataFrames, rather than just the names that appeared in both.\nThere are four types of joins, distinguished by whether they include the rows from the left DataFrame, the right DataFrame, both, or neither:\n\ninner join (default): only values that are present in both DataFrames are included in the result\nouter join: any value that appears in either DataFrame is included in the result\nleft join: any value that appears in the left DataFrame is included in the result, whether or not it appears in the right DataFrame\nright join: any value that appears in the right DataFrame is included in the result, whether or not it appears in the left DataFrame.\n\nOne way to visualize the different types of joins is using Venn diagrams. The shaded region indicates which rows that are included in the output. For example, only rows that appear in both the left and right DataFrames are included in the output of an inner join.\n\nIn pandas, the join type is specified using the how= argument.\nNow let’s look at the examples of each of these types of joins.\n\n# inner join\nnames_inner = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"inner\")\nnames_inner\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nCount_y\n\n\n\n\n0\nJessica\nF\n27935\n8108\n\n\n1\nAshley\nF\n26603\n13270\n\n\n2\nEmily\nF\n24378\n23930\n\n\n3\nSamantha\nF\n21646\n13633\n\n\n4\nSarah\nF\n21369\n11527\n\n\n...\n...\n...\n...\n...\n\n\n19119\nZeppelin\nM\n5\n7\n\n\n19120\nZerek\nM\n5\n8\n\n\n19121\nZhen\nM\n5\n7\n\n\n19122\nZiggy\nM\n5\n6\n\n\n19123\nZyon\nM\n5\n102\n\n\n\n\n19124 rows × 4 columns\n\n\n\n\n# outer join\nnames_outer = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"outer\")\nnames_outer\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nCount_y\n\n\n\n\n0\nJessica\nF\n27935.0\n8108.0\n\n\n1\nAshley\nF\n26603.0\n13270.0\n\n\n2\nEmily\nF\n24378.0\n23930.0\n\n\n3\nSamantha\nF\n21646.0\n13633.0\n\n\n4\nSarah\nF\n21369.0\n11527.0\n\n\n...\n...\n...\n...\n...\n\n\n39490\nZymiere\nM\nNaN\n5.0\n\n\n39491\nZyrell\nM\nNaN\n5.0\n\n\n39492\nZyrian\nM\nNaN\n5.0\n\n\n39493\nZyshon\nM\nNaN\n5.0\n\n\n39494\nZytavious\nM\nNaN\n5.0\n\n\n\n\n39495 rows × 4 columns\n\n\n\nNames like “Zyrell” and “Zyron” appeared in the 2005 data but not the 1995 data. For this reason, their count in 1995 is NaN. In general, there will be missing values in DataFrames that result from an outer join. Any time a value appears in one DataFrame but not the other, there will be NaNs in the columns from the DataFrame missing that value.\n\nnames_inner.isnull().sum()\n\nName       0\nSex        0\nCount_x    0\nCount_y    0\ndtype: int64\n\n\nLeft and right joins preserve data from one DataFrame but not the other. For example, if we were trying to calculate the percentage change for each name from 1995 to 2005, we would want to include all of the names that appeared in the 1995 data. If the name did not appear in the 2005 data, then that is informative.\n\n# left join\nnames_left = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"left\")\nnames_left\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nCount_y\n\n\n\n\n0\nJessica\nF\n27935\n8108.0\n\n\n1\nAshley\nF\n26603\n13270.0\n\n\n2\nEmily\nF\n24378\n23930.0\n\n\n3\nSamantha\nF\n21646\n13633.0\n\n\n4\nSarah\nF\n21369\n11527.0\n\n\n...\n...\n...\n...\n...\n\n\n26075\nZerek\nM\n5\n8.0\n\n\n26076\nZhen\nM\n5\n7.0\n\n\n26077\nZiggy\nM\n5\n6.0\n\n\n26078\nZuberi\nM\n5\nNaN\n\n\n26079\nZyon\nM\n5\n102.0\n\n\n\n\n26080 rows × 4 columns\n\n\n\nThe result of the left join has NaNs in the columns from the right DataFrame.\n\nnames_left.isnull().sum()\n\nName          0\nSex           0\nCount_x       0\nCount_y    6956\ndtype: int64\n\n\nThe result of the right join, on the other hand, has NaNs in the column from the left DataFrame.\n\n# right join\nnames_right = names1995.merge(names2005, on=[\"Name\", \"Sex\"], how=\"right\")\nnames_right\n\n\n\n\n\n\n\n\nName\nSex\nCount_x\nCount_y\n\n\n\n\n0\nEmily\nF\n24378.0\n23930\n\n\n1\nEmma\nF\n5041.0\n20335\n\n\n2\nMadison\nF\n9775.0\n19562\n\n\n3\nAbigail\nF\n7821.0\n15747\n\n\n4\nOlivia\nF\n7624.0\n15691\n\n\n...\n...\n...\n...\n...\n\n\n32534\nZymiere\nM\nNaN\n5\n\n\n32535\nZyrell\nM\nNaN\n5\n\n\n32536\nZyrian\nM\nNaN\n5\n\n\n32537\nZyshon\nM\nNaN\n5\n\n\n32538\nZytavious\nM\nNaN\n5\n\n\n\n\n32539 rows × 4 columns\n\n\n\n\nnames_right.isnull().sum()\n\nName           0\nSex            0\nCount_x    13415\nCount_y        0\ndtype: int64\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nDownload a second GapMinder dataset and merge it with the population dataset from above. Did you have to pivot first? Which order of operations makes the most sense? Is your resulting dataset tidy?\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nClick here to solve a riddle using data manipulation.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pivoting and Joining</span>"
    ]
  },
  {
    "objectID": "04-pivoting_joining.html#data-wrangling-and-ai",
    "href": "04-pivoting_joining.html#data-wrangling-and-ai",
    "title": "5  Pivoting and Joining",
    "section": "5.4 Data Wrangling and AI",
    "text": "5.4 Data Wrangling and AI\nThe advice in this section applies to data analysis tasks in general, not only to the basic wrangling and summarizing in this chapter - but since this is our first foray in to data wrangling code, let’s dive right in!\nIn data processing and summarizing, we can think of four main ways that genAI can support your work. In this chat, we make use of all four of the steps below; open it up and follow along.\n\n5.4.0.1 1. As a planner, to help you chart out your wrangling steps.\nData analysis involves going from Point A (your current dataset) to Point B (your desired calculation or visualization). One of the most difficult aspects of data science is figuring out what intermediate steps will get you there. For this, a genAI tool can be a great “thinking buddy” and suggest a path forward.\nAlthough it may not give a perfect solution (especially since it typically cannot see your data), two heads are often better than one. The generated suggestions may include data cleaning steps you hadn’t thought of, or help you fill in a gap when you are stuck.\nAs always, you will get your best results if your prompt is well-specified: make sure to describe the structure of the dataset you have, and the exact nature of the output you are trying to produce.\n\n\n\n\n\n\nExample\n\n\n\nIn our chat example, we got back some pretty useful steps, including a great reminder to make sure our date-time data is the proper type. However, because we weren’t overly detailed about the dataset, the AI had to “guess” about which columns existed. And since we also didn’t clarify what we wanted our plot to look like, it had to offer an “optional” Step 6.\n\n\n\n\n5.4.0.2 2. As documentation, when you are struggling with the exact use case or syntax of a function.\nIt is extremely common in coding to know what function(s) you need for your task, but to be a little fuzzy on the details of how to make then run the way you want. While the information you need can technically always be found in the official documentation of the functions, these can sometimes be difficult to understand. One of the absolute strongest contributions of genAI is the ability to generate examples and to explain every step. In this regard, the AI tool is much like a human tutor!\n\n\n\n\n\n\nExample\n\n\n\nIn our demo chat, we see that the AI was able to explain the to_datetime() function to us in great detail, including several examples of when you might need to use common optional arguments like format or errors.\n\n\n\n\n5.4.0.3 3. As a search tool, when you want to find the perfect function.\nThe beauty of packages like pandas is that many, many functions are provided to give you “shortcuts” for common data tasks. The curse is that you don’t always know if a particular function exists!\nCertainly, it is possible to look in official documentation - but this can be tedious and involve guessing which functions might solve your problem by their name alone. A better option might be a static search service, like Google - but to trigger useful results, you often need to use key words that precisely describe what you are looking for.\nSince genAI tools can interpret human-style text, they are often able to understand what you are looking for and help you find the perfect function.\n\n\n\n\n\n\nExample\n\n\n\nIn response to our question about misspellings in the dataset, the genAI offered two solutions: a simple one for when the possible misspellings are known, and a complex one for general use. In the second solution, it suggested an additional library called fuzzywuzzy.\n\n\n\n\n5.4.0.4 4. As a debugger, when you can spot where in your analysis something is going wrong.\n\n\n\n\n\n\nExample\n\n\n\nHere we wanted to take the mean of a column called num_admitted, grouped by hour and severity. Unfortunately we made an extremely common mistake of putting the column name inside the mean() function. The genAI tool was able to explain why this approach is wrong, and offer possible solutions.\n\n\n\n\n5.4.1 Cautionary tales and advice\n\n5.4.1.1 Debugging works best on small pieces\nSuppose you write out your full end-to-end data analysis, without testing or checking any intermediate steps, and it does not work as you hoped. Would you put the whole thing into an LLM for help?\nWhile this may work in some straightforward cases, it also may confuse and frustrate you more, as the AI suggests a long list of possible issues for you to comb through.\nInstead, we recommend stepping through your analysis line by line, until you reach a step where the results are not what you intended. (This is good troubleshooting strategy already, and you might find your bug before you even turn to an AI tool!) Then, you can ask the AI about only one or two lines of code that aren’t working, and get a more direct and helpful answer.\n\n\n5.4.1.2 Beware of hallucinations\nKeep in mind that a genAI response is not the same as a systematic search: it is producing output trying to mimic what it has seen in the training data.\nThis means that if you ask for a function for a particular task, an AI tool might simply invent a function that seems similar to others it has seen in training. As AI tools are improving, and training data sources for python code are increasing, hallucinations about code are becoming quite rare. However, it’s important to be aware that AI-produced suggestions are not equivalent to finding the function in official documentation; you’ll need to verify for yourself.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pivoting and Joining</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html",
    "href": "05-function_writing.html",
    "title": "6  Writing Custom Functions",
    "section": "",
    "text": "6.1 Introduction\nA function is a set of actions that we group together and name. Throughout this course, you’ve already used a bunch of different functions in python that are built into the language or added through packages: mean, ggplot, merge, etc. In this chapter, we’ll be writing our own functions.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html#defining-a-function",
    "href": "05-function_writing.html#defining-a-function",
    "title": "6  Writing Custom Functions",
    "section": "6.2 Defining a function",
    "text": "6.2 Defining a function\n\n6.2.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead of copy pasting. The motivation behind this is the “don’t repeat yourself” (DRY) principle. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\nConsider the following data analysis task done in python:\n\ndf = pd.DataFrame({'a':np.random.normal(1,1,10), 'b':np.random.normal(2,2,10), 'c':np.random.normal(3,3,10), 'd':np.random.normal(4,4,10)})\n\ndf['a'] = (df['a'] - min(df['a']))/(max(df['a']) - min(df['a']))\n\ndf['b'] = (df['b'] - min(df['a']))/(max(df['b']) - min(df['b']))\n\ndf['c'] = (df['c'] - min(df['c']))/(max(df['c']) - min(df['c']))\n\ndf['d'] = (df['d'] - min(df['d']))/(max(df['d']) - min(df['d']))\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat does this code do?\n\n\n\n\n\n\n\n\nExample\n\n\n\nIn theory, the code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b!\nWriting a function will help us avoid these subtle copy-paste errors.\n\n\n\n\n6.2.2 Building up the function\nTo write a function, we first analyze the code to determine how many inputs it has\n\ndf['a'] = (df['a'] - min(df['a']))/(max(df['a']) - min(df['a']))\n\nThis code has only one input: df['a'].\n\n\n\n\n\n\nCheck In\n\n\n\nWhat is the object structure of this input?\n\n\nNow we choose an argument name for our new input. It’s nice if the argument name reminds the user of what type or structure of object is expected.\nIn this case, it might help to replace df$a with vec.\n\nvec = df['a'] \n\n(vec - min(vec))/(max(vec) - min(vec))\n\n0    0.000000\n1    0.526701\n2    1.000000\n3    0.797135\n4    0.687501\n5    0.554046\n6    0.570852\n7    0.632162\n8    0.422267\n9    0.451399\nName: a, dtype: float64\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times) or separating steps to avoid nested parentheses (for instance, computing max first).\n\nvec = df['a'] \nmin_vec = min(vec)\nmax_vec = max(vec)\n\n(vec - min_vec)/(max_vec - min_vec)\n\n0    0.000000\n1    0.526701\n2    1.000000\n3    0.797135\n4    0.687501\n5    0.554046\n6    0.570852\n7    0.632162\n8    0.422267\n9    0.451399\nName: a, dtype: float64\n\n\nFinally, we turn this code into a function with the def command:\n\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled_vec = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled_vec\n\n\nThe name of the function, rescale_vector, describes what the function does - it rescales a vector (pandas Series or numpy array).\nThe function takes one argument, named vec; any references to this value within the function will use vec as the name.\nThe code that actually does what your function is supposed to do goes in the body of the function, after the :. It is important for the body of the function to be indented.\nThe function returns the computed object you want to hand back to the user: in this case, rescaled_vec.\n\n\n\n\n\n\n\nNote\n\n\n\nSome people prefer to create a final object and then return: that object, as we have done above with rescaled_vec.\nOthers prefer fewer objects and longer lines of code, i.e.,\n\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  return (vec - min_vec)/(max_vec - min_vec)\n\nThese two approaches will work identically; it’s a matter of personal preference.\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.\nNow, we are able to use our function to avoid the repetition:\n\ndf['a'] = rescale_vector(df['a'])\ndf['b'] = rescale_vector(df['b'])\ndf['c'] = rescale_vector(df['c'])\ndf['d'] = rescale_vector(df['d'])\n\nYou probably notice there is still a little bit of repetition here, with df['a'] appearing on both the left and right side of the =. But this is good repetition! When we assign or reassign an object or column, we want that to be an obvious and deliberate choice.\nIt’s also possible that you might have preferred to keep your original column untouched, and to make new columns for the rescaled data:\n\ndf['a_scaled'] = rescale_vector(df['a'])\ndf['b_scaled'] = rescale_vector(df['b'])\ndf['c_scaled'] = rescale_vector(df['c'])\ndf['d_scaled'] = rescale_vector(df['d'])\n\n\n\n6.2.3 Documenting your function\nWhen you want to use a function in python, but you can’t quite remember exactly how it works, you might be in the habit of typing ?fun_name or help(fun_name) to be able to see the documentation for that function.\nWhen you write your own function - whether for yourself or for others - it’s important to provide reminders of what the function is for and how it works.\nWe do this by adding text in a very specific structure into the body of our function:\n\ndef rescale_vector(vec):\n  \n  \"\"\"\n  Rescales a numeric vector to have a max of 1 and min of 0\n  \n  Parameter\n  ---------\n  vec : num\n    A list, numpy array, or pandas Series of numeric data.\n\n  Returns\n  -------\n  array \n    A numpy array containing the rescaled values.\n  \"\"\"\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled_vec = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled_vec\n\nA few important aspects of the above to note:\n\nThe use of three quotation marks, \"\"\" is necessary for this text to be recognized as official documentation.\nThe exact structure of the Parameters and Returns sections, with underlines, is important for consistency with other documentation. There are a few other formatting styles that are generally accepted; we’ll stick with the one in this example in this course.\nWhen listing the arguments, a.k.a. parameters of our function, we specify the name of the argument, the data type that is expected, and a brief description.\nWhen listing the returns of our function, we specify what object structure is being returned, and a brief description.\nThe blank line after the final \"\"\" is important!\n\n\n\n\n\n\n\nCheck In\n\n\n\nDefine the function using the code above, then run help(rescale_vector). Pretty satisfying, right?",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html#scope",
    "href": "05-function_writing.html#scope",
    "title": "6  Writing Custom Functions",
    "section": "6.3 Scope",
    "text": "6.3 Scope\nIn the previous example, you might have expected\n\nrescale_vector(df['a'])\n\nto change the column of the df object automatically - but it does not, unless you explicitly reassign the results! This is because everything that happens “inside” the function cannot change the status of objects outside the function.\nThis critical programming concept - determining when objects are changed and when they can be accessed - is called scope.\nIn general, an object is only available within the environment where it was created. Right now, as you work in your Jupyter notebook or Quarto document, you are interacting with your global environment. Run the globals() function to see all the objects you have created and libraries you have loaded so far.\nJust as someone sitting next to you on a different computer can’t access an object in your own global environment, the body of a function is its own function environment.\nAnything that is created or altered in the function environment does not impact the global environment - locally, you only “see” what the function returns.\n\n\n\n\n\n\nJust our opinion...\n\n\n\n\n\n\nLike Las Vegas, what happens in a function stays in that function\n\n\n\n\n\n6.3.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function.\nNote that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\nConsider the following code:\n\na = 10\n\ndef myfun(a):\n  \n  a = a + 10\n  \n  return a\n\n\nmyfun(a)\n\na + 5\n\n\n\n\n\n\n\nCheck In\n\n\n\nWithout running the code, what do you think will be printed out by the last two lines of code?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nThe object named a within the function environment, i.e. the parameter of the function, was altered to be equal to 20.\nThen, the function returned the value of its parameter a, which was 20.\nHere is a sketch of that idea:\n\nHowever, the object named a in our global environment is not impacted. The code a + 5, outside the function, still refers to the object in the global environment, which is equal to 10.\n\n\n\n\n6.3.2 Nested environments and scope\nOne thing to notice is that each time the function is run, it creates a new local environment. That is, previous running of a function can’t impact future runs of that function.\nFor example, this code gives the same answer over and over; it does not continue to add 10 to a copy of a, because it never alters the object a in the global environment or the parameters of the other functions’ local environments.\n\ndef myfun(a):\n  \n  a = a + 10\n  \n  return a\n\nmyfun(a)\nmyfun(a)\nmyfun(a)\n\nNameError: name 'a' is not defined\n\n\nHowever, all of the local environments are considered to be inside of your global environment. That is, while they cannot change objects in the global environment, they can “see” those objects.\nNotice in the below code that we don’t pass any arguments to myfun(). But it is still able to compute b + 1 and return an answer!\n\nb = 10\n\ndef myfun():\n  return b + 1\n\nmyfun()\n\n11\n\n\nA function will always look in its local environment first; then check the global for backups:\n\nb = 10\n\ndef myfun():\n  b = 20\n  return b + 1\n\nmyfun()\n\n21\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWriting a function that relies on global objects is in general terrible programming practice!!!\nWhat if you accidentally change or delete that global object? Suddenly the exact same code, i.e. myfun() runs differently or not at all.\n\n\nAlas, although this is bad practice, we quite often “cheat” in Data Science and use global references to our dataset in function shortcuts, e.g.\n\npenguins = load_penguins()\n\ndef plot_my_data(cat_var):\n  \n  plot = (ggplot(penguins, aes(x = cat_var)) + geom_bar())\n  \n  return plot\n  \n  \nplot_my_data('species')\n\n\n\n\n\n\n\n\nThis “trick” is sometimes called Dynamic Lookup. You should mostly avoid it; but when you use it carefully and deliberately in the context of a specific data analysis, it can be a convenient approach.\n\n\n\n\n\n\nCheck In\n\n\n\nWrite a custom function that does the following:\n\nLimit the penguins dataset to a user-chosen species and/or island.\nMakes a scatterplot of the penguin bill length and depth.\n\nTry writing a version of this function using dynamic lookup, and a version where everything the function needs is passed in directly.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html#unit-tests",
    "href": "05-function_writing.html#unit-tests",
    "title": "6  Writing Custom Functions",
    "section": "6.4 Unit tests",
    "text": "6.4 Unit tests\nSo: You have now written your first custom function. Hooray!\n\nNow, before you move on to further analysis, you have an important job to do. You need to make sure that your function will work - or will break in a helpful way - when users give unexpected input.\n\n6.4.1 Unit testing\nThe absolute most important thing to do after defining a function is to run some unit tests.\nThis refers to snippets of code that will try your function out on both “ordinary” user input, and strange or unexpected user input.\nFor example, consider the plot_my_data function defined above. We immediately unit tested it by running\n\nplot_my_data('species')\n\n\n\n\n\n\n\n\nBut what if someone tried to enter the name of a variable that is not categorical? Or a variable that doesn’t exist in the penguins dataset? Or an input that is not a variable name (string)? Or no input at all?\n\nplot_my_data('bill_length_mm')\n\nplot_my_data('name')\n\nplot_my_data(5)\n\nplot_my_data(True)\n\nplot_my_data()\n\nTypeError: plot_my_data() missing 1 required positional argument: 'cat_var'\n\n\n\n\n\n\n\n\nExample\n\n\n\nAre all of these outputs what you expected? Why or why not? Can you explain why the unexpected behavior happened?\n\n\n\n\n6.4.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But as we saw above, you can’t guarantee that the person using your function knows that they need a certain type of input, and they might be confused by how your function handles that input. In these cases, it’s best to validate your function input.\n\n6.4.2.1 if-else and sys.exit\nGenerally your approach to validation will be to check some conditions, and exit the function if they are not met. The function exit() from the sys library is a good approach. You want to make sure you write an informative error statement in your exit, so that the user knows how to fix what went wrong.\n\n\n\n\n\n\nLearn More\n\n\n\nThis article provides a short guide to writing informative error messages.\n\n\nThe most common condition checking is to make sure the object type is correct - i.e., that the user inputs a string, and that the string refers to a categorical (a.k.a. “object”) variable in the penguins dataset.\nRecall that you can “reverse” a boolean (True or False) value using the not statement. Sometimes, it is easier to check if a condition is not met than to list all the “good” conditions.\nPutting these together, we can check if our user input to plot_my_data is what we expect:\n\nfrom sys import exit\n\ndef plot_my_data(cat_var):\n    \n  if not isinstance(cat_var, str):\n    exit(\"Please provide a variable name in the form of a string.\")\n    \n  if not (cat_var in penguins.columns):\n    exit(\"The variable provided is not found in the penguins dataset.\")\n    \n  if not penguins[cat_var].dtypes == 'object':\n    exit(\"Please provide the name of a categorical (object type) variable.\")\n  \n  plot = (ggplot(penguins, aes(x = cat_var)) + geom_bar())\n  \n  return plot\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that we have used the isinstance base python function to check that the user inputted a string; but we have used the dtype method to check the data type of the column of data in a pandas dataframe.\nWhen checking types and structures, be careful about “special” object types from packages, like pandas data frames or numpy arrays - they each unfortunately have their own type checking functions and their own names for types.\n\n\nNow, let’s retry our unit tests:\n\nplot_my_data('bill_length_mm')\n\nplot_my_data('name')\n\nplot_my_data(5)\n\nplot_my_data(True)\n\nplot_my_data()\n\nSystemExit: Please provide the name of a categorical (object type) variable.\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused. If you’re interested, Wikipedia has more about defensive programming.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html#debugging",
    "href": "05-function_writing.html#debugging",
    "title": "6  Writing Custom Functions",
    "section": "6.5 Debugging",
    "text": "6.5 Debugging\nNow that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll find newer and more exciting ways to break your code!).\n\n\n\nThe faces of debugging (by Allison Horst)\n\n\nLet’s start with the basics: print debugging.\n\n6.5.1 Print Debugging\nThis technique is basically exactly what it sounds like. You insert a ton of print statements to give you an idea of what is happening at each step of the function.\nLet’s try it out on the rescale_vector function.\n\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\nSuppose we try the following:\n\nmy_vec = [-1,0,1,2,3]\n\nmy_vec = np.sqrt(my_vec)\n\nrescale_vector(my_vec)\n\narray([nan, nan, nan, nan, nan])\n\n\nYou probably have spotted the issue here, but what if it wasn’t obvious, and we wanted to know why our function was returning an array of nan values.\nIs the culprit the min? The max?\n\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  print(\"min: \" + str(min_vec))\n  \n  max_vec = max(vec)\n  print(\"max: \" + str(max_vec))\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\n\nrescale_vector(my_vec)\n\nmin: nan\nmax: nan\n\n\narray([nan, nan, nan, nan, nan])\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how the print() statements cause information to get printed out as the function ran, but did not change the return value of the function!\n\n\nHmmm, both the min and the max were nan. This explains why our rescaling introduced all missing values!\nSo, the issue must be with the user input itself. Let’s rewrite our function to take a look at that.\n\ndef rescale_vector(vec):\n  \n  print(vec)\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\n\nrescale_vector(my_vec)\n\n[       nan 0.         1.         1.41421356 1.73205081]\n\n\narray([nan, nan, nan, nan, nan])\n\n\nAh-ha! The first value of the input vector is a nan.\nIdeally, the user would not input a vector with missing values. But it’s our job to make sure the function is prepared to handle them.\n\n\n\n\n\n\nCheck In\n\n\n\nAdd code to the rescale_vector function definition to check if the vector has any nan values, and give an informative error message if so.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nThink of other options for handling nans in user input in this function. What are the pros and cons of writing functions that are opinionated - i.e., that give errors unless the user input is perfect - versus functions that try to work with imperfect input?\n\n\n\n\n6.5.2 Beyond print statements: breakpoints\nWhile print() statements work fine as a quick-and-dirty debugging strategy, you will soon get tired of using them, since you have to change your function and reload it every time you want to check something.\nA more elegant - and ultimately easier - approach is to “dive in” to the environment of the function itself, where you can interact with the parameters in the local environment the same way you might interact with your global environment.\nTo do this, we will set a breakpoint in our function. This will cause the function to run until that point, and then stop and let us play around in the environment.\n\ndef rescale_vector(vec):\n  \n  breakpoint()\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\n\n\n\n\n\n\nCheck In\n\n\n\nSet a breakpoint in your rescale_vector code, as above, and then run the function on a vector.\nPlay around with the interface of the local environment until you get used to this.\n\n\n\n\n6.5.3 pdb (“python de-bugger”)\nAlthough setting breakpoints can be much cleaner and more convenient than several print() statements, using breakpoint() still required us to modify and reload the function.\nThe most advanced and clean approach to debugging is to use the pdb library to dive straight in to the local environment.\n\ndef rescale_vector(vec):\n  \n  min_vec = min(vec)\n  max_vec = max(vec)\n  \n  rescaled = (vec - min_vec)/(max_vec - min_vec)\n  \n  return rescaled\n\n\nimport pdb\npdb.run(\"rescale_vector(my_vec)\")\n\n\n\n\n\n\n\nCheck In\n\n\n\nTry the above debugging approach.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "05-function_writing.html#general-debugging-strategies",
    "href": "05-function_writing.html#general-debugging-strategies",
    "title": "6  Writing Custom Functions",
    "section": "6.6 General Debugging Strategies",
    "text": "6.6 General Debugging Strategies\n\nDebugging: Being the detective in a crime movie where you are also the murderer.\n\nThe overall process of addressing a bug is:\n\nRealize that you have a bug\nGoogle! Generally Googling the error + the programming language + any packages you think are causing the issue is a good strategy.\nMake the error repeatable: This makes it easier to figure out what the error is, faster to re-try to see if you fixed it, and easier to ask for help. Unit tests are perfect for this.\nFigure out where it is. Print statements and debuggers help you dig into the function to find the problem area.\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation.\n\n\n6.6.1 Rubber Duck debugging\nHave you ever called a friend or teacher over for help with an issue, only to find that by explaining it to them, you solved it yourself?\nTalking through your code out loud is an extremely effective way to spot problems. In programming, we call this Rubber Duck Debugging, because coders will sometimes keep a small toy like a rubber duck by their computer, and talk to it while they are alone.\n\n\n\n\n\n\nJust our opinion...\n\n\n\n\n\n\nThis is the original RickRoll. Yes, really.\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nA more thorough explanation of rubber duck debugging can be found at gitduck.com.\n\n\n\n\n6.6.2 Refactoring your code\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solveable.\nWhen we redesign our functions to consist of many smaller functions, this is called refactoring. Consider the following function:\n\ndef rescale_all_variables(df):\n  \n  for col in df.columns:\n    \n    min_vec = min(df[col])\n    max_vec = max(df[col])\n  \n    df[col] = (df[col] - min_vec)/(max_vec - min_vec)\n    \n  \n  return df\n\nA much cleaner and easier to read way to use this function would be to use the smaller function rescale_vector inside of rescale_all_variables\n\ndef rescale_all_variables(df):\n  \n  for col in df.columns:\n    df[col] = rescale_vector(df[col])\n  \n  return df\n\nThis not only makes the code more readable to humans, it also helps us track down whether the error is happening in the outside function (rescale_all_variables) or the inside one (rescale_vector)\n\n\n6.6.3 Taking a break!\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s so well-known it has an xkcd comic.\nAt some point, getting up and going for a walk may help!\n\n\n\n\n\n\nPractice Activity\n\n\n\nClick here to answer a few questions about function code.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Writing Custom Functions</span>"
    ]
  },
  {
    "objectID": "06-iteration.html",
    "href": "06-iteration.html",
    "title": "7  Iteration",
    "section": "",
    "text": "7.1 Introduction\nIn this chapter, we will learn strategies for iteration, or performing a repeated task over many values.\nConsider the following task: Suppose I want to get a computer to print out the lyrics to the song 99 Bottles of Beer on the Wall.\nNow, we could certainly simply type up all the lyrics ourselves:\nprint(\n  \"99 Bottles of beer on the wall, 99 Bottles of beer. Take one down, pass it around, 98 Bottles of beer on the wall. 98 Bottles of beer on the wall, 98 Bottles of beer. Take one down, pass it around, 97 Bottles of beer on the wall.\" #... etc etc etc\n)\n\n99 Bottles of beer on the wall, 99 Bottles of beer. Take one down, pass it around, 98 Bottles of beer on the wall. 98 Bottles of beer on the wall, 98 Bottles of beer. Take one down, pass it around, 97 Bottles of beer on the wall.\nThat sounds like… not much fun to do.\nBut we notice that in the song, there is a ton of repetition! Every verse is almost identical, except for the number of bottles of beer.\nThis is a great time for some iteration.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "06-iteration.html#introduction",
    "href": "06-iteration.html#introduction",
    "title": "7  Iteration",
    "section": "",
    "text": "Just our opinion...\n\n\n\nFortunately, we aren’t going to try to sing this version: https://www.youtube.com/watch?v=R30DnFfVtUw",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "06-iteration.html#for-loops",
    "href": "06-iteration.html#for-loops",
    "title": "7  Iteration",
    "section": "7.2 For loops",
    "text": "7.2 For loops\nThe most basic approach to iteration is a simple for loop. At each step of the loop the value of our placeholder, i, changes to the next step in the provided list, range(100,97,-1).\n\nfor i in range(100,97,-1):\n  print(str(i) + \" bottles of beer on the wall\")\n  print(str(i) + \" bottles of beer\")\n  print(\" take one down, pass it around,\")\n  print(str(i-1) + \" bottles of beer on the wall\")\n\n100 bottles of beer on the wall\n100 bottles of beer\n take one down, pass it around,\n99 bottles of beer on the wall\n99 bottles of beer on the wall\n99 bottles of beer\n take one down, pass it around,\n98 bottles of beer on the wall\n98 bottles of beer on the wall\n98 bottles of beer\n take one down, pass it around,\n97 bottles of beer on the wall\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nAfter last chapter, hopefully we immediately see that this is a great opportunity to write a function, to make our code clearer and avoid repetition.\nWrite a function called sing_verse() to replace the body of the for loop.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\n\ndef sing_verse(num):\n  song = str(num) + \" bottles of beer on the wall \\n\" + str(num) + \" bottles of beer \\n\" + \" take one down, pass it around, \\n\" + str(num-1) + \" bottles of beer on the wall \\n\"\n  \n  return song\n\n\n\nNotice that in this function, instead of print-ing out the lines of the song, we return the an object consisting as one long string.\nOften, when running a for loop, you want to end the loop process with a single object. To do this, we start with an empty object, and then add to it at each loop.\n\nsong = \"\"\n\nfor i in range(100,97,-1):\n  song = song + sing_verse(i)\n  \nprint(song)\n\n100 bottles of beer on the wall \n100 bottles of beer \n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of beer on the wall \n99 bottles of beer \n take one down, pass it around, \n98 bottles of beer on the wall \n98 bottles of beer on the wall \n98 bottles of beer \n take one down, pass it around, \n97 bottles of beer on the wall \n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nModify the code above so that instead of creating one long string, we create a list, where each element is one of the verses of the song.\n\n\n\n7.2.1 Vectorized functions\nThe function that we wrote above can be described as not vectorized. What we mean by that it is designed to only take one value, num. If we instead try to input a list or vector of numbers, we get an error:\n\nsing_verse([100,99,98])\n\nTypeError: unsupported operand type(s) for -: 'list' and 'int'\n\n\nThis is why, in order to get results for a list of number, we needed to iterate.\nHowever, plenty of functions are designed to work well for single numbers or lists and vectors. For example:\n\na_num = 5\na_vec = [1,3,5,7]\n\nnp.sqrt(a_num)\nnp.sqrt(a_vec)\n\narray([1.        , 1.73205081, 2.23606798, 2.64575131])\n\n\nWhen we want to perform a function over many values - whether it’s one we wrote or not - we first need to ask ourselves if the function is vectorized or not. Using a for loop over a vectorized function is unnecessarily complicated and computationally slow!\n\nresult = []\nfor i in a_vec:\n  result = result + [np.sqrt(i)]\n  \nresult\n\n[1.0, 1.7320508075688772, 2.23606797749979, 2.6457513110645907]\n\n\n\n\n7.2.2 Vectorizing and booleans\nA common reason why a custom function is written in an unvectorized way is that it makes use of if statements. For example, consider the task of taking the square root of only the positive numbers in a list.\nHere is an approach that does not work:\n\na_vec = np.array([-2, 1, -3, -9, 7])\n\nif a_vec &gt; 0:\n  a_vec = np.sqrt(a_vec)\n\na_vec\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nThe statement if a_vec &gt; 0 makes no sense for a vector! The if statement needs either a single True or a single False to determine if the subsequent code will be run - but a_vec &gt; 0 returns a list of five booleans.\nInstead, we would need to iterate over the values:\n\na_vec = np.array([-2, 1, -3, -9, 7])\n\nfor val in a_vec:\n  if val &gt; 0:\n    val = np.sqrt(val)\n  print(val)\n\n-2\n1.0\n-3\n-9\n2.6457513110645907\n\n\nHowever, there is a nicer approach to this variety of problem, which is to use boolean masking:\n\nis_pos = a_vec &gt; 0\n\na_vec[is_pos] = np.sqrt(a_vec[is_pos])\n\na_vec\n\narray([-2,  1, -3, -9,  2])\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWrite two functions:\n\nsqrt_pos_unvec() takes in a single value as an argument, and returns the square root if the value is positive. Then, write a for loop that uses this function to construct a new vector where the positive values are square rooted.\nsqrt_pos_vec() takes in a vector of values, and returns a vector with the positive values square rooted. Do not use a for loop inside your function.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "06-iteration.html#iterable-functions",
    "href": "06-iteration.html#iterable-functions",
    "title": "7  Iteration",
    "section": "7.3 Iterable functions",
    "text": "7.3 Iterable functions\nAlthough for loops are a clear and basic procedure, it can become very tedious to use them frequently. This is especially true if you want to save the results of the iteration as a new object.\nHowever, it will not be possible or convenient to write every function in a vectorized way.\nInstead, we can use iterable functions, which perform the same iteration as a for loop in shorter and more elegant code.\n\n7.3.1 map()\nThe map() function requires the same information as a for loop: what values we want to iterate over, and what we want to do with each value.\n\nsong = map(sing_verse, range(100, 97, -1))\nsong = list(song)\nprint(\"\".join(song))\n\n100 bottles of beer on the wall \n100 bottles of beer \n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of beer on the wall \n99 bottles of beer \n take one down, pass it around, \n98 bottles of beer on the wall \n98 bottles of beer on the wall \n98 bottles of beer \n take one down, pass it around, \n97 bottles of beer on the wall \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the output of the map() function is a special object structure, of the type “map object”. We automatically convert this to a list with the list() function.\nThen, we make use of the join() string method to turn the list into one long string. Finally, we print() our final string out so that it looks nice.\n\n\n\n7.3.1.1 Double mapping\nSometimes, we want to loop through multiple sets of values at once. The map() function has the ability to take as many iterables, or lists of values, as you want.\nSuppose we want to change our sing_verse() function so that it has two arguments, the number of bottles and the type of drink.\n\ndef sing_verse_2(num, drink):\n  song = str(num) + \" bottles of \" + drink + \" on the wall \\n\"\n  song = song + str(num) + \" bottles of \" + drink + \"\\n\" \n  song = song + \" take one down, pass it around, \\n\"\n  song = song + str(num-1) + \" bottles of \" + drink + \" on the wall \\n\"\n  \n  return song\n\nNow, we use map() to switch the number and the drink at each iteration:\n\nnums = range(100, 97, -1)\ndrinks = [\"beer\", \"milk\", \"lemonade\"]\nsong = map(sing_verse_2, nums, drinks)\nprint(\"\".join(list(song)))\n\n100 bottles of beer on the wall \n100 bottles of beer\n take one down, pass it around, \n99 bottles of beer on the wall \n99 bottles of milk on the wall \n99 bottles of milk\n take one down, pass it around, \n98 bottles of milk on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWrite a sing_verse_3() function that also lets us change the container (e.g. bottle, can, …) at each step of the loop.\nUse map() to sing a few verses.\nWhat happens if you supply three different drinks, but only two different types of containers?",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "06-iteration.html#lambda-functions",
    "href": "06-iteration.html#lambda-functions",
    "title": "7  Iteration",
    "section": "7.4 Lambda functions",
    "text": "7.4 Lambda functions\nWhat would you do if you still wanted to count down the number of bottles, but you wanted them all to be lemonade?\nIn this case, we want one of the arguments of our function to be iterated over many values, and the other one to stay consistent.\nOne rather inelegant way we could accomplish this is with a new function:\n\ndef sing_verse_lemonade(num):\n  return sing_verse_2(num, \"lemonade\")\n\nsong = map(sing_verse_lemonade, nums)\nprint(\"\".join(list(song)))\n\n100 bottles of lemonade on the wall \n100 bottles of lemonade\n take one down, pass it around, \n99 bottles of lemonade on the wall \n99 bottles of lemonade on the wall \n99 bottles of lemonade\n take one down, pass it around, \n98 bottles of lemonade on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\nThis is a lot of extra lines of code, though, for a task that should be straightforward - and we’ll probably never use sing_verse_lemonade() again, so it’s a bit of a waste to create it.\nInstead, we will use what is called a lambda function, which is like making a new sing_verse_lemonade wrapper function for temporary use:\n\nsong = map(lambda i: sing_verse_2(i, \"lemonade\"), nums)\nprint(\"\".join(list(song)))\n\n100 bottles of lemonade on the wall \n100 bottles of lemonade\n take one down, pass it around, \n99 bottles of lemonade on the wall \n99 bottles of lemonade on the wall \n99 bottles of lemonade\n take one down, pass it around, \n98 bottles of lemonade on the wall \n98 bottles of lemonade on the wall \n98 bottles of lemonade\n take one down, pass it around, \n97 bottles of lemonade on the wall \n\n\n\nThe code lambda i: sing_verse_2(i, \"lemonade\") made a new anonymous function - sometimes called a headless function - that takes in one argument, i, and sends that argument into sing_verse_2.\n\n\n\n\n\n\nCheck In\n\n\n\nUse a lambda function with sing_verse_3() to sing a few verses about milk in glasses.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "06-iteration.html#iterating-on-datasets",
    "href": "06-iteration.html#iterating-on-datasets",
    "title": "7  Iteration",
    "section": "7.5 Iterating on datasets",
    "text": "7.5 Iterating on datasets\nThis task of repeating a calculation with many inputs has a natural application area: datasets!\nIt is extremely common that we want to perform some calculation involving many variables of the dataset, and we want to repeat that same calculation over the values in each row.\nFor this situation, we use an iterable function that is very similar to map(): the .apply() method from pandas.\nAt its core, the .apply() method is meant for repeating a calculation over columns:\n\ndat = pd.DataFrame({\"x\": [99, 50, 2], \"y\": [1, 2, 3]})\n\ndat.apply(np.sqrt)\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n9.949874\n1.000000\n\n\n1\n7.071068\n1.414214\n\n\n2\n1.414214\n1.732051\n\n\n\n\n\n\n\nIn this chapter, though, we are more interested in using it to repeat a function, using each row as input:\n\ndat.apply(np.sum, axis=1)\n\n0    100\n1     52\n2      5\ndtype: int64\n\n\nSuppose we have a pandas dataframe consisting of all the numbers, drinks, and containers that we are interested in singing about:\n\ndat = pd.DataFrame({\"num\": [99, 50, 2], \"drink\": [\"beer\", \"soda\", \"Capri Sun\"], \"container\": [\"bottles\", \"cans\", \"pouches\"]})\n\ndat\n\n\n\n\n\n\n\n\nnum\ndrink\ncontainer\n\n\n\n\n0\n99\nbeer\nbottles\n\n\n1\n50\nsoda\ncans\n\n\n2\n2\nCapri Sun\npouches\n\n\n\n\n\n\n\nOur goal is to apply the sing_verse_3 function over all these combinations of values.\nUnfortunately, this doesn’t happen automatically:\n\ndat.apply(sing_verse_3, axis=1)\n\nTypeError: sing_verse_3() missing 2 required positional arguments: 'drink' and 'container'\n\n\nThis is because .apply doesn’t “know” which columns below with which arguments of the sing_verse_3 function. We’ll need to use a lambda function to help it out:\n\ndat.apply(lambda x: sing_verse_3(x['num'], x['drink'], x['container']), axis=1)\n\n0    99 bottles of beer on the wall \\n99 bottles of...\n1    50 cans of soda on the wall \\n50 cans of soda\\...\n2    2 pouches of Capri Sun on the wall \\n2 pouches...\ndtype: object\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nClick here to open the practice activity.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "07-webscraping.html",
    "href": "07-webscraping.html",
    "title": "8  Webscraping",
    "section": "",
    "text": "8.1 Introduction\nThis document demonstrates the use of the BeautifulSoup library in Python to do webscraping.\nWeb scraping is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation. Some websites don’t like it when automatic scrapers gather their data, while others don’t mind.\nIf you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Sill, it’s a good idea to do some research on your own and make sure that you’re not violating any Terms of Service before you start a large-scale project.\nimport pandas as pd",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "07-webscraping.html#introduction",
    "href": "07-webscraping.html#introduction",
    "title": "8  Webscraping",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the beautifulsoup4 library installed then you will need to run\npip install beautifulsoup4\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "07-webscraping.html#an-alternative-to-web-scraping-apis",
    "href": "07-webscraping.html#an-alternative-to-web-scraping-apis",
    "title": "8  Webscraping",
    "section": "8.2 An Alternative to Web Scraping: APIs",
    "text": "8.2 An Alternative to Web Scraping: APIs\nSome website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\nWhen you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\n\n8.2.1 The Tasty API\n\n\n\n\n\n\nCheck In\n\n\n\nTasty.co is a website and app that offers food recipes. They have made these recipes available through a REST API. You do need authentication, though.\nSpecifically, you will need to sign up for free account. You will then be provided with an API key that will need to be supplied with every request you make. This is used to track and limit usage.\n\n\nFor our example, we’ll us the recipes/list endpoint.\nMake sure you are logged into the account you are created, and select the recipes/list endpoint from the menu at the left of the documentation here. We’ll use the requests Python library to make use of this.\nLet’s search for recipes containing “daikon” (an Asian radish). Which one is the cheapest per portion?\n\nimport requests\n\nurl = \"https://tasty.p.rapidapi.com/recipes/list\"\n\nquerystring = {\"from\":\"0\",\"size\":\"20\",\"q\":\"daikon\"}\n\nheaders = {\n    \"X-RapidAPI-Key\": &lt;your key here&gt;,\n    \"X-RapidAPI-Host\": \"tasty.p.rapidapi.com\"\n}\n\nresponse = requests.get(url, headers=headers, params=querystring)\n\nprint(response.json())\n\nNotice that there are two elements to this object we got back, and we only really want the results piece.\n\ndaikon_recipes = pd.json_normalize(response.json(), \"results\")\ndaikon_recipes\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith the 2-3 people around you, look up what the JSON format is and what it looks like. Then look up the json_normalize function and discuss the differences between the results of this and the JSON format.\n\n\nThe JSON format is extremely common, but can be somewhat easily worked with because it’s highly structured.\nBefore we move on from APIs it’s again important to note that gathering data from websites often comes with constraints. For example, the Tasty API only returns 20 results by default and only 40 results maximum, even if you specify the size= parameter. So, to gather more than 40 results we might need to use some form of iteration while still respecting the API’s rate limits.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "07-webscraping.html#html-and-web-scraping",
    "href": "07-webscraping.html#html-and-web-scraping",
    "title": "8  Webscraping",
    "section": "8.3 HTML and Web Scraping",
    "text": "8.3 HTML and Web Scraping\nHTML, which stands for “hypertext markup language”, is an XML-like language for specifying the appearance of web pages. Each tag in HTML corresponds to a specific page element. For example:\n\n&lt;img&gt; specifies an image. The path to the image file is specified in the src= attribute.\n&lt;a&gt; specifies a hyperlink. The text enclosed between &lt;a&gt; and &lt;/a&gt; is the text of the link that appears, while the URL is specified in the href= attribute of the tag.\n&lt;table&gt; specifies a table. The rows of the table are specified by &lt;tr&gt; tags nested inside the &lt;table&gt; tag, while the cells in each row are specified by &lt;td&gt; tages nested inside each &lt;tr&gt; tag.\n\nOur goal is not to teach you HTML to make a web page. You will learn just enough HTML to be able to scrape data programmatically from a web page.\n\n8.3.1 Inspecting HTML Source Code\nSuppose we want to scrape faculty information from the Cal Poly Statistics Department directory. Once we have identified a web page that we want to scrape, the next step is to study the HTML source code. All web browsers have a “View Source” or “Page Source” feature that will display the HTML source of a web page.\n\n\n\n\n\n\nCheck In\n\n\n\nVisit the web page above, and view the HTML source of that page. (You may have to search online to figure out how to view the page source in your favorite browser.) Scroll down until you find the HTML code for the table containing information about the name, office, phone, e-mail, and office hours of the faculty members.\n\n\nNotice how difficult it can be to find a page element in the HTML source. Many browsers allow you to right-click on a page element and jump to the part of the HTML source corresponding to that element.\n\n\n8.3.2 Scraping an HTML Table with pandas\nThe pandas command read_html can be used to scrape information from an HTML table on a webpage.\nWe can call read_html on the URL.\n\npd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\n\nHowever, this scrapes all the tables on the webpage, not just the one we want. As we will see with Beautiful Soup, we can narrow the search by specifying the table attributes.\n\npd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", attrs = {'class': 'wikitable sortable', \"style\": \"text-align:center\"})\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhere did the attrs details in the code above come from? With the 2-3 people around you, inspect the HTML source code for this page and see if you can identify this.\n\n\nThis still returns 3 tables. The table that we want is the first one!\n\ndf_cities2 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", attrs = {'class': 'wikitable sortable', \"style\": \"text-align:center\"})[0]\ndf_cities2\n\nThis is a good first pass at scraping information from a webpage and it returns it to us well in the form of a data frame. This works well for HTML tables. Unfortunately, you often want to scrape information from a webpage that isn’t conveniently stored in an HTML table, in which case read_html won’t work. (It only searches for &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, and &lt;td&gt; tags, but there are many other HTML tags.)\n\n\n8.3.3 Web Scraping Using BeautifulSoup\nBeautifulSoup is a Python library that makes it easy to navigate an HTML document. Like with XML, we can query tags by name or attribute, and we can narrow our search to the ancestors and descendants of specific tags. Also, many web sites have malformed HTML, which BeautifulSoup is able to handle gracefully.\nFirst we issue an HTTP request to the URL to get the HTML source code.\n\nimport requests\nresponse = requests.get(\"https://statistics.calpoly.edu/content/directory\")\n\nThe HTML source is stored in the .content attribute of the response object. We pass this HTML source into BeautifulSoup to obtain a tree-like representation of the HTML document.\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n\n8.3.3.1 Find Elements by ID\nIn an HTML web page, every element can have an id attribute assigned. As the name already suggests, that id attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID.\n\n\n\n\n\n\nCheck In\n\n\n\nVisit the web page above, and view the HTML source of that page. Locate an element of interest and identify its id attribute. Then run the following to try extracting it\n\nresults = soup.find(id=\"your id here\")\nprint(results.prettify())\n\n\n\n\n\n8.3.3.2 Find Elements by HTML Class Name\nYou will often see that every similar piece of a web page is wrapped in the same HTML element, like &lt;div&gt; with a particular class. We’re able to extract all of the parts of the HTML of interest to us, also, by specifying a containing HTML element and the specific classes we want with code like the following:\n\nresults.find_all(\"element name\", class_=\"class name\")\n\n\n\n8.3.3.3 Find Elements by Class Name and Text Content\nIt’s often the case that we only want pieces of a web page’s content that match certain criteria. We can refine our search using the string option of .find_all().\n\nrefined_results = results.find_all(\"class name\", string=\"search text\")\n\nThis code finds all class name elements where the contained string matches \"search text\" exactly. We’ll discuss later how to be more robust in the specification of string matches like this!\n\n\n8.3.3.4 Find Elements by Tag\nNow we can search for tags within this HTML document, using functions like .find_all(). For example, we can find all tables on this page.\n\ntables = soup.find_all(\"table\")\nlen(tables)\n\nAs a visual inspection of the web page would confirm, there are 3 tables on the page (chair and staff, faculty, emeritus faculty), and we are interested in the second one (for faculty).\n\ntable = tables[1]\ntable\n\nThere is one faculty member per row (&lt;tr&gt;), except for the first row, which is the header. We iterate over all rows except for the first, extracting the information about each faculty to append to rows, which we will eventually turn into a DataFrame. As you read the code below, refer to the HTML source above, so that you understand what each line is doing.\n\n\n\n\n\n\nNote\n\n\n\nYou are encouraged to add print() statements inside the for loop to check your understanding of each line of code.\n\n\n\n# initialize an empty list\nrows = []\n\n# iterate over all rows in the faculty table\nfor faculty in table.find_all(\"tr\")[1:]:\n\n    # Get all the cells (&lt;td&gt;) in the row.\n    cells = faculty.find_all(\"td\")\n\n    # The information we need is the text between tags.\n\n    # Find the the name of the faculty in cell[0]\n    # which for most faculty is contained in the &lt;strong&gt; tag\n    name_tag = cells[0].find(\"strong\") or cells[0]\n    name = name_tag.text\n\n    # Find the office of the faculty in cell[1]\n    # which for most faculty is contained in the &lt;a&gt; tag\n    link = cells[1].find(\"a\") or cells[1]\n    office = link.text\n\n    # Find the email of the faculty in cell[3]\n    # which for most faculty is contained in the &lt;a&gt; tag\n    email_tag = cells[3].find(\"a\") or cells[3]\n    email = email_tag.text\n\n    # Append this data.\n    rows.append({\n        \"name\": name,\n        \"office\": office,\n        \"email\": email\n    })\n\n\n\n\n\n\n\nCheck In\n\n\n\nWith the 2-3 people around you do the following referring to the code above:\n\nWhat does cells look like for the first faculty (i.e. first iteration of the loop)?\nWhy do we need to call .find(\"strong\") within cells[0] (i.e. a &lt;td&gt; tag) to get the name of the faculty member?\nCould you extract the phone information for each faculty as well? If so, then add the code necessary to do this.\n\n\n\nIn the code above, observe that .find_all() returns a list with all matching tags, while .find() returns only the first matching tag. If no matching tags are found, then .find_all() will return an empty list [], while .find() will return None.\nFinally, we turn rows into a DataFrame.\n\npd.DataFrame(rows)\n\nNow this data is ready for further processing.\n\n\n\n\n\n\nPractice Activity\n\n\n\nPractice Activity notebook here.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Webscraping</span>"
    ]
  },
  {
    "objectID": "08-regex.html",
    "href": "08-regex.html",
    "title": "9  Strings and Regular Expressions",
    "section": "",
    "text": "9.1 Introduction\nThis chapter goes into more detail on dealing with string objects, using methods and regular expressions.\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Strings and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "08-regex.html#introduction",
    "href": "08-regex.html#introduction",
    "title": "9  Strings and Regular Expressions",
    "section": "",
    "text": "Note\n\n\n\nMany of the functions in this chapter are from a library called re. This is built into base python, so you do not need to install it!",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Strings and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "08-regex.html#string-methods",
    "href": "08-regex.html#string-methods",
    "title": "9  Strings and Regular Expressions",
    "section": "9.2 String Methods",
    "text": "9.2 String Methods\nWe have already seen many examples so far of objects that have the string data type. These might be referred to as str or character or object, depending on the library we are using to check type.\nString objects can be combined with the + operator:\n\nstring_1 = \"Moses supposes his toeses are roses,\"\nstring_2 = \"Moses supposes erroneously.\"\n\nstring_1 + \", but \" + string_2\n\n'Moses supposes his toeses are roses,, but Moses supposes erroneously.'\n\n\nHowever, they cannot be subtracted, divided, or multiplied!\n\nstring_1 - \"Moses\"\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\nMuch as we can subset lists and similar objects, we can also subset strings according to their characters:\n\nstring_1[0:5]\nstring_1[5:]\nstring_2[-12:-1]\n\n'erroneously'\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA list of string objects is not the same as a standalone string object! The length of a list is the number of elements it has. The length of a string is the number of characters it has.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat do you think the following code will return?\n\nme = \"Kelly\"\nme_list = [\"Kelly\"]\n\nlen(me)\nlen(me_list)\nlen(me_list[0])\n\n\n\n\n9.2.1 Cleaning up a string\nWhat other changes might we commonly want to make to string objects? Many of the tasks we might need to do are available in python as string methods. Recall that a method is a special function that can work only on a certain object type or structure.\nFor example, I might want to turn my whole string into lowercase letters, perhaps for simplicity.\n\nstring_1.lower()\n\n'moses supposes his toeses are roses,'\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat other string methods can you find for changing the case of the string?\n\n\nI also might want to get rid of any extra white space that is unnecessary:\n\nstring_3 = \"          doot de doo de doo     \"\nstring_3.strip()\n\n'doot de doo de doo'\n\n\n\n\n9.2.2 Searching and replacing\nPerhaps we want to make changes to the contents of a string. First, we might check to see if the word we want to change is truly present in the string:\n\nstring_4 = \"My name is Bond, James Bond.\"\nstring_4.find(\"Bond\")\n\n11\n\n\nNotice that this gives back the character index where the desired word starts.\nIf the pattern is not found, we get back a value of -1.\n\nstring_4.find(\"007\")\n\n-1\n\n\nNext, we can replace the word with a different string:\n\nstring_4.replace(\"Bond\", \"Earl Jones\")\n\n'My name is Earl Jones, James Earl Jones.'\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs with any object, nothing changes permanently until we reassign the object. The .replace() method did not alter the object string_4:\n\nstring_4\n\n'My name is Bond, James Bond.'\n\n\n\n\nSometimes, when we want to build up a particularly complex string, or repeat a string alteration with different values, it is more convenient to put a “placeholder” in the string using curly brackets {} and fill the space in later.\n\nstring_4 = \"My name is {}, James {}.\"\nstring_4.format(\"Bond\", \"Bond\")\nstring_4.format(\"Franco\", \"Franco\")\n\n'My name is Franco, James Franco.'\n\n\nThe {} placeholder combined with the .format() method also allows for named placeholders, which is handy when you want to repeat a value:\n\nstring_4 = \"My name is {lastname}, {firstname} {lastname}.\"\nstring_4.format(firstname = \"James\", lastname = \"Baldwin\")\n\n'My name is Baldwin, James Baldwin.'\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nRecall from the Chapter 7 check-ins that we wrote a function to sing the “99 bottles of beer on the wall” song with a new drink and container. Rewrite this function using the .format() trick.\n\ndef sing_verse_3(num, drink, container):\n  song = str(num) + \" \" + container +\" of \" + drink + \" on the wall \\n\"\n  song = song + str(num) + \" \"+ container +\" of \" + drink + \"\\n\" \n  song = song + \" take one down, pass it around, \\n\"\n  song = song + str(num-1) + container + \" of \" + drink + \" on the wall \\n\"\n  \n  return song\n\n\n\n\n\n9.2.3 Splitting and joining\nSometimes, it may be convenient to convert our strings into lists of strings, or back into one single string object.\nTo turn a long string into a list, we split the string:\n\nfish_string = \"One fish, two fish, red fish, blue fish.\"\nfish_list = fish_string.split(\", \")\nfish_list\n\n['One fish', 'two fish', 'red fish', 'blue fish.']\n\n\nNotice that our argument to the .split() was the pattern we wanted to split on - in this case, every time there was a comma and a space. The characters used for splitting are removed, and each remaining section becomes an object in the list.\nNow that we have a list, if we want to use string methods, we can’t apply them directly to the list object:\n\nfish_list.replace(\"fish\", \"moose\")\n\nAttributeError: 'list' object has no attribute 'replace'\n\n\nInstead, we’ll need to iterate over the string objects in the list.\n\nnew_list = list(map(lambda x: x.replace(\"fish\", \"moose\"), fish_list))\n\nnew_list\n\n['One moose', 'two moose', 'red moose', 'blue moose.']\n\n\nNow, if we want to recombine this list into one string, we will join all its elements together. The .join() method is a bit of a peculiar construct: we call the method on a string that we want to put between each list element when we bring them together.\n\n\" and \".join(new_list)\n\", \".join(new_list)\n\n'One moose, two moose, red moose, blue moose.'",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Strings and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "08-regex.html#regular-expressions",
    "href": "08-regex.html#regular-expressions",
    "title": "9  Strings and Regular Expressions",
    "section": "9.3 Regular Expressions",
    "text": "9.3 Regular Expressions\nIn the .replace() method above, we supplied the exact pattern that we wanted to replace in the string.\nBut what if we wanted to find or replace all approximate matches? For example, if we have the string\n\nmoses_string = \"Moses supposes his toeses are roses, but Moses supposes erroneously.  Moses he knowses his toeses aren't roses, as Moses supposes his toeses to be.\"\n\nwe might be interested in finding all the rhyming words in this string, i.e., all words ending in “-oses” or “-oeses”.\nTo perform this kind of fuzzy matching, we need to use regular expressions.\nA regular expression is a special type of string, that can be interpreted by particular functions as a series of commands for fuzzy matching.\nFor example, instead of using the .findall() string method, we’ll use the very similar function re.findall() to search a string using regular expressions:\n\nimport re\nre.findall(r\"[Mr]oses\", moses_string)\n\n['Moses', 'roses', 'Moses', 'Moses', 'roses', 'Moses']\n\n\nIn the above code, the r in front of the regular expression \"[Mr]oses\" let the function know a regular expression was being provided. This isn’t always needed, but it’s a good habit to get into, to make it clear when you supplying an ordinary string (a.k.a. string literal) or a regex.\nThe [Mr] part of the regex told the re.findall() function to match any “M” or any “r” - so we were able to find instances of both “Moses” and “roses”!\nRegular expressions can be both very powerful and very frustrating. With the right expression, you can match any complicated string bit you might want to search for in data! But putting the expressions together requires learning the special characters that lead to fuzzy matching, such as knowing that something in brackets, like [Mr] means “match either of these characters”.\n\n\n\n\n\n\n\nCheck In\n\n\n\nApart from using regular expressions instead of exact strings, do you notice another major difference between the .find method versus the re.findall() function? When might it be better to use one, and when might it be better to use the other?\n\n\n\n9.3.1 Shortcuts\nIn our Moses example, we wanted to match all rhyming words. Rather than go through the whole string to figure out possible letters that come before “-oses”, we can instead use the \\w regular expression shortcut to say “match any character that might be found in a word” - i.e., not punctuation or whitespace.\n\nre.findall(r\"\\woses\", moses_string)\n\n['Moses',\n 'poses',\n 'roses',\n 'Moses',\n 'poses',\n 'Moses',\n 'roses',\n 'Moses',\n 'poses']\n\n\nOther handy shortcuts include:\n\n\\b: “boundary” between word and non-word, such as punctuation or whitespace\n\\s : “space” matches a single whitespace character\n\\d : “digit” matches any single number 0-9\n^ : matches the start of a string\n$ : matches the end of a string\n. : matches any character at all, except a new line (\\n)\n\n\n\n\n\n\n\nCheck In\n\n\n\nConsider the following string:\n\nnursery_rhyme = \"Peter Piper picked a peck of pickled peppers, A peck of pickled peppers Peter Piper picked; If Peter Piper picked a peck of pickled peppers, Where’s the peck of pickled peppers Peter Piper picked?\"\n\n\nUse re.split() and a regular expression to turn this into a list where each element is one word.\nHow many words start with the letter p?\n\n\n\n\n\n9.3.2 Repetition\nWe still haven’t quite completed our goal of finding the rhyming words, because we were only able to match the string “poses” instead of the full word “supposes”.\nAn important set of special symbols in regular expressions are those that control how many of a particular character to look for. For example,\n\nnames = \"Key, Kely, Kelly, Kellly, Kelllly\"\nre.findall(r\"Kel+y\", names)\n\n['Kely', 'Kelly', 'Kellly', 'Kelllly']\n\n\nIn this regex, the + character means “match the previous thing at least one time”. Since the thing before the + is the letter l, we match any string that starts with “Ke”, then has one or more l’s, then has a y.\nOther regex symbols for repetition are:\n\n* : Match 0 or more of the previous character\n? : Match 0 or one of the previous character\n{2} : Match exactly two of the previous character\n{1,3}: Match 1 to 3 repeats of the previous character\n\n\nre.findall(r\"Kel*y\", names)\nre.findall(r\"Kel?y\", names)\nre.findall(r\"Kel{2}y\", names)\nre.findall(r\"Kel{2,3}y\", names)\n\n['Kelly', 'Kellly']\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nReturn to the Peter Piper example in the previous check-in. Instead of finding how many words start with a p, can you extract the words themselves?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nUse repetition to find all the rhyming words in the Moses rhyme.\nHint: Don’t forget “toeses”! How can you make the “e” optional, to match “-oses” or “-oeses” or “-owses”?\n\n\n\n\n9.3.3 Escaping special characters\nWith characters like * or ? or \\ being given special roles in a regular expression, you might wonder how to actually find these exact symbols in a string?\nThe answer is that we escape the character, by putting a \\ in front of it:\n\nstring_5 = \"Are you *really* happy?\"\nre.findall(r\"\\*\\w+\\*\", string_5) \n\n['*really*']\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nThe following will read in the text of Bob Dylan’s song Blowing in the Wind, which you can download here.\n\nf = open('wind_lyrics.txt', 'r')\nwind = f.read()\nf.close()\n\n\nHow many questions does Dylan ask in the song?\nCan you extract these questions?\n\nHint: consider splitting the lyrics into a list of sentences, then finding which sentences are questions.\n\n\n\n\n9.3.4 Look-ahead and look-behind\nLastly, sometimes we want to match a piece of a string based on what comes before it. For example, let’s return one last time to Moses and his toeses. To find all the verbs that Moses does, we want to find words that come after the word Moses:\n\nre.findall(r\"(?&lt;=Moses )\\w+\", moses_string)\n\n['supposes', 'supposes', 'he', 'supposes']\n\n\nThe (?&lt;= ) part of the regex means “don’t actually match these characters, but look for them before our actual pattern”. This is called a look-behind.\nOr, we can use (?=) to do a look-ahead:\n\nre.findall(r\"\\w+(?= Moses )\", moses_string)\n\n['but', 'as']\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nIn Blowin in the Wind, what words follow the phrase “how many”?\n\n\n\n\n9.3.5 Conclusion\n\n\n\n\n\n\nWarning\n\n\n\nThe special symbols and structures for a regular expression are built into a programming language. python uses what are called perl-like regular expressions, which are the most common in modern languages. However, you may encounter other programming languages that use slightly different symbols to do various matching tasks!\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nClick here to practice using regular expressions and string methods to decode a message.\nHint: You will need the re.sub() function for this task!\n\n\n\n\n\n\n\n\nJust our opinion...\n\n\n\nIf you, like me, think Regular Expressions are super fun, try out the RegEx Crossword Puzzle.",
    "crumbs": [
      "Data Science with Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Strings and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html",
    "href": "09-modeling_intro.html",
    "title": "10  Introduction to Predictive Modeling",
    "section": "",
    "text": "10.1 Introduction\nIn this chapter, you will be introduced to the basic structure and philosophy of the world of predictive modeling, or as it is often known, machine learning.\nMachine learning in python is usually performed using the SciKit-Learn package. This library not only provides convenient functions for fitting predictive learning models, it also enforces a strict structure of workflow that will help you make responsible choices in your machine learning decisions.\nTo help introduce the mindset and procedure of predictive modeling, we begin with a metaphor:\nHow does this story relate at all to Machine Learning? Read on to find out…",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html#introduction",
    "href": "09-modeling_intro.html#introduction",
    "title": "10  Introduction to Predictive Modeling",
    "section": "",
    "text": "You are the wealthy Baroness Von Machlearn, and you have decided to commission a painting of yourself to hang in your mansion for posterity. Of course, to fully capture your beauty, this portrait needs to be 100 feet tall at least - so you’ll only be able to commission one final painting. But who shall have the honor of immortalizing you?\n\n\n\n\nFig 1. The Baroness herself.\n\n\n\nYou being sneakily exploring your friends’ houses every week at Baroness Card Club to see the portraits they have commissioned for themselves. You write down the names of these painters, who you now know are capable of decent quality portrature. (After all, the painter cannot be blamed for the hideous dress that Baroness Artificia was wearing!)\n\n\n\n\nFig 2. They didn’t even notice she was gone!\n\n\n\nThen, you send each of these portrait painters a photograph of yourself and pay them to recreate it as a miniature portrait. You bring these portraits to your weekly Card Game, and see which one most impresses your Baroness friends.\nSurely, whichever painter’s minature interpretation impresses them the most, that is the painter you should hire!\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Painter 1's submission\n\n\n\n\n\n\n\n\n\n\n\n(b) Painter 2's submission\n\n\n\n\n\n\n\n\n\n\n\n(c) Painter 3's submission\n\n\n\n\n\n\n\nFigure 10.1: Fig 3. Three portrait submissions. Which will be your legacy?\n\n\n\n\nAt Baroness Card Club, your friends are blown away by Painter 1’s majestic portrait. You hire them at once to paint you in your full glory and secure your regal legacy.\n\n\n\n\nFig 4. The Baroness in her full glory.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html#elements-of-a-machine-learning-process",
    "href": "09-modeling_intro.html#elements-of-a-machine-learning-process",
    "title": "10  Introduction to Predictive Modeling",
    "section": "10.2 Elements of a machine learning process",
    "text": "10.2 Elements of a machine learning process\n\n10.2.1 Predictors and Targets\nIn a predictive modeling setting, there is one target variable that we are hoping to be able to predict in the future.\nThis target variable could take many forms; for example it could be:\n\nThe price that a particular house will sell for.\nThe profits of a company next year.\nWhether or not a person will click a targeted ad on a website.\n\nThe goal is to come up with a strategy for how we will use the data we can observe to make a good guess about the unknown value of the target variable.\nThe next question, then, is: what data can we observe? The information we choose to use in our prediction strategy is called the predictors. For the above three target variables, some predictors might be:\n\nThe size of the house in square feet, the neighborhood it is located in, and the number of bedrooms it has.\nThe company’s profits last year.\nThe person’s age, the person’s previous search terms, the image used for the ad, and the website it is hosted on.\n\nUltimately, every machine learning model - even very complex ones - are simply procedures that take in some predictors and give back a predicted value for the target.\n\n\n\n\n\n\nExample\n\n\n\nFor Baroness Von Machlearn, the desired target was a beautiful portrait. Her “predictors” were the elements of a portrait: what dress was she wearing, what background was shown, how she styled her hair, etc.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIn machine learning, it is common to refer to predictors or features for the input and target or target variable for the output.\nIn computer science, you will sometimes hear these simply called input and output.\nIn statistics, we sometimes say covariates or independent variables for the input, and response variable or dependent variable for the output.\nThis book will use all the terms interchangeably, but we will mostly stick to “predictors” and “target”.\n\n\n\n\n10.2.2 Model Specifications\nNow, once we have identified our target variable, and decided on some predictors we will use, we need to come up with a process for making predictions.\nFor example, consider the problem of predicting the price a newly listed house will sell for, based on its size, neighborhood, and number of bedrooms. Here are a few different prediction strategies:\n\nWe will find ten houses in the same neighborhood with approximately the same size and number of bedrooms. We will look at the most recent prices these ten houses sold for, and take the average of those. This is our predicted price for the new house.\nWe will make an equation\n\n\\[a * \\text{size} + b * \\text{neighborhood 1}  + c* \\text{neighborhood 2} + d*\\text{num bedrooms} = \\text{price}\\] We will find out what choices of \\(a,b,c,d\\) lead to the best estimates for recently sold houses. Then, we will use those to predict the new house’s price.\n\nWe will define a few categories of houses, such as “large, many bedrooms, neighborhood 1” or “medium, few bedrooms, neighborhood 2”. Then, we will see which category the new house fits into best. We will predict the price of the new house to be the average price of the ones in its category.\n\nEach of these strategies is what we call a model specification. We are specifying the procedure we intend to use to model the way house prices are determined.\n\n\n\n\n\n\nNote\n\n\n\nFor the curious, the examples above roughly correspond to the model specifications:\n\nK-Nearest-Neighbors\nLinear Regression\nDecision Trees\n\nIn this class, you will learn a few of the many different model specifications that exist.\n\n\n\n\n\n\n\n\nExample\n\n\n\nIn our Baroness’s journey to a portrait, she considered many portrait painters. These represent her model specifications: the procedures that will turn her image into a portrait.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html#choosing-a-final-model",
    "href": "09-modeling_intro.html#choosing-a-final-model",
    "title": "10  Introduction to Predictive Modeling",
    "section": "10.3 Choosing a final model",
    "text": "10.3 Choosing a final model\n\n10.3.1 Training data\nNotice something very important about all of the examples of model specifications: They required us to know something about the prices and qualities of other houses, not just the one we wanted to predict. That is, to help us develop our strategy for future prediction, we had to rely on past or known information.\nThis process, by which we use known data to nail down the details of our prediction strategy, is called fitting the models.\nFor the three specifications above, the model fitting step is:\n\nSimply collecting information about house sizes and prices in the same neighborhood.\nDetermining which exact values of \\(a,b,c,d\\) do a good job predicting for known house prices.\nDeciding on categories of houses that seem to be priced similarly.\n\n\n\n\n\n\n\nExample\n\n\n\nTo help figure out which painters were capable of portraits, the Baroness observed portraits in her friend’s homes. The painters had been trained on portraits of other fancy ladies.\n\n\n\n\n10.3.2 Test Data and Metrics\nUltimately, we need to settle on only one procedure to use to come up with our prediction(s) of unknown target values.\nSince our goal is to choose a good strategy for future data, we’d like to see how our fitted models perform on new data. This brand-new data - which was not involved in the model fitting process, but for which we do know the true target values - is called the test data.\n\n\n\n\n\n\nWarning\n\n\n\nWhy might we not want to measure prediction success on the training data?\nConsider the model specification “Find the most similar house in the training data, and predict that price?” If we use this approach to make predictions about the houses in the training data, what will happen?\nIf we want to predict the price of a house in the training data, we look for the most similar house, which is… itself! So we predict the price perfectly!\nThis doesn’t necessarily mean our modeling approach is good: remember, our goal here is to come up with a strategy that will work well for future data.\n\n\n\n\n\n\n\n\nExample\n\n\n\nTo help figure out which painter to ultimately hire, the Baroness sent each painter a photograph of herself. This allowed the painter to create a sample portrait, i.e., “test data”, before they ever saw her in person.\n\n\nOnce we make predictions on the test data, we need to decide on a metrics: a measurement of prediction success of our different models on the test data that will help us choose the “best” one.\nA metric is typically an equation that can be calculated using the test data and the predictions from a fitted model.\nFor example, some good metrics for choosing a model to predict house prices might be Mean Squared Error: We gather houses whose recent sales prices are known, use our fitted model to predict prices, and find the average squared distance between the predicted price and the actual price.\nIn math notation, this looks like:\n\\[ (y_1, ..., y_{100}) = \\text{actual sales prices of 100 houses}\\] \\[ (\\hat{y}_1, ..., \\hat{y}_{100}) = \\text{predicted sales prices of those 100 houses}\\]\n\\[ \\text{MSE} = \\frac{1}{100} \\sum_{i = 1}^{100} (y_i - \\hat{y}_i)^2\\]\nOf our three (or however many) model specifications, which have been fitted with training data, one will have the “best metrics” - the lowest MSE on the test data.\nThis “winner” is our final model: the modeling approach we will use to make predictions on the new house.\n\n\n\n\n\n\nExample\n\n\n\nAt Baroness Card Club, the other ladies gave their opinions on the candidate painters’ mini portraits. Baroness Von Machlearn’s metric was her friends’ opinions, which is how she chose the painter to make the final portrait.\n\n\nOur last - and very important! - step is to fit the final model: That is, to use all the data we have, test and training, to re-train the winning model specification. This is the fitted model we will use on our actual future unknown data.\n\n\n\n\n\n\nExample\n\n\n\nAfter all this effort choosing a painter, the Baroness still needed to sit for her massive portrait!",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html#modeling-with-scikit-learn",
    "href": "09-modeling_intro.html#modeling-with-scikit-learn",
    "title": "10  Introduction to Predictive Modeling",
    "section": "10.4 Modeling with Scikit-learn",
    "text": "10.4 Modeling with Scikit-learn\nNow, let’s walk through a simple example of this predictive model workflow in python with the scikit-learn library.\nFirst, install and import sklearn, as well as our usual suspects:\n\nimport sklearn\nimport pandas as pd\nimport numpy as np\n\nNext, load the example dataset we will use: the “Ames Housing” dataset, which contains information about house sizes and prices in Ames, Iowa.\n\ndat = pd.read_csv(\"https://www.dropbox.com/scl/fi/yf8t1x0uvrln93dzi6xd8/housing_small.csv?rlkey=uen32y937kqarrjra0v6jaez4&dl=1\")\n\n\n10.4.1 Target and Predictors\nFirst, we will establish which variable is our response and which are our predictors. We’ll call these y and X.\n\ny = dat['SalePrice']\nX = dat[['Gr Liv Area', 'Bedroom AbvGr', 'Neighborhood_NAmes', 'Neighborhood_NWAmes']]\n\n\n\n\n\n\n\nCaution\n\n\n\nImportant! Notice that the object y is a Series, containing all the values of the target variable, while X is a Data Frame with three columns.\nWe often name y in lowercase and X in uppercase, to remind ourselves that y is one-dimensional and X is two-dimensional.\nIn general, sklearn functions will expect a one-dimensional target and two-dimensional object for predictors.\n\n\n\n\n10.4.2 Model specifications\nOur next step is to establish which model specifications in sklearn we are going to consider as possible prediction procedures:\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\nknn = KNeighborsRegressor()\nlr = LinearRegression()\ndt = DecisionTreeRegressor()\n\n\n\n\n\n\n\nCaution\n\n\n\nIMPORTANT: Nothing in the above code chunk mentioned the data at all!\nWe are simply preparing three objects, named knn and lr and dt, which we will use with the data to obtain fitted models.\n\n\n\n\n10.4.3 Test/training split\nTo choose between our model specifications, we will need some training data for fitting the models, and some test data for computing the metrics. How will we get two separate datasets? Easy, we’ll just split up the one dataset we already have!\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\n\n\n\n\n\nCheck In\n\n\n\nTry running the above code, and looking at the objects it produces. Answer the following questions:\n\nHow many rows are in the datasets X_train and X_test?\nHow many elements are in the series y_train and y_test?\nWhat would you change to increase the number of rows in X_train?\nRun the code again, and re-examine the objects. Do they contain the exact same data as before? Why or why not?\n\n\n\n\n\n10.4.4 Model fitting and Metrics\nNow, we are ready to put our specifications to the test.\nFirst we fit our models on the training data:\n\nlr_fit = lr.fit(X_train, y_train)\ndt_fit = dt.fit(X_train, y_train)\nknn_fit = knn.fit(X_train, y_train)\n\n\n\n\n\n\n\nCaution\n\n\n\nThis is one of the few times you will modify an object in place; the .fit() method called on a model specification object, like lr or knn, will permanently modify that object to be the fitted version. However, for clarity, we recommend storing the fitted model under a new name, in case we re-fit the model objects later.\n\n\nThere isn’t much worth examining in the “model fit” details for knn and dt, but for lr we might want to see what coefficients were chosen - i.e., what were the values of \\(a,b,c,d\\) in our equation.\n\nlr_fit.coef_\n\narray([    85.62719887, -18528.22326865,  -4633.26657369,   4633.26657369])\n\n\nNext, we use the fitted models to get predicted values for the test data.\n\ny_pred_knn = knn_fit.predict(X_test)\ny_pred_lr = lr_fit.predict(X_test)\ny_pred_dt = dt_fit.predict(X_test)\n\n\n\n\n\n\n\nCheck In\n\n\n\nMake a plot of the predicted values versus the true values, y_test, for each of the three models. Which of the three models seems best to you?\n\n\nFinally, we choose a metric and compute it for the test data predictions. In this example, we’ll use the MSE:\n\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(y_test, y_pred_knn)\nmean_squared_error(y_test, y_pred_lr)\nmean_squared_error(y_test, y_pred_dt)\n\n1830841337.76\n\n\nThe smallest squared error was achieved by the linear regression!\nThus, our final model will be the linear regression model spec, fit on all our data:\n\nfinal_model = lr.fit(X, y)\nfinal_model.coef_\n\narray([    74.37717581, -13083.50151923,  -7484.17760684,   7484.17760684])",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "09-modeling_intro.html#conclusion",
    "href": "09-modeling_intro.html#conclusion",
    "title": "10  Introduction to Predictive Modeling",
    "section": "10.5 Conclusion",
    "text": "10.5 Conclusion\nNo practice exercise this week!",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html",
    "href": "10-multiple_linear_regression.html",
    "title": "11  Multiple Linear Regression",
    "section": "",
    "text": "11.1 Introduction\nThis document discusses modeling via multiple linear regression, and the tools in pandas and sklearn that can assist with this.\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html#introduction",
    "href": "10-multiple_linear_regression.html#introduction",
    "title": "11  Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html#machine-learning-mission",
    "href": "10-multiple_linear_regression.html#machine-learning-mission",
    "title": "11  Multiple Linear Regression",
    "section": "11.2 Machine Learning Mission",
    "text": "11.2 Machine Learning Mission\nRecall that in machine learning our goal is to predict the value of some target variable using one or more predictor variables. Mathematically, we we’re in the following setup\n\\[y = f(X) + \\epsilon \\]\nwhere \\(y\\) is our target variable and \\(X\\) represents the collection (data frame) of our predictor variables. To predict \\(y\\) well we need to estimate \\(f\\) well. We will see many different ways to estimate \\(f\\) including those methods mentioned in our previous modeling introduction:\n\nLinear Regression\nk-Nearest Neighbors\nDecision Trees\n\nFor the sake of completeness, the \\(\\epsilon\\) in the equation above represents random error that is independent of \\(X\\) and has mean zero. Our focus will be on using our predictors (\\(X\\)) and good construction of our estimate of \\(f\\) to make accurate predictions of \\(y\\) for new data.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html#simple-linear-regression",
    "href": "10-multiple_linear_regression.html#simple-linear-regression",
    "title": "11  Multiple Linear Regression",
    "section": "11.3 Simple Linear Regression",
    "text": "11.3 Simple Linear Regression\nOur estimate of \\(f\\) will eventually take on very complicated forms, but one of the simplest estimates is a straight line using a single predictor:\n\\[y = \\beta_0 + \\beta_1 X_1 + \\epsilon\\]\nThis is called simple linear regression and is an especially good place to start in our predictive modeling journey for many reasons, but in particular because our data and model are visualizable!\nConsider the following data in which we’d like to predict Sales from TV (i.e. the amount spent on TV advertising for a particular product).\n\n\n\nFig 1. Sales vs. TV.\n\n\nSuppose we want to use a straight-line model to predict Sales from TV, i.e. fit a simple linear regression model to these data.\n\n\n\n\n\n\nCheck In\n\n\n\nHow do we use our data here to estimate the values of \\(\\beta_0\\) and \\(\\beta_1\\) in our simple linear regression model?\n\n\nRecall that the vertical distance between a point and our simple linear regression model is called the residual for that observation (i.e. observed Sales minus predicted Sales).\n\n\n\nFig 2. Sales vs. TV with residuals.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThere are actually multiple ways to arrive at the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), but in our machine learning context it’s most useful to think of using calculus (you don’t need to know this calculus) to find the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the sum of squared residuals:\n\\[\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\).\n\n\n\nIn your other linear regression-related experiences you may have been introduced to some technical conditions (assumptions) associated with fitting a linear regression model to data:\n\nLinearity (the relationship between y and x is indeed linear)\nIindependence (of the errors)\nNormality (of the errors)\nEqual variance (of the errors)\n\n\n\n\n\n\n\nNote\n\n\n\nNone of these technical conditions were necessary to do the calculus of finding the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the sum of squared residuals!\nThese linear regression models can be fit by simply minimizing an error metric (e.g. sum of squared residuals).\nThe technical conditions above are necessary if we want to do inference about the model and its coefficients. That is, if we want to run hypothesis test(s) about the significance of predictors or the values of their coefficients then the technical conditions need to satisfied.\nOur course, and treatment of machine learning, will make use of other model evaluation techniques. So, for the most part, we will not worry about these technical conditions.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html#polynomial-regression",
    "href": "10-multiple_linear_regression.html#polynomial-regression",
    "title": "11  Multiple Linear Regression",
    "section": "11.4 Polynomial Regression",
    "text": "11.4 Polynomial Regression\nOne of the simplest ways to extend simple linear regression is to replace our straight-line model\n\\[y = \\beta_0 + \\beta_1 X_1 + \\epsilon\\]\nwith a polynomial function\n\\[y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\cdots + \\beta_d X_1^d + \\epsilon\\]\nWe can still estimate the coefficients with the least squares method described above for simple linear regression. Note that we’re still only using a single predictor variable here; we’ve added polynomial terms of that predictor variable. This can be useful if the relationship between y and x is not linear.\n\n\n\n\n\n\nNote\n\n\n\nAdding polynomial terms to our model is just one way to transform (and augment) our data set in a way that can improve our modeling and predictive efforts. In general, transforming variables in our dataset is a very common data wrangling strategy that can take place multiple times throughout modeling. The following are a few other ways variables can be transformed:\n\nStandardize numeric variables (i.e. transformed to have mean 0 and standard deviation 1)\nDummifying categorical variables (i.e. creating 0-1 variables out of text variables so they can be used in a model)\nTake log of numeric variables\nDiscretize/categorize numeric variables\n\n\n\nConsider the following set of models fit to the Sales dataset from above:\n\n\n\nFig 3. Sales vs. TV with polynomial models.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhich model seems to fit the Sales data best? Why?\nWhat is less good about the other models?\n\n\nLet’s establish some official language for your answers to the previous question!\n\n11.4.1 Underfitting\nUnderfitting is the scenario in which a model is unable to capture the relationship between the input(s) and the output variable accurately.\n\n\n\n\n\n\nCheck In\n\n\n\nDo you think any of the models in Fig 3. are underfitting the data here? If so, which ones and why?\n\n\n\n\n11.4.2 Overfitting\nOverfitting is the scenario in which a model captures too much about the data its being trained on. That is, model is capturing the relationship between the input(s) and the output, but ALSO some of the noise or nuance present in the data.\n\n\n\n\n\n\nCheck In\n\n\n\nDo you think any of the models in Fig 3. are overfitting the data here? If so, which ones and why?\n\n\n\n\n11.4.3 Under/Overfitting and Our ML Mission\nRemember that our goal is to estimate \\(f\\) in in a way that allows us to make accurate predictions for new data. In both the underfitting and the overfitting situations, we have model that will not generalize well (i.e. not make good predictions on new data), albeit in different ways. Use this idea about generalizability to comment one more time:\n\n\n\n\n\n\nCheck In\n\n\n\n\n\n\nFig 4. MPG vs Horespower with polynomial models.\n\n\nWhich of these models are underfitting? Overfitting? Why?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn Python, all of our data wrangling and variable preparation (e.g. transformations) needs to happen in the creation of y and X before any models get fit.\nFor example, if we wanted to fit a degree 3 polynomial model to our data then we would need to create columns in X for the squared and cubic terms in our model:\n\nX[\"x_sq\"] = X[\"x\"]**2\nX[\"X_cube\"] = X[\"x\"]**3",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-multiple_linear_regression.html#multiple-linear-regression",
    "href": "10-multiple_linear_regression.html#multiple-linear-regression",
    "title": "11  Multiple Linear Regression",
    "section": "11.5 Multiple Linear Regression",
    "text": "11.5 Multiple Linear Regression\nIt’s almost always the case that we have more than one predictor variable in our dataset. To estimate \\(f\\) in the best possible way we usually want to take advantage of everything we have access to. Extending our simple linear and polynomial regression models to this multiple linear regression model is straightforward!\n\\[y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon\\]\nwhere \\(X_1\\) represents the first predictor variable and so on. Coefficients are estimated in the same exact way! Minimize the sum of squared residuals:\n\\[\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nbut now \\(\\hat{y}_i\\) is based on the multiple linear regression model above.\nWe just blew this regression modeling wide open! We should now consider the inclusion of variables of any type within our model. If we think it can help in the prediction effort, then we should consider including it in our model.\n\n\n11.5.1 Dummifying Categorical Variables\nUnfortunately (?), sklearn in Python cannot handle character variables in our input dataset (X). We still want to make use of character-based, categorical variables in our modeling efforts. So, we’ll need to code or dummify them.\n\n\n\n\n\n\nExample\n\n\n\nSuppose we have a character variable in our dataset with “Yes” and “No” values. This variable could be dummified by creating a new variable whose value is 1 if the original variable’s value was “Yes” and 0 if the original variable’s value was “No”.\nIn this case, the original variable had two distinct values (“Yes” and “No”), which required a single new variable to encode that information. For most of our modeling techniques this will be the case: we need n-1 new variables to encode the information from a variable with n distinct values.\n\n\nThankfully, there exist Python functions to help us dummify variables without having to do this by hand ourselves. There are at least two such functions:\n\nOneHotEncoder in the sklearn library\nget_dummies in the pandas library\n\n\n\n\n\n\n\nCheck In\n\n\n\nApply both the OneHotEncoder and get_dummies functions to the species variable in the Palmer Penguins dataset. Observe the results and discuss the differences, if there are any.\n\n\n\n\n11.5.2 Standardizing Quantitative Variables\nData are not always nicely behaved. Many machine learning techniques greatly benefit from quantitative variables that do not contain extreme values and are nicely shaped. One way to help ensure this is to standardize our quantitative predictors of interest.\nTo standardize a quantitative variable means to subtract the mean from all values and divide by the standard deviation. The resulting variable will still contain useful information, but have been transformed to have mean 0 and standard deviation 1.\nThankfully, once again, there is a Python function that will assist us with this: StandardScaler in the sklearn library.\n\n\n\n\n\n\nCheck In\n\n\n\nApply the StandardScaler function to the bill_length_mm variable in the Palmer Penguins dataset. Observe the results. Did you overwrite the original variable or create a new one? If the former, are you able to get back to the original variable if you wanted to?\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "11-model_validation.html",
    "href": "11-model_validation.html",
    "title": "12  Model Validation",
    "section": "",
    "text": "12.1 Introduction\nThis document discusses modeling via multiple linear regression, and the tools in pandas and sklearn that can assist with this. We will expand on our previous content by diving deeper into model evaluation.\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "11-model_validation.html#introduction",
    "href": "11-model_validation.html#introduction",
    "title": "12  Model Validation",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "11-model_validation.html#machine-learning-mission",
    "href": "11-model_validation.html#machine-learning-mission",
    "title": "12  Model Validation",
    "section": "12.2 Machine Learning Mission",
    "text": "12.2 Machine Learning Mission\nRecall that in machine learning our goal is to predict the value of some target variable using one or more predictor variables. Mathematically, we we’re in the following setup\n\\[y = f(X) + \\epsilon \\]\nwhere \\(y\\) is our target variable and \\(X\\) represents the collection (data frame) of our predictor variables. So far we’ve discussed tackling this via multiple linear regression.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "11-model_validation.html#multiple-linear-regression",
    "href": "11-model_validation.html#multiple-linear-regression",
    "title": "12  Model Validation",
    "section": "12.3 Multiple Linear Regression",
    "text": "12.3 Multiple Linear Regression\nRecall that we’ve discussed the following model specification:\n\\[y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon\\]\nwhere \\(X_j\\) (\\(j = 1,...,p\\)) can represent any type of predictor (or transformed predictor) variable in our dataset.\nFor most of our machine learning methods we’ll want to include every variable in our dataset that we consider a predictor. So, multiple linear regression is also our way of setting the stage for the rest of our machine learning journey. Dataset preparation for other machine learning methods will mostly be the same as we’ve discussed for multiple linear regression (e.g. dummifying categorical variables, standardizing quantitative variables, etc.).\n\n\n\n\n\n\nCheck In\n\n\n\nLet’s try, once again, to predict bill_depth_mm in the Palmer Penguins dataset. However, you should use all available predictors in the dataset. Train your model on the entire dataset and compute the sum of the squared residuals for your fitted model.\n\n\nIs your multiple regression model good?!\nWe discussed before that we could compute the Mean Squared Error as one particular metric of interest. However, the magnitude of the value of the mean squared error for a single model specification is meaningless. There is no universal threshold to compare this value to.\nOne metric that does have more standardized values is R-squared, the coefficient of determination. This metric takes value between 0 and 1, with values closer to 1 indicating a better fit.\n\n\n\n\n\n\nCheck In\n\n\n\nCompute the R-squared value for the your full model from above and comment on its value concerning the model fit. The sklearn library has a function for this:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "11-model_validation.html#model-evaluation",
    "href": "11-model_validation.html#model-evaluation",
    "title": "12  Model Validation",
    "section": "12.4 Model Evaluation",
    "text": "12.4 Model Evaluation\nEvaluating the quality of a model can be complex, but it’s been well studied. As standardized as R-squared is as a metric for a model, it suffers from at least one weakness:\n\nIt is mathematically guaranteed to stay the same or increase in value if we add terms (predictors) to the model.\n\nThis is essentially saying that any new variable can’t make the model any worse; it can add zero or more information, but not “negative information”. We generally don’t want to arbitrarily add predictors to our model. Bigger and more complex models are often more computationally costly, harder to interpret (if at all), and more likely to overfit our data.\nBut wait…aren’t we supposed to be computing metrics and evaluating our models on test data?! Yes!\nThis guarantee about the value of R-squared never decreasing is only true if computed on the training data.\n\n\n\n\n\n\nNote\n\n\n\nRecall…\nTo choose between our model specifications, we will need some training data for fitting the models, and some test data for computing the metrics. How will we get two separate datasets? Easy, we’ll just split up the one dataset we already have!\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\n\nWith our training and testing datasets established, we’re ready to fit our models on the training data and then make predictions on the test data!\nOnce we have predictions on the test data we need to decide on one or more metrics to use to compare our candidate models.\n\n\n\n\n\n\nCheck In\n\n\n\nThere is a large number of metrics we can use to evaluate models of different types. Identify at least two different metrics we could use to evaluate our bill_depth_mm model:\nhttps://scikit-learn.org/stable/search.html?q=metrics\nSplit the penguin dataset into training and testing subsets. Fit your model to the training data and compute the value of your two metrics on the test data.\n\n\nNo matter what metrics you happened to identify, a few popular choices are:\n\nRoot Mean Squared Error (RMSE)\nMean Absolute Error (MAE)\nR-Squared\n\nAll of these metrics have corresponding functions in sklearn.\n\n12.4.1 Under/Overfitting\nDo you think your full model for bill_depth_mm underfit the data? Overfit? Was just fine? How do we even tell?!\n\n\n\n\n\n\nNote\n\n\n\nIn general, we will organize a short list of model specifications that we want to explore and then compute the value of our metrics for each, on our test dataset. The “winner” is the one with the best values of the metrics on our test data.\n\n\nBut how do we assess underfit and overfit when we can’t visualize our data and model as easily like we did previously with our polynomial regression models…?\nThe hope is that we can evaluate complex models in the following way:\n\n\n\nFig 1. Fit vs. Complexity\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nIn the graph above, which model is underfit? Which model is overfit? Does this make sense in the right graph as well? How does the right graph help us assess under/overfit when the model isn’t visualizable?\n\n\n\n\n12.4.2 Suspiciously Good\nSuppose we fit a model to our training data and apply it to our test data and obtain extremely low values for both the training and the test error metrics…\n\n\n\nFig 2. Sus\n\n\nOf course we want our models to do well, but it’s actually fantastic to have intuition that model performance on the test dataset is suspicious if it’s seemingly too good.\nA few things to check in this situation:\n\nReview all of your code to make sure your X dataframe didn’t include the target variable (y) as well\nCheck to see if any of your predictor variables are “surrogates” (or superficially associated with) for your target variable (y)\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Validation</span>"
    ]
  },
  {
    "objectID": "12-cross_validation.html",
    "href": "12-cross_validation.html",
    "title": "\n13  Pipelines, Cross-Validation, and Tuning\n",
    "section": "",
    "text": "13.1 Introduction\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nIn the last two chapters, we learned to use sklearn and python to perform the main steps of the modeling procedure:\nIn this chapter, we will combine all these steps into one pipeline, sometimes called a workflow, to streamline our modeling process.\nThen, we will use our pipeline objects to quickly and easily perform more complex model fitting and validation tasks.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pipelines, Cross-Validation, and Tuning</span>"
    ]
  },
  {
    "objectID": "12-cross_validation.html#introduction",
    "href": "12-cross_validation.html#introduction",
    "title": "\n13  Pipelines, Cross-Validation, and Tuning\n",
    "section": "",
    "text": "Preprocessing: Choosing which predictors to include, and how we will transform them to prepare them for the model, e.g. pd.get_dummies() or StandardScaler()\nModel Specification: Choosing the procedure we will use to make sure predictions; e.g. LinearRegression() or NearestNeighborsRegressor()\nFitting on training data: Using train_test_split() to establish a randomly chosen training set, then using .fit() to fit the model on that set.\nValidating on test data: Using .predict to make predictions on the test set, and then computing desired metrics to compare models, like rmse() or r2().",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pipelines, Cross-Validation, and Tuning</span>"
    ]
  },
  {
    "objectID": "12-cross_validation.html#pipelines",
    "href": "12-cross_validation.html#pipelines",
    "title": "\n13  Pipelines, Cross-Validation, and Tuning\n",
    "section": "\n13.2 Pipelines",
    "text": "13.2 Pipelines\nIf we already know how to perform each modeling step, why would we need pipelines? Consider the following cautionary tale…\n\n13.2.1 Cautionary Tale:\n\n13.2.1.1 Chapter One\nSuppose you want to predict (of course) house prices from house characteristics.\nClick here to download the full AMES housing dataset\nClick here for data documentation\nYou might take an approach like this:\n\nlr = LinearRegression()\n\n\names = pd.read_csv(\"data/AmesHousing.csv\")\nX = ames[[\"Gr Liv Area\", \"TotRms AbvGrd\"]]\ny = ames[\"SalePrice\"]\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nX_train_s = (X_train - X_train.mean())/X_train.std()\n\nlr_fitted = lr.fit(X_train_s, y_train)\nlr_fitted.coef_\n\narray([ 74207.85836241, -19901.17744922])\n\n\nThen, you decide to apply your fitted model to the test data:\n\ny_preds = lr_fitted.predict(X_test)\n\nr2_score(y_test, y_preds)\n\n-2507767.075238028\n\n\nOh no! An \\(R^2\\) score of negative 2 million??? How could this have happened?\nLet’s look at the predictions:\n\ny_preds[1:5]\n\narray([7.69806469e+07, 1.06138938e+08, 8.83688547e+07, 8.39905911e+07])\n\n\nWow. We predicted that the first five test houses would all be worth over $50 million dollars. That doesn’t seem quite right.\n\n\n\n\n\n\nCheck In\n\n\n\nWhat went wrong here?\n\n\n\n13.2.1.2 Chapter Two\nNow a new house has come along, and you need to predict it. That house has a living area of 889 square feet, and 6 rooms.\n\nnew_house = pd.DataFrame(data = {\"Gr Liv Area\": [889], \"TotRms AbvGrd\": [6]})\nnew_house\n\n   Gr Liv Area  TotRms AbvGrd\n0          889              6\n\n\nWe won’t make the same mistake again! Time to standardize our new data:\n\nnew_house_s = (new_house - new_house.mean())/new_house.std()\nnew_house_s\n\n   Gr Liv Area  TotRms AbvGrd\n0          NaN            NaN\n\n\nOh no! Our data is now all NaN!!!\n\n\n\n\n\n\nCheck In\n\n\n\nWhat happened this time, and how can we fix it?\n\n\n\n13.2.1.3 The Moral of the Story\nA massively important principle of the modeling process is: New data that we want to predict on must go through the exact same pre-processing as the training data.\nBy “exact same”, we don’t mean “same idea”, we mean the same calculations.\nTo standardize our training data, we subtracted from each column its mean in the training data, and then divided each column by the standard deviation in the training data. Thus, for any new data that comes along - whether it is a larger test dataset, or a single new house to predict on - we need to use the same numbers to standardize:\n\nX_test_s = (X_test - X_train.mean())/X_train.std()\ny_preds = lr_fitted.predict(X_test_s)\n\nr2_score(y_test, y_preds)\n\n0.4949343935252205\n\n\n\nnew_house_s = (new_house - X_train.mean())/X_train.std()\nlr_fitted.predict(new_house_s)\n\narray([97709.94301488])\n\n\nNotice that we used X_train.mean() and X_train.std() in each case: we “learned” our estimates for the mean and sd of the columns when we fit the model, and we use those for all future predictions!\n\n13.2.2 Pipeline Objects\nNow, for an easier way to make sure preprocessing happens correctly. Instead of making a model object, like LinearRegression(), that we use for fitting and predicting, we will make a pipeline object that contains all our steps:\n\nlr_pipeline = Pipeline(\n  [StandardScaler(),\n  LinearRegression()]\n)\n\nlr_pipeline\n\n\n\n\nPipeline(steps=[StandardScaler(), LinearRegression()])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nPipelinePipeline(steps=[StandardScaler(), LinearRegression()])\n\n\n\n\nWe can even name the steps in our pipeline, in order to help us keep track of them:\n\nlr_pipeline = Pipeline(\n  [(\"standardize\", StandardScaler()),\n  (\"linear_regression\", LinearRegression())]\n)\n\nlr_pipeline\n\n\n\n\nPipeline(steps=[('standardize', StandardScaler()),\n                ('linear_regression', LinearRegression())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipelinePipeline(steps=[('standardize', StandardScaler()),\n                ('linear_regression', LinearRegression())])\n\n\n\nStandardScalerStandardScaler()\n\n\nLinearRegressionLinearRegression()\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPay careful attention to the use of [ and ( inside the Pipeline function. The function takes a list ([]) of steps; each step may be put into a tuple () with a name of your choice.\n\n\nNow, we can use this pipeline for all our modeling tasks, without having to worry about doing the standardizing ourselves ahead of time:\n\nlr_pipeline_fitted = lr_pipeline.fit(X_train, y_train)\n\ny_preds = lr_pipeline_fitted.predict(X_test)\nr2_score(y_test, y_preds)\n\n0.4949343935252205\n\n\n\nlr_pipeline_fitted.predict(new_house)\n\narray([97709.94301488])\n\n\n\n13.2.3 Column Transformers\nBecause there may be many different steps to the data preprocessing, it can sometimes be convenient to separate these steps into individual column transformers.\nFor example, suppose you wanted to include a third predictor in your house price prediction: The type of building it is (Bldg Type); e.g., a Townhouse, a single-family home, etc.\nSince this is a categorical variable, we need to turn it into dummy variables first, using OneHotEncoder(). But we don’t want to put OneHotEncoder() directly into our pipeline, because we don’t want to dummify every variable!\nSo, we’ll make column transformers to handle our variables separately:\n\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(\n  [\n    (\"dummify\", OneHotEncoder(sparse_output = False), [\"Bldg Type\"]),\n    (\"standardize\", StandardScaler(), [\"Gr Liv Area\", \"TotRms AbvGrd\"])\n  ],\n  remainder = \"drop\"\n)\n\n\nlr_pipeline = Pipeline(\n  [(\"preprocessing\", ct),\n  (\"linear_regression\", LinearRegression())]\n)\n\nlr_pipeline\n\n\n\n\nPipeline(steps=[('preprocessing',\n                 ColumnTransformer(transformers=[('dummify',\n                                                  OneHotEncoder(sparse_output=False),\n                                                  ['Bldg Type']),\n                                                 ('standardize',\n                                                  StandardScaler(),\n                                                  ['Gr Liv Area',\n                                                   'TotRms AbvGrd'])])),\n                ('linear_regression', LinearRegression())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(transformers=[('dummify',\n                                                  OneHotEncoder(sparse_output=False),\n                                                  ['Bldg Type']),\n                                                 ('standardize',\n                                                  StandardScaler(),\n                                                  ['Gr Liv Area',\n                                                   'TotRms AbvGrd'])])),\n                ('linear_regression', LinearRegression())])\n\n\n\n\npreprocessing: ColumnTransformerColumnTransformer(transformers=[('dummify', OneHotEncoder(sparse_output=False),\n                                 ['Bldg Type']),\n                                ('standardize', StandardScaler(),\n                                 ['Gr Liv Area', 'TotRms AbvGrd'])])\n\n\n\n\ndummify['Bldg Type']\n\n\nOneHotEncoderOneHotEncoder(sparse_output=False)\n\n\n\n\nstandardize['Gr Liv Area', 'TotRms AbvGrd']\n\n\nStandardScalerStandardScaler()\n\n\n\n\n\nLinearRegressionLinearRegression()\n\n\n\n\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhat does the remainder = \"drop\" part of the ColumnTransformer() function do? Why might that be useful?\nHint: What happens when you try to fit this pipeline on X_train?\n\n\n\nX = ames.drop(\"SalePrice\", axis = 1)\ny = ames[\"SalePrice\"]\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlr_fitted = lr_pipeline.fit(X_train, y_train)\n\n\n13.2.3.1 Checking preprocessing\nWe’ve seen the value of including preprocessing steps in a pipeline instead of doing them “by hand”. However, you might sometimes want to see what that processed data looks like. This is one advantage of a column transformer - it can be separately used to fit and transform datasets:\n\nct_fitted = ct.fit(X_train)\n\nct.transform(X_train)\n\narray([[ 0.        ,  0.        ,  1.        , ...,  0.        ,\n         2.53707757,  0.96562117],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n         0.17823224, -0.29154606],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n         0.72242894, -0.29154606],\n       ...,\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.61728438, -0.92012968],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.70633475, -0.29154606],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.73403931, -0.29154606]])\n\nct.transform(X_test)\n\narray([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.10105526, -0.29154606],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n         0.45132004,  0.33703755],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.76174387, -0.92012968],\n       ...,\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.72810262, -0.92012968],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n         0.96979108,  0.96562117],\n       [ 1.        ,  0.        ,  0.        , ...,  0.        ,\n        -0.18588482, -0.29154606]])\n\n\n\n13.2.4 Challenges of pipelines\nAlthough Pipeline objects are incredible tools for making sure your model process is reproducible and correct, they come with some frustrations. Here are a few you might encounter, and our advice for dealing with them:\n\n13.2.4.1 Extracting information\nWhen we wanted to find the fitted coefficients of a model object, we could simply use .coef_. However, since a Pipeline is not a model object, this no longer works:\n\nlr_pipeline_fitted.coef_\n\n'Pipeline' object has no attribute 'coef_'\n\n\nWhat we need to do instead is find the step of the pipeline where the model fitting happened, and get those coefficients:\n\nlr_pipeline_fitted.named_steps['linear_regression'].coef_\n\narray([ 74190.96798814, -19896.6477627 ])\n\n\n\n13.2.4.2 Pandas input, numpy output\nYou may have noticed that sklearn functions are designed to handle pandas objects nicely - which is a good thing, since we like to do our data cleaning and manipulation in pandas! However, the outputs of these model functions are typically numpy arrays:\n\ntype(y_preds)\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nOccasionally, this can cause trouble; especially when you want to continue data manipulation after making predictions.\nFortunately, it is possible to set up your pipeline to output pandas objects instead, using the set_output() method:\n\nlr_pipeline = Pipeline(\n  [(\"preprocessing\", ct),\n  (\"linear_regression\", LinearRegression())]\n).set_output(transform=\"pandas\")\n\n\nct.fit_transform(X_train)\n\n      dummify__Bldg Type_1Fam  ...  standardize__TotRms AbvGrd\n815                       0.0  ...                    0.965621\n2846                      1.0  ...                   -0.291546\n868                       1.0  ...                   -0.291546\n1250                      1.0  ...                   -1.548713\n175                       1.0  ...                   -1.548713\n...                       ...  ...                         ...\n2568                      1.0  ...                    0.965621\n2684                      1.0  ...                   -0.920130\n389                       1.0  ...                   -0.920130\n2743                      1.0  ...                   -0.291546\n1962                      1.0  ...                   -0.291546\n\n[2197 rows x 7 columns]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that in this transformed dataset, the column names now have prefixes for the named steps in the column transformer.\nNotice also the structure of the names of the dummified variables:\n[step name]__[variable name]_[category]\n\n\n\n13.2.4.3 Interactions and Dummies\nSometimes, we want to include an interaction term in our model; for example,\n\\[ \\text{House Price} = \\text{House Size} + \\text{Num Rooms} + \\text{Size}*\\text{Rooms}\\]\nAdding interactions between numeric variables is simple, we simply add a “polynomial” step to our preprocessing, except we leave off the squares and cubes, and keep only the interactions:\n\nct_inter = ColumnTransformer(\n  [\n    (\"interaction\", PolynomialFeatures(interaction_only = True), [\"Gr Liv Area\", \"TotRms AbvGrd\"])\n  ],\n  remainder = \"drop\"\n).set_output(transform = \"pandas\")\n\nct_inter.fit_transform(X_train)\n\n      interaction__1  ...  interaction__Gr Liv Area TotRms AbvGrd\n815              1.0  ...                                 22296.0\n2846             1.0  ...                                  9570.0\n868              1.0  ...                                 11220.0\n1250             1.0  ...                                  3092.0\n175              1.0  ...                                  2988.0\n...              ...  ...                                     ...\n2568             1.0  ...                                 17256.0\n2684             1.0  ...                                  5240.0\n389              1.0  ...                                  5965.0\n2743             1.0  ...                                  6888.0\n1962             1.0  ...                                  6804.0\n\n[2197 rows x 4 columns]\n\n\nHowever, to add an interaction with a dummified variable, we first need to know what the new column names are after the dummification step.\nFor example, suppose we wanted to add an interaction term for the number of rooms in the house and whether the house is a single family home.\nWe’ll need to run the data through one preprocessing step, to get the dummy variables, then a second preprocessing that uses those variables.\n\nct_dummies = ColumnTransformer(\n  [(\"dummify\", OneHotEncoder(sparse_output = False), [\"Bldg Type\"])],\n  remainder = \"passthrough\"\n).set_output(transform = \"pandas\")\n\nct_inter = ColumnTransformer(\n  [\n    (\"interaction\", PolynomialFeatures(interaction_only = True), [\"remainder__TotRms AbvGrd\", \"dummify__Bldg Type_1Fam\"]),\n  ],\n  remainder = \"drop\"\n).set_output(transform = \"pandas\")\n\nX_train_dummified = ct_dummies.fit_transform(X_train)\nX_train_dummified\n\n      dummify__Bldg Type_1Fam  ...  remainder__Sale Condition\n815                       0.0  ...                     Normal\n2846                      1.0  ...                     Normal\n868                       1.0  ...                     Normal\n1250                      1.0  ...                    Abnorml\n175                       1.0  ...                     Normal\n...                       ...  ...                        ...\n2568                      1.0  ...                     Normal\n2684                      1.0  ...                     Normal\n389                       1.0  ...                     Normal\n2743                      1.0  ...                     Normal\n1962                      1.0  ...                     Family\n\n[2197 rows x 85 columns]\n\nct_inter.fit_transform(X_train_dummified)\n\n      interaction__1  ...  interaction__remainder__TotRms AbvGrd dummify__Bldg Type_1Fam\n815              1.0  ...                                                0.0            \n2846             1.0  ...                                                6.0            \n868              1.0  ...                                                6.0            \n1250             1.0  ...                                                4.0            \n175              1.0  ...                                                4.0            \n...              ...  ...                                                ...            \n2568             1.0  ...                                                8.0            \n2684             1.0  ...                                                5.0            \n389              1.0  ...                                                5.0            \n2743             1.0  ...                                                6.0            \n1962             1.0  ...                                                6.0            \n\n[2197 rows x 4 columns]\n\n\nOoof! This is not very elegant, we admit. But it is still worth the effort to set up a full pipeline, instead of transforming things by hand, as we will see in the next section.\n\n13.2.5 Your turn\n\n\n\n\n\n\nPractice Activity\n\n\n\nConsider four possible models for predicting house prices:\n\nUsing only the size and number of rooms.\nUsing size, number of rooms, and building type.\nUsing size and building type, and their interaction.\nUsing a 5-degree polynomial on size, a 5-degree polynomial on number of rooms, and also building type.\n\nSet up a pipeline for each of these four models.\nThen, get predictions on the test set for each of your pipelines, and compute the root mean squared error. Which model performed best?\nNote: You should only use the function train_test_split() one time in your code; that is, we should be predicting on the same test set for all three models.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pipelines, Cross-Validation, and Tuning</span>"
    ]
  },
  {
    "objectID": "12-cross_validation.html#cross-validation",
    "href": "12-cross_validation.html#cross-validation",
    "title": "\n13  Pipelines, Cross-Validation, and Tuning\n",
    "section": "\n13.3 Cross-Validation",
    "text": "13.3 Cross-Validation\nNow that we have all our modeling process wrapped up in one tidy pipeline bundle, we can upgrade another piece of the puzzle: the test/training split.\nIn the previous exercise, we used our test metrics to compare pipelines, with the idea that a pipeline which performs well on test data is likely to perform well on future data. This is a bit of a big assumption though. What if we just so happened to get an “unlucky” test/training split, where the test set just so happened to contain houses that don’t follow the usual price patterns?\nTo try to avoid unlucky splits, an easy solution is to simply do many different test/training splits and see what happens. But we have to be careful - if we end up doing many random splits, we still might end up getting similar “unlucky” test sets every time.\nOur solution to this challenge is k-fold cross-validation (sometimes called v-fold, since the letter k is used for so many other things in statistics). In cross-validation, we perform multiple test/training splits, but we make sure each observation only gets one “turn” in the test set.\nA procedure for 5-fold cross-validation on the housing data might look like this:\n\nRandomly divide the houses into 5 sets. Call these “Fold 1”, “Fold 2”, … “Fold 5”.\nMake Fold 1 the test sets, and Folds 2-5 together the training set.\nFit the data on the houses in the training set, predict the prices of the houses test set, and record the resulting R-squared value.\nRepeat (2) and (3) four more times, letting each fold have a turn as the test set.\nTake the average of the 5 different R-squared values.\n\nHere is a picture that helps illustrate the procedure:\n\nThe advantage of this process is that our final cross-validated metric for our proposed pipeline is in fact the average of five different values - so even if one of the test sets is “unlucky”, the others will average it out.\n\n\n\n\n\n\nNote\n\n\n\nRemember that ultimately, all of this effort has only one goal: to get a “fair” measurement of how good a particular pipeline is at predicting, so that we can choose our best final pipeline from among the many options.\n\n\n\n13.3.1 cross_val_score\n\nEven if you are convinced that cross-validation could be useful, it might seem daunting. Imagine needing to create five different test/training sets, and go through the process of fitting, predicting, and computing metrics for every single one.\nFortunately, now that we have Pipeline objects, sklearn has shortcuts.\n\nfrom sklearn.model_selection import cross_val_score\n\nX = ames.drop(\"SalePrice\", axis = 1)\ny = ames[\"SalePrice\"]\n\n\nct = ColumnTransformer(\n  [\n    (\"dummify\", OneHotEncoder(sparse_output = False), [\"Bldg Type\"]),\n    (\"standardize\", StandardScaler(), [\"Gr Liv Area\", \"TotRms AbvGrd\"])\n  ],\n  remainder = \"drop\"\n)\n\nlr_pipeline_1 = Pipeline(\n  [(\"preprocessing\", ct),\n  (\"linear_regression\", LinearRegression())]\n).set_output(transform=\"pandas\")\n\n\nscores = cross_val_score(lr_pipeline_1, X, y, cv=5, scoring='r2')\nscores\n\narray([0.53184383, 0.53247653, 0.42905573, 0.56571819, 0.60646427])\n\n\nWow! Once the pipeline is set up, only one small line of code performs the entire cross-validation procedure!\nNotice that here, we never used train_test_split(). We simply passed our entire X and y objects to the cross_val_score function, and it took care of making the 5 cross validation folds; as well as all of the fitting, predicting, and computing of metrics.\nWe now can simply report our final cross-validated R-squared value:\n\nscores.mean()\n\n0.5331117091273414\n\n\n\n\n\n\n\n\nPractice Activity\n\n\n\nOnce again consider four modeling options for house price:\n\nUsing only the size and number of rooms.\nUsing size, number of rooms, and building type.\nUsing size and building type, and their interaction.\nUsing a 5-degree polynomial on size, a 5-degree polynomial on number of rooms, and also building type.\n\nUse cross_val_score with the pipelines you made earlier to find the cross-validated root mean squared error for each model.\nWhich do you prefer? Does this agree with your conclusion from earlier?\n\n\n\n13.3.2 Tuning\nIn our previous exercise, we considered one-degree polynomials as well as 5-degree polynomials. But what if we wanted to try everything from 1-degree to 10-degree, without needing to manually create 10 different pipelines?\nThis process - where we want to try a range of different values in our model specification and see which has the best cross-validation metrics - is called tuning.\nSince tuning is a common need in modeling, sklearn has another shortcut for us, grid_search_cv. But in order to capture the process, this function needs to know:\n\nWhat pipeline structure you want to try.\nWhich piece of the pipeline you are plugging in a range of different values for.\nWhich values to plug in.\n\nWe’ll start by writing ourselves a function that makes the pipeline we need for a particular degree number.\n\nfrom sklearn.model_selection import GridSearchCV\n\nct_poly = ColumnTransformer(\n  [\n    (\"dummify\", OneHotEncoder(sparse_output = False), [\"Bldg Type\"]),\n    (\"polynomial\", PolynomialFeatures(), [\"Gr Liv Area\"])\n  ],\n  remainder = \"drop\"\n)\n\nlr_pipeline_poly = Pipeline(\n  [(\"preprocessing\", ct_poly),\n  (\"linear_regression\", LinearRegression())]\n).set_output(transform=\"pandas\")\n\ndegrees = {'preprocessing__polynomial__degree': np.arange(1, 10)}\n\ngscv = GridSearchCV(lr_pipeline_poly, degrees, cv = 5, scoring='r2')\n\nA few important things to notice in this code:\n\nIn the polynomial step of our ColumnTransformer, we did not specify the number of features in the PolynomialFeatures() function.\nWe created a dictionary object named degrees, which was simply a list of the numbers we wanted to try inserting into the PolynomialFeatures() function.\nThe name of the list of numbers in our dictionary object was preprocessing__polynomial__degree. This follows the pattern\n\n[name of step in pipeline]__[name of step in column transformer]__[name of argument to function]\n\nThe code above did not result in cross-validated metrics for the 9 model options (degrees 1 to 9). Like a Pipeline object, a GridSearchCV object “sets up” a process - we haven’t actually given it data yet.\n\n\n\n\n\n\n\nNote\n\n\n\nYou might be noticing a pattern in sklearn here: in general (although not always), a capitalized function is a “set up” step, like LinearRegression() or OneHotEncoder or Pipeline(). A lowercase function is often a calculation on data, like .fit() or .predict() or cross_val_score().\n\n\nNow, we will fit our “grid search” tuning procedure to the data:\n\ngscv_fitted = gscv.fit(X, y)\n\ngscv_fitted.cv_results_\n\n{'mean_fit_time': array([0.00373011, 0.0035553 , 0.00363498, 0.00373187, 0.00346799,\n       0.00376768, 0.00669088, 0.00375037, 0.00373635]), 'std_fit_time': array([3.38883354e-04, 9.75645415e-05, 2.30070218e-04, 1.75824584e-04,\n       1.73586763e-04, 3.46568232e-04, 5.88991242e-03, 8.52224735e-05,\n       8.85113811e-05]), 'mean_score_time': array([0.00158796, 0.00151463, 0.00150099, 0.00152459, 0.0014884 ,\n       0.00157409, 0.00168405, 0.00159721, 0.00150619]), 'std_score_time': array([1.71109784e-04, 6.70866670e-05, 1.43201241e-04, 7.17739878e-05,\n       1.24161236e-04, 9.94328850e-05, 2.57057937e-04, 1.00324816e-04,\n       2.26392722e-05]), 'param_preprocessing__polynomial__degree': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9],\n             mask=[False, False, False, False, False, False, False, False,\n                   False],\n       fill_value='?',\n            dtype=object), 'params': [{'preprocessing__polynomial__degree': 1}, {'preprocessing__polynomial__degree': 2}, {'preprocessing__polynomial__degree': 3}, {'preprocessing__polynomial__degree': 4}, {'preprocessing__polynomial__degree': 5}, {'preprocessing__polynomial__degree': 6}, {'preprocessing__polynomial__degree': 7}, {'preprocessing__polynomial__degree': 8}, {'preprocessing__polynomial__degree': 9}], 'split0_test_score': array([0.53667199, 0.53861812, 0.55155397, 0.55000299, 0.50847886,\n       0.50803388, 0.50404864, 0.48824759, 0.45353318]), 'split1_test_score': array([0.52379929, 0.51739889, 0.52489524, 0.52536327, 0.45451042,\n       0.45231332, 0.44389116, 0.42226201, 0.38464169]), 'split2_test_score': array([  0.43205901,   0.44999102,   0.50538581,   0.45008921,\n         0.1733882 ,  -0.39418512,  -1.67096399,  -4.78706736,\n       -12.6045523 ]), 'split3_test_score': array([  0.56266573,   0.57574172,   0.58653708,   0.59197747,\n         0.54742275,   0.53273129,   0.31370342,  -1.49591606,\n       -11.46561678]), 'split4_test_score': array([0.59424739, 0.57528078, 0.58780969, 0.59316272, 0.57550038,\n       0.5702938 , 0.55592936, 0.53199328, 0.50402512]), 'mean_test_score': array([ 0.52988868,  0.5314061 ,  0.55123636,  0.54211913,  0.45186012,\n        0.33383744,  0.02932172, -0.96809611, -4.54559382]), 'std_test_score': array([0.05453461, 0.04640531, 0.03280237, 0.05273277, 0.14503685,\n       0.36602408, 0.85397816, 2.05754398, 6.12585788]), 'rank_test_score': array([4, 3, 1, 2, 5, 6, 7, 8, 9], dtype=int32)}\n\n\nOof, this is a lot of information!\nUltimately, what we care about is the cross-validated metric (i.e. average over the 5 folds) for each of the 9 proposed models.\n\ngscv_fitted.cv_results_['mean_test_score']\n\narray([ 0.52988868,  0.5314061 ,  0.55123636,  0.54211913,  0.45186012,\n        0.33383744,  0.02932172, -0.96809611, -4.54559382])\n\n\nIt can sometimes be handy to convert these results to a pandas data frame for easy viewing:\n\npd.DataFrame(data = {\"degrees\": np.arange(1, 10), \"scores\": gscv_fitted.cv_results_['mean_test_score']})\n\n   degrees    scores\n0        1  0.529889\n1        2  0.531406\n2        3  0.551236\n3        4  0.542119\n4        5  0.451860\n5        6  0.333837\n6        7  0.029322\n7        8 -0.968096\n8        9 -4.545594\n\n\nIt appears that the third model - degree 3 - had the best cross-validated metrics, with an R-squared value of about 0.55.\n\n\n\n\n\n\nCheck In\n\n\n\nRecall that the model fitting step is when coefficients are computed for the linear regression.\nHow many different model fitting steps occurred when gscv.fit(X, y) was run?\n\n\n\n13.3.3 Your Turn\n\n\n\n\n\n\nPractice Activity\n\n\n\nConsider one hundred modeling options for house price:\n\nHouse size, trying degrees 1 through 10\nNumber of rooms, trying degrees 1 through 10\nBuilding Type\n\nHint: The dictionary of possible values that you make to give to GridSearchCV will have two elements instead of one.\nQ1: Which model performed the best?\nQ2: What downsides do you see of trying all possible model options? How might you go about choosing a smaller number of tuning values to try?",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Pipelines, Cross-Validation, and Tuning</span>"
    ]
  },
  {
    "objectID": "13-penalized_regression.html",
    "href": "13-penalized_regression.html",
    "title": "14  Penalized Regression",
    "section": "",
    "text": "14.1 Introduction\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_selector, ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score\nNow that we know how to create and use pipelines, it’s finally time to start adding more model specifications to our repertoire.\nTo motivate today’s new models, consider the house price prediction problem. This time, we are going to use ALL of our predictors!\nClick here to download the full AMES housing dataset\nClick here for data documentation\nFirst, we’ll read and clean our data..\n# Read the data\names = pd.read_csv(\"data/AmesHousing.csv\")\n\n# Get rid of columns with mostly NaN values\ngood_cols = ames.isna().sum() &lt; 100\names = ames.loc[:,good_cols]\n\n# Drop other NAs\names = ames.dropna()\nThen, we’ll set up our pipeline…\nX = ames.drop([\"SalePrice\", \"Order\", \"PID\"], axis = 1)\ny = ames[\"SalePrice\"]\n\n\nct = ColumnTransformer(\n  [\n    (\"dummify\", \n    OneHotEncoder(sparse_output = False, handle_unknown='ignore'),\n    make_column_selector(dtype_include=object)),\n    (\"standardize\", \n    StandardScaler(), \n    make_column_selector(dtype_include=np.number))\n  ],\n  remainder = \"passthrough\"\n)\n\nlr_pipeline_1 = Pipeline(\n  [(\"preprocessing\", ct),\n  (\"linear_regression\", LinearRegression())]\n)\nThen, we cross-validate:\ncross_val_score(lr_pipeline_1, X, y, cv = 5, scoring = 'r2')\n\narray([-1.00227561e+21, -2.13473460e+19, -4.65481157e+21, -4.24892786e+21,\n       -4.16001805e+22])\nOof. This is terrible! We used so many features in our model, we overfit to the training data, and ended up with terrible predictions.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Penalized Regression</span>"
    ]
  },
  {
    "objectID": "13-penalized_regression.html#introduction",
    "href": "13-penalized_regression.html#introduction",
    "title": "14  Penalized Regression",
    "section": "",
    "text": "Check In\n\n\n\nNotice in the Column Transformer, we added the argument handle_unknown='ignore'What did this accomplish?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nNotice in the OneHotEncoder, we used make_column_selector(dtype_include=object) instead of supplying column names. What did this accomplish?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nWhy did we drop the Order and PID columns?",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Penalized Regression</span>"
    ]
  },
  {
    "objectID": "13-penalized_regression.html#ridge-regression",
    "href": "13-penalized_regression.html#ridge-regression",
    "title": "14  Penalized Regression",
    "section": "14.2 Ridge Regression",
    "text": "14.2 Ridge Regression\nIn the previous analysis, we fit our model according to Least Squares Regression. That is, we fit a model that predicts the \\(i\\)-th house price with the equation\n\\[\\hat{y}_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots +  \\beta_p x_{ip}\\] When we chose the best numbers to plug in for \\(\\beta_1\\) … \\(\\beta_p\\), we did so by choosing the numbers that would minimize the squared error in the training data:\n\\[\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\]\nBut this equation is not our only way to define the “best” \\(\\beta\\)’s - and in this case, fitting as closely as possible to the training data did not result in good future predictions.\nRecall that modeling is a kind of “teeter totter” between models that are too inflexible (underfit) and those that are too flexible (overfit). In this instance, we are overfitting, so we need our model to be less flexible; i.e., less able to get too close to the training data.\nHow will we do this? Regularization - a fancy word that says we will add a new piece to our loss function that restricts the flexibility of the \\(\\beta\\)’s. We call these new pieces penalties.\nFor example, what if our definition of “best beta” was the ones that minimize:\n\\[ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\sum_{j = 1}^p \\beta_j^2 \\] Here, the \\(\\sum \\beta_j^2\\) is a Ridge penalty, another name for the sum of squared coefficients.\nNow our loss function - which defines the best betas - has two concerns:\n\nTo make the Sum of Squared Error (SSE) small\nTo make the betas themselves small\n\nThese two concerns are acting in opposition to each other: We can make the SSE small by choosing \\(\\beta\\)’s that overfit; or we can make the Ridge penalty small by choosing \\(\\beta\\)’s near 0, but we can’t do both at the same time.\nTo decide how much we want to prioritize each concern, we’ll throw a number in called \\(\\lambda\\):\n\\[ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j = 1}^p \\beta_j^2 \\]\nNow, we have a “knob” we can use to balance the pieces of the loss function. If \\(\\lambda\\) is very large, then we care much more about restricting our coefficients to small values than about getting small SSE. If \\(\\lambda\\) is close to 0, then we care much more about SSE.\nWhen \\(\\beta\\)’s are chosen according to the above loss function, instead of just considering SSE, we call this Ridge Regression.\n\n\n\n\n\n\nPractice Activity\n\n\n\nMake a pipeline that uses all the variables in the Ames dataset, and then fits Ridge Regression with \\(\\lambda = 1\\).\nCross-validate this pipeline and compare the results to the ordinary linear regression.\nThen fit the model on the whole dataset and get the coefficients. Make a plot of these coefficients compared to the ones from ordinary linear regression.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe sklearn function Ridge() uses the argument name alpha for \\(\\lambda\\).\n\n\n\n14.2.1 Tuning\nNow, you might be wondering, how do we know what value to use for \\(\\lambda\\)? That is, how should we prioritize between the two concerns?\nWell, we don’t really know! But what we can do is try many different values of \\(\\lambda\\) and see which one results in the best metrics.\nThat is, we can tune the hyperparameter \\(\\lambda\\) to pick a final value - not because we are interested in the value itself, but because it will tell us the best model specification (i.e., the best loss function) to use.\n\n\n\n\n\n\nPractice Activity\n\n\n\nUsing the same pipeline as previously, perform tuning on \\(\\lambda\\).\nYou should always try \\(\\lambda\\) values on a log scale; that is, don’t use [1,2,3,4]; instead use something like [0.001, 0.01, 0.1, 1, 10]",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Penalized Regression</span>"
    ]
  },
  {
    "objectID": "13-penalized_regression.html#lasso",
    "href": "13-penalized_regression.html#lasso",
    "title": "14  Penalized Regression",
    "section": "14.3 LASSO",
    "text": "14.3 LASSO\nThis idea of adding a penalty to the loss function is very powerful - and the Ridge penalty is not the only one we could have used.\nAnother common choice is the LASSO (least absolute shrinkage and selection operator) penalty, which regularizes the \\(\\beta\\)’s according to their absolute value instead of the square.\n\\[ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j = 1}^p |\\beta_j| \\]\nThis difference should feel fairly trivial, but it has some surprisingly different effects on the resulting coefficients! While Ridge Regression will result in coefficients that are generally smaller than OLS, LASSO Regression will cause some coefficients to be equal to 0.\n\n\n\n\n\n\nJust our opinion...\n\n\n\nThe mathematical reasons for this difference are very deep. We wish we had time to derive them in this class! But for the time being, you’ll just have to trust us that the small change from square to absolute value is all it takes to produce the different results you are about to see…\n\n\nThis outcome, where some of the \\(\\beta\\)’s are fully eliminated from the model, is very powerful. It essentially performs automatic feature selection; the predictors whose coefficients became zero are the ones that were not needed to get better predictive power.\n\n14.3.1 Your Turn\n\n\n\n\n\n\nPractice Activity\n\n\n\nCreate a LASSO pipeline, and tune \\(\\lambda\\).\nFit your best model on the full Ames data, and compare the coefficients to Ridge and OLS\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe sklearn function Lasso() uses the argument name alpha for \\(\\lambda\\).\n\n\n\n\n14.3.2 Elastic Net\nIf Ridge regression is one way to improve OLS in overfit cases, and LASSO regression is another, why not have the best of both worlds?\nThere is no reason not to add both penalties to our loss function:\n\\[ \\ell(\\beta) = SSE + \\lambda \\left(\\sum_{j = 1}^p \\beta^2 + \\sum_{j = 1}^p |\\beta_j| \\right)\\]\nOf course, we now have three concerns to balance: The SSE, the Ridge penalty, and the LASSO penalty. So in addition to \\(\\lambda\\), which balances SSE and penalties, we need to add in a number \\(\\alpha\\) for how much we care (relatively) about Ridge vs LASSO:\n\\[ \\ell(\\beta) = SSE + \\lambda \\left(\\alpha \\sum_{j = 1}^p \\beta^2 + (1-\\alpha) \\sum_{j = 1}^p |\\beta_j| \\right)\\]\nBut what is the best choice of \\(\\alpha\\)? I think you can see where this is headed…\n\n\n14.3.3 Your Turn\n\n\n\n\n\n\nPractice Activity\n\n\n\nCreate an Elastic Net pipeline, and tune \\(\\lambda\\) and \\(\\alpha\\).\nFit your best model on the full Ames data, and compare the coefficients to Ridge and OLS.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe sklearn function ElasticNet() uses the argument name alpha for \\(\\lambda\\), and the argument name l1_ratio for \\(\\alpha\\).\nYes, this is confusing and annoying! But since essentially all of the literature uses \\(\\lambda\\) and \\(\\alpha\\) the way we have used them in this chapter, we are choosing to be consistent with that notation instead of sklearn’s argument names.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Penalized Regression</span>"
    ]
  },
  {
    "objectID": "13-penalized_regression.html#wrap-up",
    "href": "13-penalized_regression.html#wrap-up",
    "title": "14  Penalized Regression",
    "section": "14.4 Wrap-Up",
    "text": "14.4 Wrap-Up\nIn this chapter, you may feel a bit tricked! We promised you new model specifications, but actually, we still haven’t changed our prediction approach from a simple linear model:\n\\[\\hat{y}_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots +  \\beta_p x_{ip}\\]\nIt’s important to remember, though, that how you fit the model to data is every bit as important a piece of the model specification decision as the equation itself.\nBy changing our definition of “best” coefficients - that is, changing the loss function that our ideal \\(\\beta\\)’s would minimize - we were able to massively impact the resulting prediction procedure.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Penalized Regression</span>"
    ]
  },
  {
    "objectID": "14-nonparametric_methods.html",
    "href": "14-nonparametric_methods.html",
    "title": "\n15  Nonparametric Methods\n",
    "section": "",
    "text": "15.1 Introduction\nThis document discusses modeling via nonparametric methods such as k-Nearest Neighbors and decision trees, and the tools in pandas and sklearn that can assist with this. We will expand on our previous content by diving deeper into model evaluation.\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Methods</span>"
    ]
  },
  {
    "objectID": "14-nonparametric_methods.html#introduction",
    "href": "14-nonparametric_methods.html#introduction",
    "title": "\n15  Nonparametric Methods\n",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Methods</span>"
    ]
  },
  {
    "objectID": "14-nonparametric_methods.html#machine-learning-mission",
    "href": "14-nonparametric_methods.html#machine-learning-mission",
    "title": "\n15  Nonparametric Methods\n",
    "section": "\n15.2 Machine Learning Mission",
    "text": "15.2 Machine Learning Mission\nRecall that in machine learning our goal is to predict the value of some target variable using one or more predictor variables. Mathematically, we we’re in the following setup\n\\[y = f(X) + \\epsilon\\]\nwhere \\(y\\) is our target variable and \\(X\\) represents the collection (data frame) of our predictor variables. So far we’ve discussed tackling this via multiple linear regression. In this chapter we’ll introduce two nonparametric methods for estimating \\(f\\): k-Nearest Neighbors and decision trees.\n\n\n\n\n\n\nNote\n\n\n\nNonparametric methods do not involve the estimation of parameters. For example, in multiple linear regression we needed to estimate the \\(\\beta\\) coefficients. These were the model parameters. There are no such model parameteres for k-Nearest Neighbors and decision trees.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Methods</span>"
    ]
  },
  {
    "objectID": "14-nonparametric_methods.html#k-nearest-neighbors",
    "href": "14-nonparametric_methods.html#k-nearest-neighbors",
    "title": "\n15  Nonparametric Methods\n",
    "section": "\n15.3 k-Nearest Neighbors",
    "text": "15.3 k-Nearest Neighbors\nAs we alluded to in our modeling introduction chapter, the k-Nearest Neighbors model involves looking at the observations in the training dataset closest to our new observation of interest and predicting the target value based on these closest training observations. Mathematically,\n\\[\\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i\\]\nwhere \\(K\\) is the number of neighboring training data points we’re interested in using to make our prediction, and \\(N_0\\) is the neighborhood (collection) of those \\(K\\) training data points.\nThis model is averaging the value of the target variable for the \\(K\\) training observations closest to the new data point of interest as our prediction. The “closest” training observations are traditionally determined using euclidean distance (i.e. normal distance).\n\n\n\n\n\n\nCheck In\n\n\n\nAs the value of \\(K\\) increases does our k-Nearest Neighbors model become more or less complex? Are we more likely to be underfitting or overfitting our data, with a large value of \\(K\\)?\n\n\nImagine we have predictors, \\(x_1\\) and \\(x_2\\), and some target variable, \\(y\\). The following visualization describes two different k-Nearest Neighbors models:\n\n\n\n\n\n\n\nCheck In\n\n\n\nIn the visualization above, which model has the smaller value of \\(K\\)? How can you tell? What does this suggest about training and test error for a kNN model?\n\n\nAs flexible and accurate as kNN models can be, they suffer from at least the following two following characteristics:\n\nIf the dataset is large enough, then finding the closest training data points to a new observation can be computationally costly.\nThe model is not interpretable. There are no relationships between variables that get characterized by this model.\n\nWe will see later that kNN can also be used for classification problems!",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Methods</span>"
    ]
  },
  {
    "objectID": "14-nonparametric_methods.html#decision-trees",
    "href": "14-nonparametric_methods.html#decision-trees",
    "title": "\n15  Nonparametric Methods\n",
    "section": "\n15.4 Decision Trees",
    "text": "15.4 Decision Trees\nDecision trees are an extremely popular machine learning method because of their ability to capture complex relationships while also being interpretable.\nThe idea here is to stratify or segment the predictor space into a number of simple regions. Then use these regions of “like” observations to make predictions!\nBecause the set of splitting rules can be summarized in a tree, these approaches aree known as decision trees:\n\nSuppose we want to predict Salary based on Years and Hits for baseball players:\n\nThis is a fairly simple decision tree based on two variables, but you can imagine how it would just continue down for any number of variables we wanted to include as predictors.\n\n\n\n\n\n\nCheck In\n\n\n\nHow would you read the decision tree above? What do the values at the bottom represent and how would you make a prediction for a new baseball player?\n\n\nWith respect to the data itself, this is what the decision tree model is doing:\n\nThe algorithm under the hood for fitting a decision tree creates these splits in a way that decreases the error the most at each step. That is, what variable when split next will result in the biggest reduction in error?\n\n\n\n\n\n\nCheck In\n\n\n\nIf splits are created to reduce the error the most at each step, then what can be said about variables at split at the top of the tree versus lower in the tree?\n\n\nWhat would happen if we gave the model our whole training dataset and let it grow as much as it could?\nImagine a decision tree grown to the point where the bottom of the tree had a terminal node for each observation in our training dataset.\n\n\n\n\n\n\nCheck In\n\n\n\nA decision tree grown to the point of having a terminal node for each observation in our training dataset would be considered overly complex. What would the training error of such a model be? What would you expect of the test error?\n\n\nSuch an overly complex decision tree often overfits our data. Because we hope to avoid overfitting we will often penalize our decision tree fitting procedure in a way that’s similar to our discussion of penalized regression! We seek to minimize the following quantity on our training data during the fitting procedure:\n\\[\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\\]\nAs mathematical as this expression is, it boils down to this: we want to minimize the error on the training set while penalizing the fitted tree for being big (i.e. the size of the tree (\\(|T|\\)) is large). The larger we make \\(\\alpha\\) the more we penalize the tree for being big.\n\n\n\n\n\n\nCheck In\n\n\n\nWhat would the fitted decision tree look like if \\(\\alpha = 0\\)? What about \\(\\alpha = 1,000,000,000\\)?\n\n\nBesides setting some value for \\(\\alpha\\), software will also allow us to specify some minimum number of observations for each terminal node as a way to control the size of the tree. In other words, instead of letting the tree grow all the way to the point where each terminal node (leaf) represents a single training observation, they will represent a collection of training observations (min_samples_leaf in sklearn). In fact, sklearn provides multiple ways to control the size of the fitted tree!\n\n15.4.1 Advantages and Disadvantages of Decision Trees\nAdvantages:\n\nVery easy to explain/interpret\nMay more closely mirror human decision-making than other methods like multiple linear regression\nCan be displayed graphically and easily interpreted by a non-expert\n\nDisadvantages:\n\nGenerally do not have the same level of predictive accuracy as other approaches\nCan be very non-robust (i.e. small changes in the training data can cause large changes in the fit)\n\nFor these reasons, decision trees are a very popular choice for fitting in bulk (i.e. Random Forest). If we grow them to be large and fit the data very well, but do this many times then we’re able to “adjust” for the overfitting. Random forests are beyond the scope of this class, but can be pursued in future project and coursework.\nWe will see later that kNN can also be used for classification problems!\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Methods</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html",
    "href": "15-classification_intro.html",
    "title": "\n16  Introduction to Classification\n",
    "section": "",
    "text": "16.1 Introduction\nThis document discusses classification, and the tools in pandas and sklearn that can assist with this. We will expand on our previous content by diving deeper into model evaluation.\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#introduction",
    "href": "15-classification_intro.html#introduction",
    "title": "\n16  Introduction to Classification\n",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#machine-learning-mission",
    "href": "15-classification_intro.html#machine-learning-mission",
    "title": "\n16  Introduction to Classification\n",
    "section": "\n16.2 Machine Learning Mission",
    "text": "16.2 Machine Learning Mission\nRecall that in machine learning our goal is to predict the value of some target variable using one or more predictor variables. Mathematically, we we’re in the following setup\n\\[\ny = f(X) + \\epsilon\n\\]\nwhere \\(y\\) is our target variable and \\(X\\) represents the collection (data frame) of our predictor variables. So far our discussion has assumed that \\(y\\) is a quantitative variable, but what if it’s a categorical variable?\nThe nature of our predictors could be exactly the same, but now we want to predict the value of a categorical variable.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#binary-classification-with-logistic-regression",
    "href": "15-classification_intro.html#binary-classification-with-logistic-regression",
    "title": "\n16  Introduction to Classification\n",
    "section": "\n16.3 Binary Classification with Logistic Regression",
    "text": "16.3 Binary Classification with Logistic Regression\nSuppose we want to predict whether a person is going to default on their loan based on the current balance. The variable we’re trying to predict is default and the values are “Yes” or “No”, and we have a quantitative predictor in balance.\nIn the past we’ve discussed dummifying categorical variables for use in modeling efforts and we could try that again here. That is, convert the values of the default variable to 1 (default on the loan) and 0 (not default on the loan). With the values of this variable now numeric, we might try using our previous modeling techniques…such as linear regression.\n\n\nFig 1. Logistic model of Probability of Loan Default vs. Loan Balance (right).\n\nA straight-line (i.e. traditional regression) model is not appropriate for these data, as evidenced by the graph on the left. The fact that the target variable is numeric here (0 or 1) is artificial, and so it’s not appropriate to model it as though it were numeric.\n\n16.3.1 Classification\nBecause our target variable is categorical, our machine learning task is known as classification. It also means that it no longer makes sense for our error metric to involve differences between the actual value and the predicted value. We should, instead, be looking at whether our predicted value matches the actual value. In such situations, the error term often used is mis-classification rate.\n\\[\n\\frac{1}{n} \\sum_{i = 1}^n I(y_i \\neq \\hat{y}_i)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIt should be noted that the mis-classification rate will take value between 0 and 1 (i.e. 0 being when we get everything wrong, and 1 being where we get everything right). The complement (opposite) of the mis-classification rate is the overall accuracy, i.e. how well our method did.\n\n\n\nIt turns out that the classifier that minimizes this mis-classification rate on the test dataset is called the Bayes Classifier, which assigns each observation to the most likely class, given its predictor values.\nIn other words, classify a new observation to class \\(j\\) if\n\\[Pr(Y = j | X = x_0)\\]\nis the largest. As mathematical as this seems, it actually exactly matches how our intuition works!\nBecause it can be shown that the Bayes Classifier is the best, machine learning models for classification boil down to the task of estimating these probabilities well. To this end, our classification modeling techniques should give us access to predicted probabilities and not just the predicted categories themselves.\n\n16.3.2 Logistic Regression\nWhen our target variable is categorical and has only two distinct values (i.e. is binary) then logistic regression is a method often used. This approach involves modeling the log-odds of the target variable being in one category over the other, given the predictors. Mathematically, it’s something like\n\\[log(\\frac{p(X)}{1 - p(X)}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon\\]\nIn our predictive modeling context it will suffice to know that after fitting the logistic regression model, predictions should take the form of probabilities of belonging to each possible category of our target variable.\nJust like in multiple regression, we can want to penalize a logistic regression model in ways that limit the complexity of the model.\n\n\n\n\n\n\nCheck In\n\n\n\nWhat type of quantity would we want to minimize in a penalized logistic regression situation?\n\n\nThankfully, in Python, the LogisticRegression function allows for the specification of a penalty term if we wish!\n\n\n\n\n\n\nCheck In\n\n\n\nOpen the Palmer Penguins dataset and create the following gentoo variable:\n\nA value of 1 if the species variable is Gentoo\n\nA value of 0 otherwise\n\nUse the other variables in the dataset and a logistic regression model to classify penguins as either “Gentoo” or “Not”. What is your training accuracy?",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#general-classification",
    "href": "15-classification_intro.html#general-classification",
    "title": "\n16  Introduction to Classification\n",
    "section": "\n16.4 General Classification",
    "text": "16.4 General Classification\nUnfortunately, logistic regression is especially built for binary classification problems. There are ways to employ it for classification problems in which the target variable has more than two distinct values, but they’re a little ad-hoc.\nInstead we’ll turn to other methods when our target variable has more than 2 distinct values.\n\n16.4.1 kNN for Classification\nk-Nearest Neighbors is another approach for estimating the probabilities we need to do our classification. Once again, we will choose a value of \\(K\\) that determines the size of our neighborhood; i.e. the number of closest training data points we want to use. With a value for \\(K\\) chosen,\nestimate the conditional probability for category \\(j\\) as the fraction of points in the neighborhood whose response value equal \\(j\\)\nSimply put, for a new observation predict the category that is most frequent among the \\(K\\) closest training data points.\nThe kNN classifier can come surprisingly close to the optimal Bayes Classifier. However, the performance can be drastically affected by the choice of \\(K\\) somewhat similarly to the regression situation.\n\n\nFig 2. kNN Classification Example\n\n\n\n\n\n\n\nCheck In\n\n\n\nIn the graph above, has either model overfit the data? Underfit? How can you tell?\nWhat would be your general recommendations for values of \\(K\\) to use when using kNN for classification?\n\n\n\n16.4.2 Decision Trees for Classification\nThe use of decision trees for classification is VERY similar to their use for regression (i.e. numerical target variable).\n\n\nFig 3. Classification Tree example\n\n\n\n\n\n\n\nCheck In\n\n\n\nIn the graph above, we see the same type of variable splitting from the top to the bottom. However, at the bottom of the tree we have “Yes” and “No” values instead of average numeric values. What do these values represent? How are they arrived at?\nHow are we estimating and using our probabilities of interest using a decision tree?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nOpen the Palmer Penguins dataset.\nUse the other variables in the dataset and a decision tree model to classify the penguins using the species as the target variables.\nWhat is your training accuracy?",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#validating-classification-models",
    "href": "15-classification_intro.html#validating-classification-models",
    "title": "\n16  Introduction to Classification\n",
    "section": "\n16.5 Validating Classification Models",
    "text": "16.5 Validating Classification Models\nSo far we’ve been evaluating our models using mis-classification rate, or alternatively overall accuracy. As a small, motivating example for the rest of this section, consider the following situation.\nOur target variable is binary and our dataset consists of the following: 95% class 1 and 5% class 2.\nIf we built a classifier that just called everything in our training dataset class 1 then our model would have a 95% training accuracy…which sounds really good!\nHowever, we will have gotten everything that was class 2 wrong!\nIn many situations we hope that our mistakes (errors) are distributed across all categories of our target variable and are not exclusive to, say, one category as in the previous example. In some situations we do want to prioritize not making certain mistakes over making others. Because of this there are other metrics of interest beyond just overall accuracy when it comes to classification.\n\n16.5.1 Confusion Matrices, Precision, and Recall\nIn classification problems, a confusion matrix is a convenient way to organize our results as a two-way table of our predicted categories versus the actual categories.\nIn general the orientation of this table is not important, but the confusion_matrix function in sklearn will organize our results with the actual categories along the rows of the table and the predicted categories along the columns of the table. In this way, we can see exactly what kind of mistakes are being made!\n\n\n\n\n\n\nCheck In\n\n\n\nFor the species classification model (kNN or decision tree) of the Palmer Penguins data, construct a confusion matrix of your results on the training data.\nWhat kinds of errors are most prevalent?\n\n\n\n16.5.1.1 Precision\nPrecision is defined as the proportion of positive identifications that are actually correct. In other words,\n\\[\nPrecision = \\frac{True Positives}{True Positives + False Positives}\n\\]\n\n16.5.1.2 Recall\nRecall is defined as the proportion of actual positives that are identified correctly. In other words,\n\\[\nRecall = \\frac{True Positives}{True Positives + False Negatives}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that both precision and **recall* have the same numerator; what differs is the denominator. The denominator is determined by whether we’re interested in dividing by the row total (from the confusion matrix) or the column total (of the confusion matrix), and again depends on what kinds of mistakes we want to prioritize not making…\nNo matter what kinds of mistakes we prioritize, higher values of both precision and recall are always better.\n\n\n\n\n\n\n\n\nExample\n\n\n\nSuppose we had a medical test that gave one of two results: “cancer” or “not cancer”. Would you rather the test accidentally give a result of cancer when the person didn’t have cancer than give a result of “not cancer” when the person did have cancer? Or the other way around?\nHow does this relate to precision and recall?\n\n\nIt should also be noted that both precision and recall were defined in terms of positives and negatives, which are only well defined in a binary classification situation. In a non-binary classification situation, we can still explore precision and **recall, but we would need to do so in a one-versus-all* way. For example, in the Palmer Penguins dataset, it might need to be\n\nGentoo vs. Not Gentoo\nAdelie vs. Not Adelie\nChinstrap vs. Not Chinstrap\n\n16.5.2 ROC and AUC\nSo what the heck is a ROC-AUC anyway?!\n\n\nArtwork by Cal Poly ’22 data science students.\n\n\n16.5.2.1 Sensitivity and Specificity\nSensitivity is actually another name for recall, or the true positive rate.\nSpecificity is another name for the true negative rate:\n\\[\nSpecificity = \\frac{True Negatives}{True Negatives + False Positives}\n\\]\nSimilar to precision and recall, higher values of sensitivity and specificity are always better.\n\n16.5.2.2 ROC\nOne extremely popular way to assess a classification model is via a Receiver Operating Curve, often called a ROC Curve.\n\n\nFig 4. ROC\n\nNote that the false positive rate on the x-axis is also \\(1 - Specificity\\). It’s easiest to think about Fig 4. in terms of a binary classification problem.\n\n\n\n\n\n\nCheck In\n\n\n\nWith respect to Fig 4.\n\nWhat does “Random classifier” correspond to? How would this classifier work?\nHow are these curves being constructed? That is, what does each point on these curves represent?\n\n\n\n\n16.5.2.3 AUC\nWe’ve established that better-looking classifiers are further above the diagonal line in the ROC plot, but many people have gone one step further in summarizing this visualization.\nThe area-under-the-curve (AUC) on the ROC plot gives a decent summary of the overall performance of the classifier.\n\n\n\n\n\n\nCheck In\n\n\n\nWhat is the range of values for the AUC of a classifier? What values are more desirable?\n\n\n\n16.5.3 F1 Score\nBecause classification model metrics can be so complicated, researchers have come up with a class of metrics that can summarize quantities like precision and recall into a single number. The \\(F_1\\) score is the “first” of these metrics:\n\\[\nF_1 = 2\\frac{precision \\cdot recall}{precision + recall}\n\\]\nThe \\(F_1\\) score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric. The highest possible value is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "15-classification_intro.html#conclusion",
    "href": "15-classification_intro.html#conclusion",
    "title": "\n16  Introduction to Classification\n",
    "section": "\n16.6 Conclusion",
    "text": "16.6 Conclusion\nEven in classification problems we should be doing all of our evaluation on test data!\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen up this colab notebook and make a copy.\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction to Classification</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html",
    "href": "16-linear_classifiers.html",
    "title": "\n17  Linear Classifiers\n",
    "section": "",
    "text": "17.1 Introduction\nIn this chapter, we will explore two new model types: Linear Discriminant Analysis (LDA) and Support Vector Classifiers (SVC). Both of these approaches, along with Logistic Regression from the previous chapter, share the feature of being what is called Linear Classifiers\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html#introduction",
    "href": "16-linear_classifiers.html#introduction",
    "title": "\n17  Linear Classifiers\n",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html#linear-classifiers",
    "href": "16-linear_classifiers.html#linear-classifiers",
    "title": "\n17  Linear Classifiers\n",
    "section": "\n17.2 Linear Classifiers",
    "text": "17.2 Linear Classifiers\nIn binary classification, our goal is to predict which of two categories a target variable belongs to, using one or more predictor variables.\nLinear classifiers are a general type of modeling approach that uses a linear combination of the predictors to create a score, and then assigns a class based on this score. That is, for some constants \\(w_1, ..., w_p\\) and \\(c\\),\n\\[z_i = w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_p x_{ip} + c\\] and we predict \\(y_i = 1\\) if \\(z_i \\geq 0\\), and \\(y_i = 0\\) otherwise.\nThe following picture illustrates a simple two-predictor setting. We can see that categories “A” and “B” are somewhat different, with “B” falling on the bottom right and “A” in the top left.\n\nThe purple line, \\(w_1 x_1 + w_2 x_2\\) is the equation for the score. Where the blue and the purple line cross are the exact values of \\(x_1\\) and \\(x_2\\) that would make the score equal to exactly \\(0\\).\nTherefore, the blue line is the linear separator for these two categories, based on this particular score function. Future observations that land up and to the left of this line would be predicted as “A”, and those that land down and to the right are predicted as “B”. This is also sometimes called the decision boundary.\n\n17.2.1 Fitting a linear classifier\nMuch like with ordinary linear regression, the big question we need to answer is: what values of \\(w_1, ..., w_p\\) and \\(c\\) will give us the best possible predictions? This, of course, depends on your definition of “best”.\nAs we shall see, Logistic Regression, LDA, and SVC are all linear classifiers; the only difference is that they take very different approaches to choosing the “best” \\(w\\) and \\(c\\) values.\n\n17.2.2 A bit of notation\nFor simplicity you will sometimes see the vectors \\((w_1, ..., w_p)\\) and \\((x_{i1}, ..., x_{ip})\\) written as simply \\(\\mathbf{w}\\) and \\(\\mathbf{x_i}\\). This lets us write the score \\(z_i\\) in shorthand as the “matrix product” of the two vectors:\n\\[z_i = \\mathbf{w}^T\\mathbf{x_i} + c\\] Similarly, we might sometimes define our prediction \\(y_i\\) as:\n\\[y_i = \\mathcal{I} \\{z_i \\geq 0\\}\\]\nwhere \\(\\mathcal{I} \\{\\}\\) is the indicator function, equal to 1 if the statement is true and 0 if not..",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html#logistic-regression-as-a-linear-classifier",
    "href": "16-linear_classifiers.html#logistic-regression-as-a-linear-classifier",
    "title": "\n17  Linear Classifiers\n",
    "section": "\n17.3 Logistic Regression as a Linear Classifier",
    "text": "17.3 Logistic Regression as a Linear Classifier\nRecall that in logistic regression, we wanted to model the log-odds of an observation being in a particular category. To do so, we fit the model:\n\\[\\text{log-odds} = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\] For consistency, let’s just call the log-odds of the \\(i\\)-th observation \\(z_i\\). Then we’ll call \\(\\beta_1 = w_1\\), \\(beta_2 = w_2\\), etc. and call \\(\\beta_0 = c\\).\nClearly, then, the log-odds in this case is simply a score based on a linear combination of the predictors.\nBut does it fit that \\(z_i = 0\\) is our “cutoff” for the score, i.e., our decision boundary? Yes!\nThe log-odds equation is\n\\[z_i = \\log \\left(\\frac{p_i}{1 - p_i}\\right)\\]\nwhere \\(p_i\\) is the probability of the \\(i\\)-th observation being in category 1.\nIf \\(z_i\\) is negative, that means the value inside of the log must be less than one, which means that \\(p_i &lt; (1 - p_i)\\).\nIf \\(z_i\\) is positive, that means the value inside of the log must be greater than one, which means that \\(p_i &gt; (1 - p_i)\\).\nIn other words: The cutoff \\(z_i = 0\\) is exactly the same as the cutoff \\(p_i = 0.5\\) - and it sure seems pretty reasonable for us to predict Category 1 whenever the probability is above 50%!\n\n17.3.1 Loss function\nSo, what is the definition of “best” that is used when fitting Logistic Regression? Essentially, this model takes a probabilistic approach: The “best” choices of \\(\\mathbf{w}\\) and \\(c\\) are the ones that lead to \\(p_i\\) values (probabilities) that would be most likely to produce the observed true categories. This method of model fitting is called Maximum Likelihood Estimation.\nIn this class, we will omit the mathematics of the loss function.\n\n\n\n\n\n\nCheck In\n\n\n\nOpen this Colab notebook, where you will find a dataset and instructions. Complete Section 1 (Logistic Regression).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html#linear-discriminant-analysis",
    "href": "16-linear_classifiers.html#linear-discriminant-analysis",
    "title": "\n17  Linear Classifiers\n",
    "section": "\n17.4 Linear Discriminant Analysis",
    "text": "17.4 Linear Discriminant Analysis\n\n\n\n\n\n\nWarning\n\n\n\nThere is another model - common in Machine Learning, but completely different from Linear Discriminant Analysis - called “Latent Dirichlet Allocation”. Unfortunately, both are usually abbreviated as “LDA”. Be careful when using the internet for resources; make sure you are reading about the right LDA!\n\n\nIn Linear Discriminant Analysis, we assume that the scores our observations follow two different Normal distributions. First consider a one-dimensional example: suppose we measure the cholesterol of eight people, of which five have heart disease. We might mark these eight values on a number line. Then, we might assume that these values came from some overarching Normal distribution, like this:\n\nOur idea of the “best” decision boundary, if our assumption about Normal distributions is correct, is the one that maximizes the probability of future observations falling on the “correct” side of the line.\nHow does this work with more than one predictor? Well, instead of the number line in the above illustration containing cholesterol values, imagine that it contained scores from some \\(z = \\mathbf{w}^T \\mathbf{x} + c\\). We still assume that we end up with two different Normal curves: one for each category. Then, our decision boundary would be at \\(z = 0\\).\n\n17.4.1 Loss function\nSo, how does LDA decide on a “best” choice of \\(w\\)’s and \\(c\\)? It finds the score function that creates the largest separation between the two resulting estimated Normal curves.\n\n\n17.4.2 QDA\nMathematically, in LDA we also assume that both Normal curves have the same variance. This is a pretty big assumption, but it is necessary for LDA to be a linear classifier, and it creates some mathematical conveniences for solving for the \\(w\\) and \\(c\\) values.\nA variant on LDA is called Quadratic Discriminant Analysis (QDA). In this approach, we allow the two curves to have different variances. Through some math details we won’t get into here, this creates a quadratic (non-linear) classifer; i.e., the score is given by\n\\[z_i = w_1 (x_{1i} - a_1)^2 + \\cdots + w_p (x_{pi} - a_p)^2 + c\\]\nMuch more complicated under the hood, but sometimes this can be a better classifier than a simple linear separator.\n\nFortunately, we aren’t responsible for the more complex math, and it’s easy to simply use a different model specification in python!\n\n\n\n\n\n\nCheck In\n\n\n\nOpen this Colab notebook, where you will find a dataset and instructions. Complete Section 2 (LDA).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "16-linear_classifiers.html#support-vector-classifiersmachines",
    "href": "16-linear_classifiers.html#support-vector-classifiersmachines",
    "title": "\n17  Linear Classifiers\n",
    "section": "\n17.5 Support Vector Classifiers/Machines",
    "text": "17.5 Support Vector Classifiers/Machines\nA Support Vector Classifier (SVC) takes a very non-statistical approach to choosing the “best” linear separator. The logic goes like this:\n\nThe best separating line would be one where one category is all far to one side, and the other category is all far to the other side.\nIn real data, this is not possible, so we have to “ignore” some observations that land in the “wrong” place.\n\nThe SVC model approach is to draw soft margins around the score cutoff of \\(z_i = 0\\), between \\(z_i = -1\\) and \\(z_i = 1\\). This means that we want the to choose our \\(w\\)’s so that the space between the margins is as big as possible, to keep the two groups separate - but we acknowledge that there may be some observations “caught” in the margins. The observations right at the margins are called the support vectors.\n\n\n17.5.1 Loss function\nThe way the SVC model fitting chooses the “best” \\(w\\)’s and \\(c\\) is to balance two concerns:\n\nWe want as much of our training data as possible to be on the “correct” side of the line and outside the margin.\nWe want as big of a margin as we can get.\n\nIn the loss function for SVC, we will find a tuning parameter, \\(\\lambda\\) or sometimes \\(C\\), which is the balancing of these two concerns, much like in penalized regression.\n\n17.5.2 Support Vector Machines\nAs with LDA and QDA, there is a way to “level up” an SVC model into a non-linear classifier, called a Support Vector Machine.\nThe way this works is that a nonlinear transformation function is applied to all the predictors, and then an SVC is fit. This transformation is called the kernel.\n\nSo, how do we know which kernel to use? Radial? Quadratic? Something else? We don’t really have a magic way to know. We simply try many different options and see which one produces the best metrics.\n\n\n\n\n\n\nWarning\n\n\n\nThe way the loss functions are set up for SVM and SVC require us to represent our two categories as 1 and -1 instead of 1 and 0. Some implementations, including sklearn, will handle this for you. Others will not. If you are ever getting very strange results for SVM/SVC - such as decision boundaries at positive or negative infinity - this is worth checking up on.\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nOpen this Colab notebook, where you will find a dataset and instructions. Complete Section 3 (Support Vector Classifiers) and Section 4 (Comparison).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linear Classifiers</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html",
    "href": "17-multiclass.html",
    "title": "\n18  Multiclass Classification\n",
    "section": "",
    "text": "18.1 Introduction\nThus far, we have only covered methods for binary classification - that is, for predicting between two categories.\nWhat happens when our target variable of interest contains more than two categories? For example, instead of predicting whether or not someone has heart disease, perhaps we want to predict what type of disease they have, out of three options.\nRead on to find out…\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html#introduction",
    "href": "17-multiclass.html#introduction",
    "title": "\n18  Multiclass Classification\n",
    "section": "",
    "text": "Note\n\n\n\nIf you do not have the sklearn library installed then you will need to run\npip install sklearn\nin the Jupyter/Colab terminal to install. Remember: you only need to install once per machine (or Colab session).",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html#naturally-multiclass-models",
    "href": "17-multiclass.html#naturally-multiclass-models",
    "title": "\n18  Multiclass Classification\n",
    "section": "\n18.2 Naturally Multiclass Models",
    "text": "18.2 Naturally Multiclass Models\nSome model specifications lend themselves naturally to the multiclass setting.\nLet’s take a quick look at how each of these predicts for three or more classes.\n\n18.2.1 Multiclass KNN\nRecall that in a binary setting, KNN considers the “votes” of the \\(K\\) most similar observations in the training set to classify a new observation.\nIn a multiclass setting, nothing changes! KNN still considers the “votes” of the closest observations; we simply now have votes for more than two options.\n\n18.2.2 Multiclass Trees\nSimilarly, in a binary setting, Decision Trees assign new observations to the class that is most common in the node/leaf (or “bucket”) that they land in.\nThe same is true for the multiclass setting. However, it’s important to remember that the splits in tree itself were chosen automatically during the model fitting procedure to try to make the nodes have as much “purity” as possible - that is, to have mostly one class represented in each leaf. This means the fitted tree for a two-class prediction setting might look very different from the fitted tree for a three-class setting!\n\n18.2.3 Multiclass LDA\nIn the binary setting, LDA relies on the assumption that the “scores” (linear combinations of predictors) for observations in the two classes were generated from two Normal distributions with different means. After using the training data to pick a score function and estimate means, we then assign new predictions to the class whose distribution would be most likely to output that data.\nInstead of two Normal distributions, we can easily imagine three or more! We still use the observed data to pick a score function and then approximate the means and standard deviations of the Normal distributions, and we still assign new predictions to the “most likely” group.\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen this Colab notebook. Fit a multiclass KNN, Decision Tree, and LDA for the heart disease data; this time predicting the type of chest pain (categories 0 - 3) that a patient experiences. For the decision tree, plot the fitted tree, and interpret the first couple splits.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html#multiclass-from-binary-classifiers",
    "href": "17-multiclass.html#multiclass-from-binary-classifiers",
    "title": "\n18  Multiclass Classification\n",
    "section": "\n18.3 Multiclass from Binary Classifiers",
    "text": "18.3 Multiclass from Binary Classifiers\nSome models simply cannot be easily “upgraded” to the multiclass setting. Of those we have studied, Logistic Regression and SVC/SVM fall into this category.\nIn Logistic Regression, we rely on the logistic function to transform our linear combination of predictors into a probability. We only have one “score” from the linear combination, and we can only turn it into one probability. Thus, it only make sense to fit this model to compare two classes; i.e., to predict the “probability of Class 1”.\nIn SVC, our goal is do find a separating line that maximizes the margin to the two classes. What do we do with three classes? Find three separating lines? But then which margins do we look at? And which classes do we measure the margins between? There is no way to define our “model preferences” to include “large margins” in this setting.\nSo, how do we proceed? There are two approaches to using binary classification models to answer multiclass prediction questions…\n\n18.3.1 One vs. Rest (OvR)\nThe first approach is to try to target only one category at a time, and fit a model that can extract those observations from the rest of them. This is called “One vs Rest” or OvR modeling.\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen this Colab notebook. Create a new column in the ha dataset called “cp_is_3”, which is equal to 1 if the cp variable is equal to 3 and 0 otherwise.\nThen, fit a Logistic Regression to predict this new target, and report the F1 Score.\nRepeat for the other three cp categories. Which category was the OvR approach best at distinguishing?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\nYour four OvR Logistic Regressions produced four probabilities for each observation: prob of cp_is_0, prob of cp_is_1, etc.\nIs it guaranteed that these four probabilities add up to 1? Why or why not?\n\n\n\n18.3.2 One vs. One (OvO)\nThe second approach is to try to fit a model that are able to separate every pair of categories. This is called “One vs One” or OvO modeling.\n\n\n\n\n\n\nPractice Activity\n\n\n\nOpen this Colab notebook. Reduce your dataset to only the 0 and 1 types of chest pain.\nThen, fit a Logistic Regression to predict between the two grousp, and report the ROC-AUC.\nRepeat comparing category 0 to 2 and 3. Which pair was the OvO approach best at distinguishing?\n\n\n\n\n\n\n\n\nCheck In\n\n\n\n\nWhy do you think we reported ROC-AUC instead of F1 Score this time?\nYour three OvO Logistic Regressions produced four probabilities for each observation: prob of 0 compared to 1, prob of 0 compared to 2, and prob of 0 compared to 3. Is it guaranteed that these four probabilities add up to 1? Why or why not?\nIf we had done all the OvO pairs, how many regressions would we have fit?\nHow would you use the results of all the OvO pairs to arrive at one final class prediction?\n\n\n\n\n18.3.3 How to choose\nIn general, the OvO approach is better because:\n\nIt gives better predictions. Distinguishing between individual groups gives more information than lumping many (possibly dissimilar) groups into a “Rest” category.\nIt gives more interpretable information. We can discuss the coefficient estimates of the individual models to figure out what patterns exist between the categories.\n\nHowever, the OvR might be preferred when:\n\nYou have many categories. Consider a problem with 10 classes to predict. In OvR, we then need to fit 10 models for each specification. In OvO, we need to fit 45 different models for each specification!\nYou are interested in what makes a single category stand out. For example, perhaps you are using these models to understand what features define different bacteria species. You are not trying to figure out how Bacteria A is different from Bacteria B or Bacteria C specifically; you are trying to figure out what makes Bacteria A unique among the rest.\nYou have “layers” of categories. For example, in the heart attack data, notice that Chest Pain category 0 was “asymptomatic”, aka, no pain. We might be most interested in learning what distiguishes no pain (0) from yes pain (“the rest”); but we still are secondarily interested in distinguishing the three pain types.",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html#metrics-and-multiclass-estimators",
    "href": "17-multiclass.html#metrics-and-multiclass-estimators",
    "title": "\n18  Multiclass Classification\n",
    "section": "\n18.4 Metrics and Multiclass Estimators",
    "text": "18.4 Metrics and Multiclass Estimators\nRecall that in the binary setting, we have two metrics that do not change based which class is considered “Class 1” or the “Target Class”:\n\naccuracy: How many predictions were correct\nROC-AUC: A measure of the trade-off for getting Class 1 wrong or Class 0 wrong as the decision boundary changes.\n\nWe also have many metrics that are asymmetrical, and are calculated differently for different target classes:\n\nprecision: How many of the predicted Target Class were truly from the Target Class?\nrecall: How many of the true Target Class observations were successfully identified as Target Class?\nF1 Score: “Average” of precision and recall.\nF2 Score: “Average” of precision and 2*recall.\n\nNow that we are in the multiclass setting, we can think of precision, recall, and F1 Score as “OvR” metrics: They measure the model’s ability to successfully predict one category of interest out of the pack.\nWe can think of ROC-AUC as an “OvO” metric: It measures the model’s trade-off between success for two classes.\nOnly accuracy is truly a multiclass metric!\n\n\n\n\n\n\nCheck In\n\n\n\nIf you randomly guess categories in a two-class setting by flipping a coin, how often do you expect to be right?\nIf you randomly guess categories in a six-class setting by rolling a die, how often do you expect to be right?\nWhat does this tell you about what you should consider a “good” accuracy for a model to achieve in multiclass settings?\n\n\n\n18.4.1 Macro and micro\nSo, if we want to use a metric besides accuracy to measure our model’s success, what should we do? Three options:\n\nWe look at the micro version of the metric: we choose one category that is most important to us to be the target category, and then we measure that. Realistically, we only really report micro metrics to summarize how well we can predict each individual category. We don’t use them to select between models - because if our definition of “best” model is just the one that pulls out the target category, why are we bothering with multiclass in the first place?\nWe look at the macro version of the metric: the average of the micro versions across all the possible categories. This is the most common approach; you will often see classification models measured by f1_macro.\nWe look at a weighted average of the micro metrics. This might be useful if there is one category that matters more, but we still care about all the categories. (Such as in the cp variable, where we care most about distinguishing 0 from the rest, but we still want to separate 1-3.)",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  },
  {
    "objectID": "17-multiclass.html#conclusion",
    "href": "17-multiclass.html#conclusion",
    "title": "\n18  Multiclass Classification\n",
    "section": "\n18.5 Conclusion",
    "text": "18.5 Conclusion\nThere are many reasons why it’s important for a data scientist to understand the intuition and motivation behind the models they use, even if the computation and math are taken care of by the software.\nMulticlass classification is a great example of this principle. What if we had just chucked some multiclass data into all our classification models: KNN, Trees, Logistic, LDA, QDA, SVC, and SVM. Some models would be fine, while others would be handling the multiclass problem in very different ways than they handle binary settings - and this could lead to bad model fits, or worse, incorrect interpretations of the results!",
    "crumbs": [
      "Machine Learning with Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiclass Classification</span>"
    ]
  }
]