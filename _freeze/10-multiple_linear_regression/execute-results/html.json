{
  "hash": "75486c90755ed830b05d1c7f85e065b1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multiple Linear Regression\"\n---\n\n## Introduction\n\nThis document discusses modeling via *multiple linear regression*, and the tools in `pandas` and `sklearn` that can assist with this.\n\n::: callout-note\nIf you do not have the `sklearn` library installed then you will need to run\n\n`pip install sklearn`\n\nin the Jupyter/Colab terminal to install. **Remember:** you only need to install once per machine (or Colab session).\n:::\n\n::: {#7b2e7c1c .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n```\n:::\n\n\n## Machine Learning Mission\n\nRecall that in machine learning our goal is to **predict** the value of some *target* variable using one or more *predictor* variables. Mathematically, we we're in the following setup\n\n$$y = f(X) + \\epsilon $$\n\nwhere $y$ is our target variable and $X$ represents the collection (data frame) of our predictor variables. To **predict** $y$ well we need to estimate $f$ well. We will see many different ways to estimate $f$ including those methods mentioned in our previous modeling introduction:\n\n* Linear Regression\n\n* k-Nearest Neighbors\n\n* Decision Trees\n\nFor the sake of completeness, the $\\epsilon$ in the equation above represents random error that is independent of $X$ and has mean zero. Our focus will be on using our predictors ($X$) and good construction of our estimate of $f$ to make accurate predictions of $y$ for new data.\n\n## Simple Linear Regression\n\nOur estimate of $f$ will eventually take on very complicated forms, but one of the simplest estimates is a straight line using a single predictor:\n\n$$y = \\beta_0 + \\beta_1 X_1 + \\epsilon$$\n\nThis is called **simple linear regression** and is an especially good place to start in our predictive modeling journey for many reasons, but in particular because our data and model are visualizable!\n\nConsider the following data in which we'd like to predict `Sales` from `TV` (i.e. the amount spent on TV advertising for a particular product).\n\n![Fig 1. Sales vs. TV.](images/SalesVSTV.jpeg){width=\"50%\"}\n\nSuppose we want to use a straight-line model to predict `Sales` from `TV`, i.e. fit a simple linear regression model to these data.\n\n:::{.callout-check-in .icon}\n\nHow do we use our data here to estimate the values of $\\beta_0$ and $\\beta_1$ in our simple linear regression model?\n\n:::\n\nRecall that the vertical distance between a point and our simple linear regression model is called the *residual* for that observation (i.e. observed `Sales` minus predicted `Sales`).\n\n![Fig 2. Sales vs. TV with residuals.](images/SalesVSTV-Residuals.png){width=\"50%\"}\n\n:::{.callout-note collapse=\"true\"}\n\nThere are actually multiple ways to arrive at the estimates for $\\beta_0$ and $\\beta_1$, but in our machine learning context it's most useful to think of using calculus (you don't need to know this calculus) to find the values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the **sum of squared residuals**:\n\n$$\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n\nwhere $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i$.\n\n:::\n\nIn your other linear regression-related experiences you may have been introduced to some technical conditions (assumptions) associated with fitting a linear regression model to data:\n\n1. **L**inearity (the relationship between `y` and `x` is indeed linear)\n\n2. **I**independence (of the errors)\n\n3. **N**ormality (of the errors)\n\n4. **E**qual variance (of the errors)\n\n:::{.callout-note}\n\n**None** of these technical conditions were necessary to do the calculus of finding the values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared residuals!\n\nThese linear regression models can be fit by simply minimizing an error metric (e.g. sum of squared residuals). \n\nThe technical conditions above are **necessary** if we want to do *inference* about the model and its coefficients. That is, if we want to run hypothesis test(s) about the significance of predictors or the values of their coefficients then the technical conditions need to satisfied.\n\nOur course, and treatment of machine learning, will make use of other model evaluation techniques. So, for the most part, we will not worry about these technical conditions.\n\n:::\n\n## Polynomial Regression\n\nOne of the simplest ways to extend simple linear regression is to replace our straight-line model\n\n$$y = \\beta_0 + \\beta_1 X_1 + \\epsilon$$\n\nwith a polynomial function\n\n$$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\cdots + \\beta_d X_1^d + \\epsilon$$\n\nWe can still estimate the coefficients with the **least squares** method described above for simple linear regression. **Note** that we're still only using a single predictor variable here; we've added polynomial terms of that predictor variable. This can be useful if the relationship between `y` and `x` is not linear.\n\n:::{.callout-note}\n\nAdding polynomial terms to our model is just one way to transform (and augment) our data set in a way that can improve our modeling and predictive efforts. In general, transforming variables in our dataset is a very common data wrangling strategy that can take place multiple times throughout modeling. The following are a few other ways variables can be transformed:\n\n* Standardize numeric variables (i.e. transformed to have mean 0 and standard deviation 1)\n\n* Dummifying categorical variables (i.e. creating 0-1 variables out of text variables so they can be used in a model)\n\n* Take `log` of numeric variables\n\n* Discretize/categorize numeric variables\n\n:::\n\nConsider the following set of models fit to the `Sales` dataset from above:\n\n\n![Fig 3. Sales vs. TV with polynomial models.](images/SalesPoly.jpeg){width=\"50%\"}\n\n:::{.callout-check-in .icon}\n\nWhich model seems to fit the `Sales` data best? Why?\n\nWhat is less good about the other models?\n\n:::\n\nLet's establish some official language for your answers to the previous question! \n\n### Underfitting\n\n**Underfitting** is the scenario in which a model is unable to capture the relationship between the input(s) and the output variable accurately.\n\n:::{.callout-check-in .icon}\n\nDo you think any of the models in Fig 3. are underfitting the data here? If so, which ones and why?\n\n:::\n\n### Overfitting\n\n**Overfitting** is the scenario in which a model captures *too much* about the data its being trained on. That is, model is capturing the relationship between the input(s) and the output, but **ALSO** some of the noise or nuance present in the data.\n\n:::{.callout-check-in .icon}\n\nDo you think any of the models in Fig 3. are overfitting the data here? If so, which ones and why?\n\n:::\n\n### Under/Overfitting and Our ML Mission\n\nRemember that our goal is to estimate $f$ in **in a way that allows us to make accurate predictions for new data**. In both the underfitting and the overfitting situations, we have model that will not generalize well (i.e. not make good predictions on new data), albeit in different ways. Use this idea about generalizability to comment one more time:\n\n:::{.callout-check-in .icon}\n\n![Fig 4. MPG vs Horespower with polynomial models.](images/AutoPoly.jpeg){width=\"50%\"}\n\nWhich of these models are underfitting? Overfitting? Why?\n\n:::\n\n:::{.callout-warning}\n\nIn Python, all of our data wrangling and variable preparation (e.g. transformations) needs to happen in the creation of `y` and `X` before any models get fit.\n\nFor example, if we wanted to fit a degree 3 polynomial model to our data then we would need to create columns in `X` for the squared and cubic terms in our model:\n\n::: {#19db6c27 .cell execution_count=2}\n``` {.python .cell-code}\nX[\"x_sq\"] = X[\"x\"]**2\nX[\"X_cube\"] = X[\"x\"]**3\n```\n:::\n\n\n:::\n\n## Multiple Linear Regression\n\nIt's almost always the case that we have more than one predictor variable in our dataset. To estimate $f$ in the best possible way we usually want to take advantage of everything we have access to. Extending our simple linear and polynomial regression models to this **multiple linear regression** model is straightforward!\n\n$$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$$\n\nwhere $X_1$ represents the first predictor variable and so on. Coefficients are estimated in the same exact way! Minimize the sum of squared residuals:\n\n$$\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n\nbut now $\\hat{y}_i$ is based on the multiple linear regression model above.\n\nWe just blew this regression modeling wide open! We should now consider the inclusion of variables of any type within our model. If we think it can help in the prediction effort, then we should consider including it in our model.\n\n<!-- ![](https://media.giphy.com/media/4cUCFvwICarHq/giphy.gif) -->\n\n### Dummifying Categorical Variables\n\nUnfortunately (?), `sklearn` in Python cannot handle character variables in our input dataset (`X`). We still want to make use of character-based, categorical variables in our modeling efforts. So, we'll need to **code** or **dummify** them. \n\n:::{.callout-example .icon}\n\nSuppose we have a character variable in our dataset with \"Yes\" and \"No\" values. This variable could be **dummified** by creating a new variable whose value is 1 if the original variable's value was \"Yes\" and 0 if the original variable's value was \"No\". \n\nIn this case, the original variable had **two** distinct values (\"Yes\" and \"No\"), which required a **single** new variable to encode that information. For most of our modeling techniques this will be the case: we need **n-1** new variables to encode the information from a variable with **n** distinct values.\n\n:::\n\nThankfully, there exist Python functions to help us **dummify** variables without having to do this by hand ourselves. There are at least two such functions:\n\n* `OneHotEncoder` in the `sklearn` library\n\n* `get_dummies` in the `pandas` library\n\n:::{.callout-check-in .icon}\n\nApply both the `OneHotEncoder` and `get_dummies` functions to the `species` variable in the Palmer Penguins dataset. Observe the results and discuss the differences, if there are any.\n\n:::\n\n### Standardizing Quantitative Variables\n\nData are not always nicely behaved. Many machine learning techniques greatly benefit from quantitative variables that do not contain extreme values and are nicely shaped. One way to help ensure this is to **standardize** our quantitative predictors of interest.\n\nTo **standardize** a quantitative variable means to subtract the mean from all values and divide by the standard deviation. The resulting variable will still contain useful information, but have been transformed to have mean 0 and standard deviation 1.\n\n\nThankfully, once again, there is a Python function that will assist us with this: `StandardScaler` in the `sklearn` library.\n\n:::{.callout-check-in .icon}\n\nApply the `StandardScaler` function to the `bill_length_mm` variable in the Palmer Penguins dataset. Observe the results. Did you overwrite the original variable or create a new one? If the former, are you able to get back to the original variable if you wanted to?\n\n:::\n\n:::{.callout-practice-exercise .icon}\n\nOpen up [this colab notebook](https://colab.research.google.com/drive/13tYScdRF8__2JDh9YKEcq9eo0h3RXXrK?usp=sharing) and make a copy.\n\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.\n\n:::\n\n",
    "supporting": [
      "10-multiple_linear_regression_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}