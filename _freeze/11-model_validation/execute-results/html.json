{
  "hash": "a23ce0feecf13a49e4e32b29ff82c07e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Model Validation\"\n---\n\n## Introduction\n\nThis document discusses modeling via *multiple linear regression*, and the tools in `pandas` and `sklearn` that can assist with this. We will expand on our previous content by diving deeper into model evaluation.\n\n::: callout-note\nIf you do not have the `sklearn` library installed then you will need to run\n\n`pip install sklearn`\n\nin the Jupyter/Colab terminal to install. **Remember:** you only need to install once per machine (or Colab session).\n:::\n\n::: {#2561dfed .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n```\n:::\n\n\n## Machine Learning Mission\n\nRecall that in machine learning our goal is to **predict** the value of some *target* variable using one or more *predictor* variables. Mathematically, we we're in the following setup\n\n$$y = f(X) + \\epsilon $$\n\nwhere $y$ is our target variable and $X$ represents the collection (data frame) of our predictor variables. So far we've discussed tackling this via **multiple linear regression**.\n\n## Multiple Linear Regression\n\nRecall that we've discussed the following model specification:\n\n$$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$$\n\nwhere $X_j$ ($j = 1,...,p$) can represent any type of predictor (or transformed predictor) variable in our dataset.\n\nFor most of our machine learning methods we'll want to include every variable in our dataset that we consider a predictor. So, **multiple linear regression** is also our way of setting the stage for the rest of our machine learning journey. Dataset preparation for other machine learning methods will mostly be the same as we've discussed for multiple linear regression (e.g. dummifying categorical variables, standardizing quantitative variables, etc.).\n\n:::{.callout-check-in .icon}\n\nLet's try, once again, to predict `bill_depth_mm` in the Palmer Penguins dataset. However, you should use all available predictors in the dataset. Train your model on the entire dataset and compute the sum of the squared residuals for your fitted model.\n\n:::\n\n**Is your multiple regression model good?!** \n\nWe discussed before that we could compute the Mean Squared Error as one particular metric of interest. However, the magnitude of the value of the mean squared error for a single model specification is meaningless. There is no universal threshold to compare this value to. \n\nOne metric that *does* have more standardized values is **R-squared**, the coefficient of determination. This metric takes value between 0 and 1, with values closer to 1 indicating a better fit.\n\n:::{.callout-check-in .icon}\n\nCompute the **R-squared** value for the your full model from above and comment on its value concerning the model fit. The `sklearn` library has a function for this:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n\n:::\n\n## Model Evaluation\n\nEvaluating the quality of a model can be complex, but it's been well studied. As standardized as **R-squared** is as a metric for a model, it suffers from at least one weakness:\n\n* It is mathematically guaranteed to stay the same or increase in value if we add terms (predictors) to the model.\n\nThis is essentially saying that any new variable can't make the model any worse; it can add zero or more information, but not \"negative information\". We generally don't want to arbitrarily add predictors to our model. Bigger and more complex models are often more computationally costly, harder to interpret (if at all), and more likely to overfit our data.\n\n**But wait...**aren't we supposed to be computing metrics and evaluating our models on **test data**?! Yes!\n\nThis *guarantee* about the value of **R-squared** never decreasing is only true if computed on the training data.\n\n:::{.callout-note}\n\nRecall...\n\nTo choose between our model specifications, we will need some training data for fitting the models, and some test data for computing the metrics. How will we get two separate datasets? Easy, weâ€™ll just split up the one dataset we already have!\n\n::: {#da0a28c3 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n```\n:::\n\n\n:::\n\nWith our training and testing datasets established, we're ready to fit our models on the training data and then make predictions on the test data!\n\nOnce we have predictions on the test data we need to decide on one or more metrics to use to compare our candidate models. \n\n:::{.callout-check-in .icon}\n\nThere is a large number of metrics we can use to evaluate models of different types. Identify at least two different metrics we could use to evaluate our `bill_depth_mm` model:\n\n[https://scikit-learn.org/stable/search.html?q=metrics](https://scikit-learn.org/stable/search.html?q=metrics)\n\nSplit the penguin dataset into training and testing subsets. Fit your model to the training data and compute the value of your two metrics on the test data.\n\n:::\n\nNo matter what metrics you happened to identify, a few popular choices are:\n\n* Root Mean Squared Error (RMSE)\n* Mean Absolute Error (MAE)\n* R-Squared\n\nAll of these metrics have corresponding functions in `sklearn`.\n\n### Under/Overfitting\n\nDo you think your full model for `bill_depth_mm` underfit the data? Overfit? Was just fine? How do we even tell?!\n\n:::{.callout-note}\n\nIn general, we will organize a short list of model specifications that we want to explore and then compute the value of our metrics for each, on our test dataset. The \"winner\" is the one with the best values of the metrics on our test data. \n\n:::\n\nBut how do we assess underfit and overfit when we can't visualize our data and model as easily like we did previously with our polynomial regression models...?\n\nThe hope is that we can evaluate complex models in the following way:\n\n![Fig 1. Fit vs. Complexity](images/PolyFit_TrainTest.png){width=\"50%\"}\n\n\n:::{.callout-check-in .icon}\n\nIn the graph above, which model is underfit? Which model is overfit? Does this make sense in the right graph as well? How does the right graph help us assess under/overfit when the model isn't visualizable?\n\n:::\n\n### Suspiciously Good\n\nSuppose we fit a model to our training data and apply it to our test data and obtain extremely low values for both the training and the test error metrics...\n\n![Fig 2. Sus](images/sus.jpeg){width=\"50%\"}\n\nOf course we want our models to do well, but it's actually **fantastic** to have intuition that model performance on the test dataset is suspicious if it's seemingly **too good**. \n\nA few things to check in this situation:\n\n* Review all of your code to make sure your `X` dataframe didn't include the target variable (`y`) as well\n\n* Check to see if any of your predictor variables are \"surrogates\" (or superficially associated with) for your target variable (`y`)\n\n:::{.callout-practice-exercise .icon}\n\nOpen up [this colab notebook](https://colab.research.google.com/drive/1v-ifKFjx9ZOIkGsLMf_NsYy_r-rckoh2?usp=sharing) and make a copy.\n\nFill out the sections where indicated, render it to html with Quarto, and push your final notebook and html document to a repository on GitHub (same one as Practice Activity 1.1 is good). Then share this repository link in the quiz question.\n\n:::\n\n",
    "supporting": [
      "11-model_validation_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}