{
  "hash": "c0ff750d8c6aa6669c40b30305e5ba27",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Predictive Modeling\"\n---\n\n## Introduction\n\nIn this chapter, you will be introduced to the basic structure and philosophy of the world of **predictive modeling**, or as it is often known, **machine learning.**\n\nMachine learning in `python` is usually performed using the `SciKit-Learn` package. This library not only provides convenient functions for fitting predictive learning models, it also enforces a strict structure of workflow that will help you make responsible choices in your machine learning decisions.\n\nTo help introduce the mindset and procedure of predictive modeling, we begin with a metaphor:\n\n> You are the wealthy Baroness Von Machlearn, and you have decided to commission a painting of yourself to hang in your mansion for posterity. Of course, to fully capture your beauty, this portrait needs to be 100 feet tall at least - so you'll only be able to commission one final painting. But who shall have the honor of immortalizing you?\n\n![Fig 1. The Baroness herself.](images/baroness.jpeg){width=\"50%\"}\n\n> You being sneakily exploring your friends' houses every week at Baroness Card Club to see the portraits they have commissioned for themselves. You write down the names of these painters, who you now know are capable of decent quality portrature. (After all, the painter cannot be blamed for the hideous dress that Baroness Artificia was wearing!)\n\n![Fig 2. They didn't even notice she was gone!](images/cats_cards.jpeg){width=\"50%\"}\n\n> Then, you send each of these portrait painters a photograph of yourself and pay them to recreate it as a *miniature* portrait. You bring these portraits to your weekly Card Game, and see which one most impresses your Baroness friends.\\\n> Surely, whichever painter's minature interpretation impresses them the most, that is the painter you should hire!\n\n::: {#fig-portraits layout-ncol=3}\n\n![Painter 1\\'s submission](images/cat_portrait_2.jpeg){#fig-1} \n\n![Painter 2\\'s submission](images/cat_portrait_5.jpeg){#fig-2} \n\n![Painter 3\\'s submission](images/cat_portrait_6.jpeg){#fig-3}\n\nFig 3. Three portrait submissions. Which will be your legacy?\n:::\n\n> At Baroness Card Club, your friends are blown away by Painter 1's majestic portrait. You hire them at once to paint you in your full glory and secure your regal legacy.\n\n![Fig 4. The Baroness in her full glory.](images/cat_portrait_1.jpeg){width=\"80%\"}\n\nHow does this story relate at all to Machine Learning? Read on to find out...\n\n## Elements of a machine learning process\n\n### Predictors and Targets\n\nIn a predictive modeling setting, there is one **target variable** that we are hoping to be able to predict in the future.\n\nThis target variable could take many forms; for example it could be:\n\n-   The price that a particular house will sell for.\n\n-   The profits of a company next year.\n\n-   Whether or not a person will click a targeted ad on a website.\n\nThe goal is to come up with a *strategy* for how we will use the data we *can* observe to make a good guess about the unknown value of the *target* variable.\n\nThe next question, then, is: what data *can* we observe? The information we choose to use in our prediction strategy is called the *predictors*. For the above three target variables, some predictors might be:\n\n-   The size of the house in square feet, the neighborhood it is located in, and the number of bedrooms it has.\n\n-   The company's profits last year.\n\n-   The person's age, the person's previous search terms, the image used for the ad, and the website it is hosted on.\n\nUltimately, every machine learning model - even very complex ones - are simply procedures that take *in* some predictors and *give back* a predicted value for the target.\n\n::: {.callout-example .icon}\nFor Baroness Von Machlearn, the desired target was a beautiful portrait. Her \"predictors\" were the elements of a portrait: what dress was she wearing, what background was shown, how she styled her hair, etc.\n:::\n\n::: callout-caution\nIn machine learning, it is common to refer to **predictors** or **features** for the input and **target** or **target variable** for the output.\n\nIn computer science, you will sometimes hear these simply called **input** and **output**.\n\nIn statistics, we sometimes say **covariates** or **independent variables** for the input, and **response variable** or **dependent variable** for the output.\n\nThis book will use all the terms interchangeably, but we will mostly stick to \"predictors\" and \"target\".\n:::\n\n### Model Specifications\n\nNow, once we have identified our target variable, and decided on some predictors we will use, we need to come up with a process for making predictions.\n\nFor example, consider the problem of predicting the price a newly listed house will sell for, based on its size, neighborhood, and number of bedrooms. Here are a few different prediction strategies:\n\n-   We will find ten houses in the same neighborhood with approximately the same size and number of bedrooms. We will look at the most recent prices these ten houses sold for, and take the average of those. This is our predicted price for the new house.\n\n-   We will make an equation\n\n$$a * \\text{size} + b * \\text{neighborhood 1}  + c* \\text{neighborhood 2} + d*\\text{num bedrooms} = \\text{price}$$ We will find out what choices of $a,b,c,d$ lead to the best estimates for recently sold houses. Then, we will use those to predict the new house's price.\n\n-   We will define a few categories of houses, such as \"large, many bedrooms, neighborhood 1\" or \"medium, few bedrooms, neighborhood 2\". Then, we will see which category the new house fits into best. We will predict the price of the new house to be the average price of the ones in its category.\n\nEach of these strategies is what we call a **model specification**. We are *specifying* the procedure we intend to use to *model* the way house prices are determined.\n\n::: callout-note\nFor the curious, the examples above roughly correspond to the model specifications:\n\n-   K-Nearest-Neighbors\n\n-   Linear Regression\n\n-   Decision Trees\n\nIn this class, you will learn a few of the many different model specifications that exist.\n:::\n\n::: {.callout-example .icon}\nIn our Baroness's journey to a portrait, she considered many portrait painters. These represent her model specifications: the procedures that will turn her image into a portrait.\n:::\n\n## Choosing a final model\n\n### Training data\n\nNotice something very important about all of the examples of model specifications: They required us to know something about the prices and qualities of *other* houses, not just the one we wanted to predict. That is, to help us develop our strategy for *future prediction*, we had to rely on *past* or *known* information.\n\nThis process, by which we use known data to nail down the details of our prediction strategy, is called **fitting the models**.\n\nFor the three specifications above, the model fitting step is:\n\n-   Simply collecting information about house sizes and prices in the same neighborhood.\n\n-   Determining which exact values of $a,b,c,d$ do a good job predicting for known house prices.\n\n-   Deciding on categories of houses that seem to be priced similarly.\n\n::: {.callout-example .icon}\nTo help figure out which painters were capable of portraits, the Baroness observed portraits in her friend's homes. The painters had been *trained* on portraits of other fancy ladies.\n:::\n\n### Test Data and Metrics\n\nUltimately, we need to settle on **only one** procedure to use to come up with our prediction(s) of unknown target values.\n\nSince our goal is to choose a good strategy for *future data*, we'd like to see how our fitted models perform on *new* data. This brand-new data - which was **not** involved in the model fitting process, but for which we **do** know the true target values - is called the **test data**.\n\n::: {.callout-warning .icon}\nWhy might we not want to measure prediction success on the *training data*?\n\nConsider the model specification \"Find the most similar house in the training data, and predict that price?\" If we use this approach to make predictions about the houses *in the training data*, what will happen?\n\nIf we want to predict the price of a house in the training data, we look for the most similar house, which is... itself! So we predict the price perfectly!\n\nThis doesn't necessarily mean our *modeling approach* is good: remember, our goal here is to come up with a strategy that will work well for *future data*.\n:::\n\n::: {.callout-example .icon}\nTo help figure out which painter to ultimately hire, the Baroness sent each painter a photograph of herself. This allowed the painter to create a sample portrait, i.e., \"test data\", before they ever saw her in person.\n:::\n\nOnce we make predictions on the test data, we need to decide on a **metrics**: a measurement of prediction success of our different models on the **test data** that will help us choose the \"best\" one.\n\nA *metric* is typically an equation that can be calculated using the test data and the predictions from a fitted model.\n\nFor example, some good metrics for choosing a model to predict house prices might be **Mean Squared Error:** We gather houses whose recent sales prices are known, use our fitted model to predict prices, and find the average squared distance between the predicted price and the actual price.\n\nIn math notation, this looks like:\n\n$$ (y_1, ..., y_{100}) = \\text{actual sales prices of 100 houses}$$ $$ (\\hat{y}_1, ..., \\hat{y}_{100}) = \\text{predicted sales prices of those 100 houses}$$\n\n$$ \\text{MSE} = \\frac{1}{100} \\sum_{i = 1}^{100} (y_i - \\hat{y}_i)^2$$\n\nOf our three (or however many) model specifications, which have been fitted with training data, one will have the \"best metrics\" - the lowest MSE on the test data.\\\nThis \"winner\" is our **final model**: the modeling approach we will use to make predictions on the new house.\n\n::: {.callout-example .icon}\nAt Baroness Card Club, the other ladies gave their opinions on the candidate painters' mini portraits. Baroness Von Machlearn's *metric* was her friends' opinions, which is how she chose the painter to make the final portrait.\n:::\n\nOur last - and very important! - step is to **fit the final model**: That is, to use *all* the data we have, test and training, to re-train the winning model specification. This is the fitted model we will use on our actual future unknown data.\n\n::: {.callout-example .icon}\nAfter all this effort choosing a painter, the Baroness still needed to sit for her massive portrait!\n:::\n\n## Modeling with Scikit-learn\n\nNow, let's walk through a simple example of this predictive model workflow in python with the `scikit-learn` library.\n\nFirst, install and import `sklearn`, as well as our usual suspects:\n\n::: {#46bae5d9 .cell execution_count=1}\n``` {.python .cell-code}\nimport sklearn\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\nNext, load the example dataset we will use: the \"Ames Housing\" dataset, which contains information about house sizes and prices in Ames, Iowa.\n\n\n::: {#e23606d3 .cell execution_count=3}\n``` {.python .cell-code}\ndat = pd.read_csv(\"https://www.dropbox.com/scl/fi/yf8t1x0uvrln93dzi6xd8/housing_small.csv?rlkey=uen32y937kqarrjra0v6jaez4&dl=1\")\n```\n:::\n\n\n### Target and Predictors\n\nFirst, we will establish which variable is our **response** and which are our **predictors**. We'll call these `y` and `X`.\n\n::: {#4500368b .cell execution_count=4}\n``` {.python .cell-code}\ny = dat['SalePrice']\nX = dat[['Gr Liv Area', 'Bedroom AbvGr', 'Neighborhood_NAmes', 'Neighborhood_NWAmes']]\n```\n:::\n\n\n::: callout-caution\n**Important!** Notice that the object `y` is a **Series**, containing all the values of the target variable, while `X` is a **Data Frame** with three columns.\n\nWe often name `y` in lowercase and `X` in uppercase, to remind ourselves that `y` is one-dimensional and `X` is two-dimensional.\n\nIn general, `sklearn` functions will expect a one-dimensional target and two-dimensional object for predictors.\n:::\n\n### Model specifications\n\nOur next step is to establish which model specifications in `sklearn` we are going to consider as possible prediction procedures:\n\n::: {#9ee3071e .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\nknn = KNeighborsRegressor()\nlr = LinearRegression()\ndt = DecisionTreeRegressor()\n```\n:::\n\n\n::: callout-caution\n**IMPORTANT:** Nothing in the above code chunk mentioned the *data* at all!\n\nWe are simply preparing three objects, named `knn` and `lr` and `dt`, which we will use with the data to obtain fitted models.\n:::\n\n### Test/training split\n\nTo choose between our model specifications, we will need some *training data* for fitting the models, and some *test data* for computing the metrics. How will we get two separate datasets? Easy, we'll just split up the one dataset we already have!\n\n::: {#4ba73b6c .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n```\n:::\n\n\n::: {.callout-check-in .icon}\nTry running the above code, and looking at the objects it produces. Answer the following questions:\n\n-   How many rows are in the datasets `X_train` and `X_test`?\n\n-   How many elements are in the series `y_train` and `y_test`?\n\n-   What would you change to increase the number of rows in `X_train`?\n\n-   Run the code again, and re-examine the objects. Do they contain the exact same data as before? Why or why not?\n:::\n\n### Model fitting and Metrics\n\nNow, we are ready to put our specifications to the test.\n\nFirst we fit our models on the training data:\n\n::: {#ea0710a7 .cell execution_count=7}\n``` {.python .cell-code}\nlr_fit = lr.fit(X_train, y_train)\ndt_fit = dt.fit(X_train, y_train)\nknn_fit = knn.fit(X_train, y_train)\n```\n:::\n\n\n::: callout-caution\nThis is one of the few times you will modify an object *in place*; the `.fit()` method called on a model specification object, like `lr` or `knn`, will permanently modify that object to be the fitted version. However, for clarity, we recommend storing the fitted model under a new name, in case we re-fit the model objects later.\n:::\n\nThere isn't much worth examining in the \"model fit\" details for `knn` and `dt`, but for `lr` we might want to see what *coefficients* were chosen - i.e., what were the values of $a,b,c,d$ in our equation.\n\n::: {#1e2c3f34 .cell execution_count=8}\n``` {.python .cell-code}\nlr_fit.coef_\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([    85.62719887, -18528.22326865,  -4633.26657369,   4633.26657369])\n```\n:::\n:::\n\n\nNext, we use the fitted models to get predicted values for the *test* data.\n\n::: {#19a8378e .cell execution_count=9}\n``` {.python .cell-code}\ny_pred_knn = knn_fit.predict(X_test)\ny_pred_lr = lr_fit.predict(X_test)\ny_pred_dt = dt_fit.predict(X_test)\n```\n:::\n\n\n::: {.callout-check-in .icon}\nMake a plot of the predicted values versus the true values, `y_test`, for each of the three models. Which of the three models seems best to you?\n:::\n\nFinally, we choose a metric and compute it for the test data predictions. In this example, we'll use the MSE:\n\n::: {#7483130d .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(y_test, y_pred_knn)\nmean_squared_error(y_test, y_pred_lr)\nmean_squared_error(y_test, y_pred_dt)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n1830841337.76\n```\n:::\n:::\n\n\nThe smallest squared error was achieved by the linear regression!\n\nThus, our final model will be the linear regression model spec, fit on *all* our data:\n\n::: {#4cfb8844 .cell execution_count=11}\n``` {.python .cell-code}\nfinal_model = lr.fit(X, y)\nfinal_model.coef_\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([    74.37717581, -13083.50151923,  -7484.17760684,   7484.17760684])\n```\n:::\n:::\n\n\n## Conclusion\n\nNo practice exercise this week!\n\n",
    "supporting": [
      "09-modeling_intro_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}