{
  "hash": "b4d4b278f845f0f3dc693f2e8c7cac84",
  "result": {
    "markdown": "---\ntitle: \"Multiclass Classification\"\n---\n\n\n\n\n## Introduction\n\nThus far, we have only covered methods for **binary classification** - that is, for predicting between two categories.\n\nWhat happens when our target variable of interest contains more than two categories?  For example, instead of predicting whether or not someone has heart disease, perhaps we want to predict what *type* of disease they have, out of three options.\n\nRead on to find out...\n\n:::{.callout-note}\nIf you do not have the `sklearn` library installed then you will need to run\n\n`pip install sklearn`\n\nin the Jupyter/Colab terminal to install. **Remember:** you only need to install once per machine (or Colab session).\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n```\n:::\n\n\n## Naturally Multiclass Models\n\nSome model specifications lend themselves naturally to the multiclass setting.\n\nLet's take a quick look at how each of these predicts for three or more classes.\n\n### Multiclass KNN\n\nRecall that in a binary setting, KNN considers the \"votes\" of the $K$ most similar observations in the training set to classify a new observation.  \n\nIn a multiclass setting, nothing changes!  KNN still considers the \"votes\" of the closest observations; we simply now have votes for more than two options.\n\n### Multiclass Trees\n\nSimilarly, in a binary setting, Decision Trees assign new observations to the class that is most common in the node/leaf (or \"bucket\") that they land in.\n\nThe same is true for the multiclass setting.  However, it's important to remember that the splits in tree itself were chosen automatically during the model fitting procedure to try to make the nodes have as much \"purity\" as possible - that is, to have mostly one class represented in each leaf.  This means the fitted tree for a two-class prediction setting might look very different from the fitted tree for a three-class setting!\n\n### Multiclass LDA\n\nIn the binary setting, LDA relies on the assumption that the \"scores\" (linear combinations of predictors) for observations in the two classes were generated from two Normal distributions with different means.  After using the training data to pick a score function and estimate means, we then assign new predictions to the class whose distribution would be *most likely* to output that data.\n\nInstead of two Normal distributions, we can easily imagine three or more!  We still use the observed data to pick a score function and then approximate the means and standard deviations of the Normal distributions, and we still assign new predictions to the \"most likely\" group.\n\n\n:::{.callout-practice-exercise}\n\n[Open this Colab notebook.](https://colab.research.google.com/drive/1PK1cQML5uzsGDLSgdSGzMDqItFi696ly?usp=sharing)  Fit a multiclass KNN, Decision Tree, and LDA for the heart disease data; this time predicting the type of chest pain (categories 0 - 3) that a patient experiences.  For the decision tree, plot the fitted tree, and interpret the first couple splits.\n\n:::\n\n\n## Multiclass from Binary Classifiers\n\nSome models simply cannot be easily \"upgraded\" to the multiclass setting.  Of those we have studied, Logistic Regression and SVC/SVM fall into this category.\n\nIn Logistic Regression, we rely on the *logistic function* to transform our linear combination of predictors into a probability.  We only have **one** \"score\" from the linear combination, and we can only turn it into **one** probability.  Thus, it only make sense to fit this model to compare two classes; i.e., to predict the \"probability of Class 1\".\n\nIn SVC, our goal is do find a separating line that maximizes the **margin** to the two classes.  What do we do with three classes?  Find three separating lines?  But then which margins do we look at?  And which classes do we measure the margins between?  There is no way to define our \"model preferences\" to include \"large margins\" in this setting.\n\nSo, how do we proceed?  There are two approaches to using *binary* classification models to answer *multiclass* prediction questions...\n\n### One vs. Rest (OvR)\n\nThe first approach is to try to target only one category at a time, and fit a model that can extract those observations from the rest of them.  This is called \"One vs Rest\" or **OvR** modeling.\n\n:::{.callout-practice-exercise}\n\n[Open this Colab notebook.](https://colab.research.google.com/drive/1PK1cQML5uzsGDLSgdSGzMDqItFi696ly?usp=sharing) Create a new column in the `ha` dataset called \"cp_is_3\", which is equal to `1` if the `cp` variable is equal to `3` and `0` otherwise.\n\nThen, fit a Logistic Regression to predict this new target, and report the **F1 Score**.\n\nRepeat for the other three `cp` categories.  Which category was the OvR approach best at distinguishing?\n\n:::\n\n:::{.callout-check-in}\n\nYour four OvR Logistic Regressions produced four probabilities for each observation: prob of `cp_is_0`, prob of `cp_is_1`, etc.\n\nIs it guaranteed that these four probabilities add up to 1?  Why or why not?\n\n:::\n\n### One vs. One (OvO)\n\nThe second approach is to try to fit a model that are able to separate every pair of categories.  This is called \"One vs One\" or **OvO** modeling.\n\n:::{.callout-practice-exercise}\n\n[Open this Colab notebook.](https://colab.research.google.com/drive/1PK1cQML5uzsGDLSgdSGzMDqItFi696ly?usp=sharing) Reduce your dataset to only the `0` and `1` types of chest pain.\n\nThen, fit a Logistic Regression to predict between the two grousp, and report the **ROC-AUC**.  \n\nRepeat comparing category `0` to `2` and `3`.  Which pair was the OvO approach best at distinguishing?\n\n:::\n\n:::{.callout-check-in}\n\n* Why do you think we reported ROC-AUC instead of F1 Score this time?\n\n* Your three OvO Logistic Regressions produced four probabilities for each observation: prob of `0` compared to `1`, prob of `0` compared to `2`, and prob of `0` compared to `3`. Is it guaranteed that these four probabilities add up to 1?  Why or why not?\n\n* If we had done *all* the OvO pairs, how many regressions would we have fit?\n\n* How would you use the results of all the OvO pairs to arrive at one final class prediction?\n\n:::\n\n\n### How to choose\n\nIn general, the OvO approach is better because:\n\n* It gives better predictions.  Distinguishing between individual groups gives more information than lumping many (possibly dissimilar) groups into a \"Rest\" category.\n\n* It gives more interpretable information.  We can discuss the coefficient estimates of the individual models to figure out what patterns exist between the categories.\n\nHowever, the OvR might be preferred when:\n\n* You have many categories.  Consider a problem with 10 classes to predict.  In OvR, we then need to fit 10 models for each specification.  In OvO, we need to fit 45 different models for each specification!\n\n* You are interested in what makes a single category stand out.  For example, perhaps you are using these models to understand what features define different bacteria species.  You are not trying to figure out how Bacteria A is different from Bacteria B or Bacteria C specifically; you are trying to figure out what makes Bacteria A unique among the rest.\n\n* You have \"layers\" of categories. For example, in the heart attack data, notice that Chest Pain category 0 was \"asymptomatic\", aka, no pain.  We might be *most* interested in learning what distiguishes no pain (0) from yes pain (\"the rest\"); but we still are secondarily interested in distinguishing the three pain types.\n\n\n## Metrics and Multiclass Estimators\n\nRecall that in the binary setting, we have two metrics that do **not** change based which class is considered \"Class 1\" or the \"Target Class\":\n\n* *accuracy*: How many predictions were correct\n\n* *ROC-AUC*: A measure of the trade-off for getting Class 1 wrong or Class 0 wrong as the decision boundary changes.\n\nWe also have many metrics that are asymmetrical, and are calculated differently for different target classes:\n\n* *precision*: How many of the predicted Target Class were truly from the Target Class?\n\n* *recall*: How many of the true Target Class observations were successfully identified as Target Class?\n\n* *F1 Score*: \"Average\" of precision and recall.\n\n* *F2 Score*: \"Average\" of precision and 2*recall.\n\n\nNow that we are in the multiclass setting, we can think of precision, recall, and F1 Score as \"OvR\" metrics: They measure the model's ability to successfully predict one category of interest out of the pack.\n\nWe can think of ROC-AUC as an \"OvO\" metric: It measures the model's trade-off between success for two classes.\n\nOnly *accuracy* is truly a multiclass metric!\n\n:::{.callout-check-in}\n\nIf you randomly guess categories in a two-class setting by flipping a coin, how often do you expect to be right?\n\nIf you randomly guess categories in a six-class setting by rolling a die, how often do you expect to be right?\n\nWhat does this tell you about what you should consider a \"good\" accuracy for a model to achieve in multiclass settings?\n\n:::\n\n### Macro and micro\n\nSo, if we want to use a metric besides accuracy to measure our model's success, what should we do?  Three options:\n\n1. We look at the **micro** version of the metric: we choose one category that is most important to us to be the target category, and then we measure that.  Realistically, we only really report micro metrics to summarize how well we can predict each individual category.  We don't use them to select between models - because if our definition of \"best\" model is just the one that pulls out the target category, why are we bothering with multiclass in the first place?\n\n2. We look at the **macro** version of the metric: the average of the micro versions across all the possible categories.  This is the most common approach; you will often see classification models measured by `f1_macro`.\n\n3. We look at a **weighted average** of the micro metrics.  This might be useful if there is one category that matters more, but we still care about all the categories. (Such as in the `cp` variable, where we care most about distinguishing `0` from the rest, but we still want to separate `1-3`.)\n\n## Conclusion\n\nThere are many reasons why it's important for a data scientist to understand the intuition and motivation behind the models they use, even if the computation and math are taken care of by the software.\n\nMulticlass classification is a great example of this principle.  What if we had just chucked some multiclass data into *all* our classification models: KNN, Trees, Logistic, LDA, QDA, SVC, and SVM.  Some models would be fine, while others would be handling the multiclass problem in *very* different ways than they handle binary settings - and this could lead to bad model fits, or worse, incorrect interpretations of the results!\n",
    "supporting": [
      "17-multiclass_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}