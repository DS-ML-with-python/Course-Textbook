{
  "hash": "591636bb1ee1dd147150b4925291a4be",
  "result": {
    "markdown": "---\ntitle: \"Penalized Regression\"\n---\n\n\n## Introduction\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_selector, ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score\n```\n:::\n\n\nNow that we know how to create and use pipelines, it's finally time to start adding more **model specifications** to our repertoire.\n\nTo motivate today's new models, consider the house price prediction problem.  This time, we are going to use ALL of our predictors!\n\n\n[Click here to download the full AMES housing dataset](https://www.dropbox.com/scl/fi/g0n5le5p6fr136ggetfsf/AmesHousing.csv?rlkey=jlr9xtz1o6u5rghfo29a5c02f&dl=1)\n\n[Click here for data documentation](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n\nFirst, we'll read and clean our data..\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Read the data\names = pd.read_csv(\"data/AmesHousing.csv\")\n\n# Get rid of columns with mostly NaN values\ngood_cols = ames.isna().sum() < 100\names = ames.loc[:,good_cols]\n\n# Drop other NAs\names = ames.dropna()\n```\n:::\n\n\nThen, we'll set up our pipeline...\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX = ames.drop([\"SalePrice\", \"Order\", \"PID\"], axis = 1)\ny = ames[\"SalePrice\"]\n\n\nct = ColumnTransformer(\n  [\n    (\"dummify\", \n    OneHotEncoder(sparse_output = False, handle_unknown='ignore'),\n    make_column_selector(dtype_include=object)),\n    (\"standardize\", \n    StandardScaler(), \n    make_column_selector(dtype_include=np.number))\n  ],\n  remainder = \"passthrough\"\n)\n\nlr_pipeline_1 = Pipeline(\n  [(\"preprocessing\", ct),\n  (\"linear_regression\", LinearRegression())]\n)\n```\n:::\n\n\n:::{.callout-check-in .icon}\nNotice in the Column Transformer, we added the argument `handle_unknown='ignore'`What did this accomplish?\n\n:::\n\n:::{.callout-check-in .icon}\nNotice in the OneHotEncoder, we used `make_column_selector(dtype_include=object)` instead of supplying column names.  What did this accomplish?\n\n:::\n\n\n:::{.callout-check-in .icon}\n\nWhy did we drop the `Order` and `PID` columns?\n\n:::\n\n\nThen, we cross-validate:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncross_val_score(lr_pipeline_1, X, y, cv = 5, scoring = 'r2')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([-1.00227561e+21, -2.13473460e+19, -4.65481157e+21, -4.24892786e+21,\n       -4.16001805e+22])\n```\n:::\n:::\n\n\nOof.  This is terrible!  We used so many features in our model, we **overfit** to the training data, and ended up with terrible predictions.\n\n## Ridge Regression\n\nIn the previous analysis, we **fit** our model according to *Least Squares Regression*.  That is, we fit a model that predicts the $i$-th house price with the equation\n\n$$\\hat{y}_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots +  \\beta_p x_{ip}$$\nWhen we chose the best numbers to plug in for $\\beta_1$ ... $\\beta_p$, we did so by choosing the numbers that would minimize the **squared error** in the training data:\n\n$$\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$$\n\n\nBut this equation is not our only way to define the \"best\" $\\beta$'s - and in this case, fitting as closely as possible to the training data did not result in good future predictions.  \n\nRecall that modeling is a kind of \"teeter totter\" between models that are **too inflexible** (underfit) and those that are **too flexible** (overfit).  In this instance, we are overfitting, so we need our model to be *less* flexible; i.e., *less* able to get too close to the training data.\n\nHow will we do this? **Regularization** - a fancy word that says we will add a new piece to our loss function that restricts the flexibility of the $\\beta$'s.  We call these new pieces **penalties**.\n\nFor example, what if our definition of \"best beta\" was the ones that minimize:\n\n$$ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\sum_{j = 1}^p \\beta_j^2 $$\nHere, the $\\sum \\beta_j^2$ is a **Ridge penalty**, another name for the sum of squared coefficients.\n\nNow our loss function - which defines the best betas - has two concerns:\n\n1. To make the Sum of Squared Error (SSE) small\n\n2. To make the betas themselves small\n\nThese two concerns are acting in opposition to each other: We can make the SSE small by choosing $\\beta$'s that overfit; or we can make the Ridge penalty small by choosing $\\beta$'s near 0, but we can't do both at the same time.\n\nTo decide how much we want to prioritize each concern, we'll throw a number in called $\\lambda$:\n\n$$ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j = 1}^p \\beta_j^2 $$\n\nNow, we have a \"knob\" we can use to balance the pieces of the loss function.  If $\\lambda$ is very large, then we care much more about restricting our coefficients to small values than about getting small SSE.  If $\\lambda$ is close to 0, then we care much more about SSE.\n\nWhen $\\beta$'s are chosen according to the above loss function, instead of just considering SSE, we call this **Ridge Regression**.\n\n\n:::{.callout-practice-exercise}\n\nMake a pipeline that uses all the variables in the Ames dataset, and then fits Ridge Regression with $\\lambda = 1$.\n\nCross-validate this pipeline and compare the results to the ordinary linear regression.\n\nThen fit the model on the whole dataset and get the coefficients. Make a plot of these coefficients compared to the ones from ordinary linear regression.\n\n:::\n\n:::{.callout-warning}\nThe `sklearn` function `Ridge()` uses the argument name `alpha` for $\\lambda$.  \n:::\n\n### Tuning\n\nNow, you might be wondering, how do we know what value to use for $\\lambda$?  That is, how *should* we prioritize between the two concerns?\n\nWell, we don't really know!  But what we can do is try *many* different values of $\\lambda$ and see which one results in the best metrics.\n\nThat is, we can **tune** the **hyperparameter** $\\lambda$ to pick a final value - not because we are interested in the value itself, but because it will tell us the best **model specification** (i.e., the best loss function) to use.\n\n\n:::{.callout-practice-exercise}\n\nUsing the same pipeline as previously, perform tuning on $\\lambda$.\n\nYou should always try $\\lambda$ values on a log scale; that is, don't use `[1,2,3,4]`; instead use something like `[0.001, 0.01, 0.1, 1, 10]`\n\n:::\n\n\n\n\n## LASSO\n\nThis idea of adding a penalty to the loss function is very powerful - and the Ridge penalty is not the only one we could have used.\n\nAnother common choice is the **LASSO** (least absolute shrinkage and selection operator) penalty, which *regularizes* the $\\beta$'s according to their *absolute value* instead of the square.\n\n$$ \\ell(\\beta) = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j = 1}^p |\\beta_j| $$\n\nThis difference should feel fairly trivial, but it has some surprisingly different effects on the resulting coefficients!  While Ridge Regression will result in coefficients that are generally *smaller* than OLS, LASSO Regression will cause some coefficients to be *equal to 0*.\n\n:::{.callout-opinion}\nThe mathematical reasons for this difference are very deep.  We wish we had time to derive them in this class!  But for the time being, you'll just have to trust us that the small change from square to absolute value is all it takes to produce the different results you are about to see...\n:::\n\nThis outcome, where some of the $\\beta$'s are fully eliminated from the model, is very powerful. It essentially performs **automatic feature selection**; the predictors whose coefficients became zero are the ones that were not needed to get better predictive power. \n\n### Your Turn\n\n:::{.callout-practice-exercise}\n\nCreate a LASSO pipeline, and tune $\\lambda$.\n\nFit your best model on the full Ames data, and compare the coefficients to Ridge and OLS\n\n:::\n\n:::{.callout-warning}\nThe `sklearn` function `Lasso()` uses the argument name `alpha` for $\\lambda$.  \n:::\n\n### Elastic Net\n\nIf Ridge regression is one way to improve OLS in overfit cases, and LASSO regression is another, why not have the best of both worlds?\n\nThere is no reason not to add *both* penalties to our loss function:\n\n$$ \\ell(\\beta) = SSE + \\lambda \\left(\\sum_{j = 1}^p \\beta^2 + \\sum_{j = 1}^p |\\beta_j| \\right)$$\n\nOf course, we now have **three** concerns to balance:  The SSE, the Ridge penalty, *and* the LASSO penalty.  So in addition to $\\lambda$, which balances SSE and penalties, we need to add in a number $\\alpha$ for how much we care (relatively) about Ridge vs LASSO:\n\n$$ \\ell(\\beta) = SSE + \\lambda \\left(\\alpha \\sum_{j = 1}^p \\beta^2 + (1-\\alpha) \\sum_{j = 1}^p |\\beta_j| \\right)$$\n\nBut what is the best choice of $\\alpha$?  I think you can see where this is headed...\n\n\n### Your Turn\n\n:::{.callout-practice-exercise}\n\nCreate an Elastic Net pipeline, and tune $\\lambda$ and $\\alpha$.\n\nFit your best model on the full Ames data, and compare the coefficients to Ridge and OLS.\n\n:::\n\n:::{.callout-warning}\nThe `sklearn` function `ElasticNet()` uses the argument name `alpha` for $\\lambda$, and the argument name `l1_ratio` for $\\alpha$.\n\nYes, this is confusing and annoying!  But since essentially **all** of the literature uses $\\lambda$ and $\\alpha$ the way we have used them in this chapter, we are choosing to be consistent with that notation instead of `sklearn`'s argument names.\n:::\n\n\n## Wrap-Up\n\nIn this chapter, you may feel a bit tricked!  We promised you new model specifications, but actually, we *still* haven't changed our prediction approach from a simple linear model:\n\n$$\\hat{y}_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots +  \\beta_p x_{ip}$$\n\nIt's important to remember, though, that how you **fit** the model to data is every bit as important a piece of the **model specification** decision as the equation itself.  \n\nBy changing our definition of \"best\" coefficients - that is, changing the *loss function* that our ideal $\\beta$'s would minimize - we were able to massively impact the resulting prediction procedure.\n\n",
    "supporting": [
      "13-penalized_regression_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}